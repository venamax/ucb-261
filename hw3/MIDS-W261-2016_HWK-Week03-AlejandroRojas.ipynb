{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW3.0.\n",
    "What is a merge sort? Where is it used in Hadoop?\n",
    "How is  a combiner function in the context of Hadoop? \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "What is the Hadoop shuffle?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)</h2>\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data source file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Complaint ID', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'State', 'ZIP code', 'Submitted via', 'Date received', 'Date sent to company', 'Company', 'Company response', 'Timely response?', 'Consumer disputed?']\n",
      "['1113072', 'Credit reporting', '', 'Incorrect information on credit report', 'Account status', 'NY', '11514', 'Web', '11/12/2014', '11/12/2014', 'TransUnion', 'Closed with explanation', 'Yes', '']\n"
     ]
    }
   ],
   "source": [
    "with open('Consumer_Complaints.csv') as f:\n",
    "    d = list(csv.reader(f))\n",
    "\n",
    "print (d[0])\n",
    "print(d[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 18:20:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>upload source file</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 18:20:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 18:20:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "-rw-r--r--   1 venamax supergroup   50906486 2016-02-02 18:20 Consumer_Complaints.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put Consumer_Complaints.csv /user/venamax                 #### save source data file to hdfs  \\\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>start/stop yarn  and hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-venamax-resourcemanager-VENAMAX.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-venamax-nodemanager-VENAMAX.local.out\n",
      "16/02/02 17:29:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-venamax-namenode-VENAMAX.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-venamax-datanode-VENAMAX.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-venamax-secondarynamenode-VENAMAX.local.out\n",
      "16/02/02 17:29:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "from csv import reader\n",
    "import sys\n",
    "\n",
    "\n",
    "for row in reader(iter(sys.stdin.readline, '')):\n",
    "    product = row[1].lower()\n",
    "    \n",
    "    if product=='debt collection' or product == 'mortgage':\n",
    "        product = product\n",
    "    else:\n",
    "        product = 'other'\n",
    "    if product:\n",
    "        sys.stderr.write(\"reporter:counter:complaints:,%s,1\\n\"%product)\n",
    "    print ('%s\\t%s' % (product, 1))             ### mapper out looks like 'product' \\t 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_product = None\n",
    "current_count = 0\n",
    "product = None\n",
    "productlist = []\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    product = line[0]                           ### integer generated randomly we got from mapper.py\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_product == product:                ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "    else:\n",
    "        if current_product:\n",
    "            \n",
    "            productlist.append((current_product,current_count))  ### store tuple in a list once totalize count per number\n",
    "\n",
    "        current_count = count                   ### set current count\n",
    "        current_product = product                 ### set current product\n",
    "\n",
    "\n",
    "if current_product == product:                    ### do not forget to output the last word if needed!\n",
    "    productlist.append((current_product,current_count))\n",
    "\n",
    "\n",
    "toplist = sorted(productlist,key=lambda record: record[1], reverse=True) ### sort list from largest count to smallest\n",
    "\n",
    "print '%25s' %'TOP Products'\n",
    "print '%20s' %'Products', '%10s' %'Count'\n",
    "for i in range(len(toplist)):\n",
    "    print '%20s%10s' % (toplist[i][0], toplist[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Consumer_Complaints in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 18:36:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 18:36:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax/complaintsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax/complaintsOutput                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar hadoopstreamingjarfile \\\n",
    "\n",
    "    -D stream.num.map.output.key.fields=n \\\n",
    "    -mapper mapperfile \\\n",
    "    -reducer reducerfile \\\n",
    "    -input inputfile \\\n",
    "    -output outputfile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 18:37:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 18:37:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 18:37:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 18:37:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 18:37:08 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 18:37:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local736012362_0001\n",
      "16/02/02 18:37:08 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 18:37:08 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 18:37:08 INFO mapreduce.Job: Running job: job_local736012362_0001\n",
      "16/02/02 18:37:08 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 18:37:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/02 18:37:08 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 18:37:08 INFO mapred.LocalJobRunner: Starting task: attempt_local736012362_0001_m_000000_0\n",
      "16/02/02 18:37:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/02 18:37:08 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 18:37:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/Consumer_Complaints.csv:0+50906486\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 18:37:08 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 18:37:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw3/./mapper.py]\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 18:37:08 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 18:37:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:08 INFO streaming.PipeMapRed: Records R/W=2369/1\n",
      "16/02/02 18:37:09 INFO streaming.PipeMapRed: R/W/S=10000/8671/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:09 INFO mapreduce.Job: Job job_local736012362_0001 running in uber mode : false\n",
      "16/02/02 18:37:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/02 18:37:09 INFO streaming.PipeMapRed: R/W/S=100000/98977/0 in:100000=100000/1 [rec/s] out:98977=98977/1 [rec/s]\n",
      "16/02/02 18:37:10 INFO streaming.PipeMapRed: R/W/S=200000/197176/0 in:100000=200000/2 [rec/s] out:98588=197176/2 [rec/s]\n",
      "16/02/02 18:37:11 INFO streaming.PipeMapRed: R/W/S=300000/297855/0 in:150000=300000/2 [rec/s] out:148927=297855/2 [rec/s]\n",
      "16/02/02 18:37:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 18:37:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 18:37:11 INFO mapred.LocalJobRunner: \n",
      "16/02/02 18:37:11 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 18:37:11 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 18:37:11 INFO mapred.MapTask: bufstart = 0; bufend = 3324280; bufvoid = 104857600\n",
      "16/02/02 18:37:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962748(99850992); length = 1251649/6553600\n",
      "16/02/02 18:37:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 18:37:12 INFO mapred.Task: Task:attempt_local736012362_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: Records R/W=2369/1\n",
      "16/02/02 18:37:12 INFO mapred.Task: Task 'attempt_local736012362_0001_m_000000_0' done.\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local736012362_0001_m_000000_0\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: Starting task: attempt_local736012362_0001_r_000000_0\n",
      "16/02/02 18:37:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/02 18:37:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 18:37:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 18:37:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@37d4166f\n",
      "16/02/02 18:37:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 18:37:12 INFO reduce.EventFetcher: attempt_local736012362_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 18:37:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local736012362_0001_m_000000_0 decomp: 3950108 len: 3950112 to MEMORY\n",
      "16/02/02 18:37:12 INFO reduce.InMemoryMapOutput: Read 3950108 bytes from map-output for attempt_local736012362_0001_m_000000_0\n",
      "16/02/02 18:37:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3950108, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3950108\n",
      "16/02/02 18:37:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 18:37:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 18:37:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 18:37:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3950090 bytes\n",
      "16/02/02 18:37:12 INFO reduce.MergeManagerImpl: Merged 1 segments, 3950108 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 18:37:12 INFO reduce.MergeManagerImpl: Merging 1 files, 3950112 bytes from disk\n",
      "16/02/02 18:37:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 18:37:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 18:37:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3950090 bytes\n",
      "16/02/02 18:37:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw3/./reducer.py]\n",
      "16/02/02 18:37:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 18:37:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/02 18:37:12 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:13 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 18:37:13 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 18:37:13 INFO streaming.PipeMapRed: Records R/W=312913/1\n",
      "16/02/02 18:37:13 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 18:37:13 INFO mapred.Task: Task:attempt_local736012362_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 18:37:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 18:37:13 INFO mapred.Task: Task attempt_local736012362_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 18:37:13 INFO output.FileOutputCommitter: Saved output of task 'attempt_local736012362_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/complaintsOutput/_temporary/0/task_local736012362_0001_r_000000\n",
      "16/02/02 18:37:13 INFO mapred.LocalJobRunner: Records R/W=312913/1 > reduce\n",
      "16/02/02 18:37:13 INFO mapred.Task: Task 'attempt_local736012362_0001_r_000000_0' done.\n",
      "16/02/02 18:37:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local736012362_0001_r_000000_0\n",
      "16/02/02 18:37:13 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 18:37:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 18:37:13 INFO mapreduce.Job: Job job_local736012362_0001 completed successfully\n",
      "16/02/02 18:37:13 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8112076\n",
      "\t\tFILE: Number of bytes written=12647416\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=156\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=3324280\n",
      "\t\tMap output materialized bytes=3950112\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=3950112\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=636485632\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcomplaints:\n",
      "\t\tdebt collection=44372\n",
      "\t\tmortgage=125752\n",
      "\t\tother=142789\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=156\n",
      "16/02/02 18:37:13 INFO streaming.StreamJob: Output directory: complaintsOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input Consumer_Complaints.csv -output complaintsOutput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 18:34:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "             TOP Products\t\n",
      "            Products      Count\t\n",
      "               other    142789\t\n",
      "            mortgage    125752\t\n",
      "     debt collection     44372\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat complaintsOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 3.1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</h2>\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = open(\"oneline.txt\", \"w\") # writing file\n",
    "\n",
    "terms = ['foo', 'foo',  'quux',  'labs',  'foo',  'bar',  'quux' ]\n",
    "b = ' '\n",
    "\n",
    "for term in terms:\n",
    "    \n",
    "    input_file.write(str(term))   \n",
    "    input_file.write(b)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "input_file.write('\\n')\n",
    "\n",
    "   \n",
    "\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 18:58:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 18:58:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 venamax supergroup   50906486 2016-02-02 18:20 Consumer_Complaints.csv\n",
      "drwxr-xr-x   - venamax supergroup          0 2016-02-02 18:37 complaintsOutput\n",
      "-rw-r--r--   1 venamax supergroup         32 2016-02-02 18:58 oneline.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put oneline.txt /user/venamax                 #### save source data file to hdfs  \\\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counter,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        print '%s\\t%s' % (word,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "words = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mappe\n",
    "    word = line[0]\n",
    "    \n",
    "    try:\n",
    "\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_word == word:                     ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "    else:\n",
    "        if current_word:\n",
    "          \n",
    "            words[current_word] = current_count  ### store tuple in a list once totalize count per number\n",
    "  \n",
    "        current_count = count                    ### set current count\n",
    "        current_word = word                      ### set current word\n",
    "\n",
    "\n",
    "if current_word == word:                         ### do not forget to output the last word if needed!\n",
    "    words[current_word] = current_count \n",
    "\n",
    "for word in words:\n",
    "    print 'We found \"%s\"' %word, ' on %s'%words[word] , 'occassions.'\n",
    "    sys.stderr.write(\"reporter:counter:Reducer Counter,Calls,1\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run onelinecount in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 19:24:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 19:24:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax/onelinecountOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax/onelinecountOutput    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 19:24:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 19:24:07 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 19:24:07 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 19:24:07 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 19:24:07 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 19:24:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local202524920_0001\n",
      "16/02/02 19:24:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 19:24:07 INFO mapreduce.Job: Running job: job_local202524920_0001\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 19:24:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: Starting task: attempt_local202524920_0001_m_000000_0\n",
      "16/02/02 19:24:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/02 19:24:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 19:24:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/oneline.txt:0+32\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw3/./wordcount/mapper.py]\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: \n",
      "16/02/02 19:24:07 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/02 19:24:07 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 19:24:07 INFO mapred.Task: Task:attempt_local202524920_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/02 19:24:07 INFO mapred.Task: Task 'attempt_local202524920_0001_m_000000_0' done.\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: Finishing task: attempt_local202524920_0001_m_000000_0\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: Starting task: attempt_local202524920_0001_r_000000_0\n",
      "16/02/02 19:24:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/02 19:24:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 19:24:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 19:24:07 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d9aa2ac\n",
      "16/02/02 19:24:07 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 19:24:07 INFO reduce.EventFetcher: attempt_local202524920_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 19:24:07 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local202524920_0001_m_000000_0 decomp: 61 len: 65 to MEMORY\n",
      "16/02/02 19:24:07 INFO reduce.InMemoryMapOutput: Read 61 bytes from map-output for attempt_local202524920_0001_m_000000_0\n",
      "16/02/02 19:24:07 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 61, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->61\n",
      "16/02/02 19:24:07 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 19:24:07 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 19:24:07 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 19:24:07 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55 bytes\n",
      "16/02/02 19:24:07 INFO reduce.MergeManagerImpl: Merged 1 segments, 61 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 19:24:07 INFO reduce.MergeManagerImpl: Merging 1 files, 65 bytes from disk\n",
      "16/02/02 19:24:07 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 19:24:07 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 19:24:07 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55 bytes\n",
      "16/02/02 19:24:07 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw3/./wordcount/reducer.py]\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 19:24:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 19:24:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 19:24:08 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 19:24:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 19:24:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 19:24:08 INFO mapred.Task: Task:attempt_local202524920_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 19:24:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 19:24:08 INFO mapred.Task: Task attempt_local202524920_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 19:24:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local202524920_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/onelinecountOutput/_temporary/0/task_local202524920_0001_r_000000\n",
      "16/02/02 19:24:08 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/02 19:24:08 INFO mapred.Task: Task 'attempt_local202524920_0001_r_000000_0' done.\n",
      "16/02/02 19:24:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local202524920_0001_r_000000_0\n",
      "16/02/02 19:24:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 19:24:08 INFO mapreduce.Job: Job job_local202524920_0001 running in uber mode : false\n",
      "16/02/02 19:24:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 19:24:08 INFO mapreduce.Job: Job job_local202524920_0001 completed successfully\n",
      "16/02/02 19:24:08 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=211950\n",
      "\t\tFILE: Number of bytes written=797299\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=64\n",
      "\t\tHDFS: Number of bytes written=138\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=65\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=65\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=547356672\n",
      "\tMapper Counter\n",
      "\t\tCalls=1\n",
      "\tReducer Counter\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=32\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=138\n",
      "16/02/02 19:24:08 INFO streaming.StreamJob: Output directory: onelinecountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper wordcount/mapper.py -reducer wordcount/reducer.py -input oneline.txt -output onelinecountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.2.1 OPTIONAL \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW3.3. Shopping Cart Analysis</h2>\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs</h2>\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.</h2>\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OPTIONAL: all HW below this are optional \n",
    "\n",
    "NOTE:   -- as of 1/28/2016 the instructions needs to be completed (Talk to Jimi)\n",
    "\n",
    "\n",
    "\n",
    "== Preliminary information ===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction.\n",
    "\n",
    "<h2>HW3.6\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.</h2>\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above): \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HW3.7. Shopping Cart Analysis </h2>\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HW3.8\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: </h2>\n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times.\n",
    "\n",
    "\n",
    "HW3.8 (Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: \n",
    "\n",
    "(item1, item2) ⇒ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  — map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
