{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATSCI W261 ASSIGNMENT 3\n",
    "Section 2<br/>\n",
    "Student: Kuan Lin<br/>\n",
    "Date: 1/29/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.0. What is a merge sort? Where is it used in Hadoop? How is  a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem. What is the Hadoop shuffle?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress). Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper to emit counts for the three defined categories\n",
    "import sys\n",
    "for line in sys.stdin:    \n",
    "    line = line.strip().lower()\n",
    "    if line == '' or line.startswith('complaint id'): continue\n",
    "        \n",
    "    # get the complaint category\n",
    "    category = line.split(',')[1].strip()\n",
    "    counter_name = 'other_complaints'\n",
    "    if category == 'debt collection':\n",
    "        counter_name = 'debt_collection_complaints'\n",
    "    elif category == 'mortgage':\n",
    "        counter_name = 'mortgage_complaints'\n",
    "    \n",
    "    # update hadoop counters:\n",
    "    sys.stderr.write(\"reporter:counter:customer_complaints,%s,1\\n\"%counter_name)\n",
    "    # emit regular mapper result\n",
    "    print '%s\\t%s' % (counter_name, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "current_category = None\n",
    "current_count = 0\n",
    "category = None\n",
    "for line in sys.stdin:\n",
    "    category, count = line.strip().split('\\t', 1)\n",
    "    if category == current_category:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_category != None:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_category, current_count)\n",
    "        current_category = category\n",
    "        current_count = int(count)\n",
    "if category == current_category:\n",
    "    print '%s\\t%s' % (current_category, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 01:24:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_1_output\n",
      "16/01/30 01:24:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob5504775449930135467.jar tmpDir=null\n",
      "16/01/30 01:24:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 01:24:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 01:24:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 01:24:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 01:24:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0005\n",
      "16/01/30 01:24:19 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0005\n",
      "16/01/30 01:24:20 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0005/\n",
      "16/01/30 01:24:20 INFO mapreduce.Job: Running job: job_1454112721658_0005\n",
      "16/01/30 01:24:36 INFO mapreduce.Job: Job job_1454112721658_0005 running in uber mode : false\n",
      "16/01/30 01:24:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 01:25:02 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/01/30 01:25:04 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/01/30 01:25:06 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/01/30 01:25:07 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "16/01/30 01:25:09 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "16/01/30 01:25:10 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/30 01:25:12 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "16/01/30 01:25:13 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "16/01/30 01:25:15 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "16/01/30 01:25:16 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/01/30 01:25:17 INFO mapreduce.Job:  map 79% reduce 0%\n",
      "16/01/30 01:25:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 01:25:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 01:25:35 INFO mapreduce.Job: Job job_1454112721658_0005 completed successfully\n",
      "16/01/30 01:25:35 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7392134\n",
      "\t\tFILE: Number of bytes written=15126327\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=84\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=78498\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13955\n",
      "\t\tTotal time spent by all map tasks (ms)=78498\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13955\n",
      "\t\tTotal vcore-seconds taken by all map tasks=78498\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=13955\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=80381952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14289920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=6766304\n",
      "\t\tMap output materialized bytes=7392140\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=7392140\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1140\n",
      "\t\tCPU time spent (ms)=21030\n",
      "\t\tPhysical memory (bytes) snapshot=565727232\n",
      "\t\tVirtual memory (bytes) snapshot=7589130240\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustomer_complaints\n",
      "\t\tdebt_collection_complaints=44372\n",
      "\t\tmortgage_complaints=125752\n",
      "\t\tother_complaints=142788\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=84\n",
      "16/01/30 01:25:35 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_1_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_1_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_1_output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debt_collection_complaints\t44372\r\n",
      "mortgage_complaints\t125752\r\n",
      "other_complaints\t142788\r\n"
     ]
    }
   ],
   "source": [
    "# examing the results\n",
    "!hdfs dfs -cat /w261/hw3/hw3_1_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* <b>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</b>\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer \n",
    "based WordCount (i.e., no combiners are used here) using user defined Counters to count up \n",
    "how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter,\n",
    " and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively.\n",
    " Please explain.\n",
    " Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using \n",
    "a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined \n",
    "Counters to count up how many time the mapper and reducer are called. What is the value of \n",
    "your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, \n",
    "Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined \n",
    "Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your \n",
    "user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? \n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and \n",
    "their frequency and their relative frequency. If there are ties please sort the tokens in \n",
    "alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hw3_2_input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw3_2_input.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the input file into hdfs\n",
    "!hdfs dfs -put hw3_2_input.txt /w261/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    line = line.strip().lower()\n",
    "    if line == '': continue\n",
    "    for word in line.split(' '):\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "from operator import itemgetter\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    if len(line.split('\\t')) != 2: continue\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/w261/hw3/hw3_2_output': No such file or directory\n",
      "16/01/30 02:07:58 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1960028508392768622.jar tmpDir=null\n",
      "16/01/30 02:08:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:08:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:08:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 02:08:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 02:08:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 02:08:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 02:08:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0008\n",
      "16/01/30 02:08:07 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0008\n",
      "16/01/30 02:08:07 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0008/\n",
      "16/01/30 02:08:07 INFO mapreduce.Job: Running job: job_1454112721658_0008\n",
      "16/01/30 02:08:25 INFO mapreduce.Job: Job job_1454112721658_0008 running in uber mode : false\n",
      "16/01/30 02:08:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 02:08:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 02:09:19 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/30 02:09:24 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 02:09:27 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/30 02:09:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 02:09:30 INFO mapreduce.Job: Job job_1454112721658_0008 completed successfully\n",
      "16/01/30 02:09:31 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=570158\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=128\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9910\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=181621\n",
      "\t\tTotal time spent by all map tasks (ms)=9910\n",
      "\t\tTotal time spent by all reduce tasks (ms)=181621\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9910\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=181621\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10147840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=185979904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=2948\n",
      "\t\tCPU time spent (ms)=7510\n",
      "\t\tPhysical memory (bytes) snapshot=683544576\n",
      "\t\tVirtual memory (bytes) snapshot=12668096512\n",
      "\t\tTotal committed heap usage (bytes)=409276416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=1\n",
      "\t\treducer_count=4\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/01/30 02:09:31 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=1 -D mapred.reduce.tasks=4 -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/hw3_2_input.txt -output /w261/hw3/hw3_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n",
      "labs\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one line and four distinct words, the optimal setting should be using 1 mapper and 4 reducers.  However, it seems that the default setting for Hadoop is to do 2 maper and 1 reducer.  If I specify mapper and reducer task counts in Hadoop Streaming parameter and 1 and 4, I now get 1 and 4 as specified in the homework problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.2 continues</b>: Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using \n",
    "a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined \n",
    "Counters to count up how many time the mapper and reducer are called. What is the value of \n",
    "your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    line = line.strip().lower()\n",
    "    if line == '' or line.startswith(\"complaint id\"): continue\n",
    "    issue_text = line.split(',')[3]\n",
    "    print '%s\\t%s' % (issue_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 02:28:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_2_1_output\n",
      "16/01/30 02:28:38 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob1057362170451352704.jar tmpDir=null\n",
      "16/01/30 02:28:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:28:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 02:28:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 02:28:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 02:28:46 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 02:28:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 02:28:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0011\n",
      "16/01/30 02:28:47 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0011\n",
      "16/01/30 02:28:47 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0011/\n",
      "16/01/30 02:28:47 INFO mapreduce.Job: Running job: job_1454112721658_0011\n",
      "16/01/30 02:29:05 INFO mapreduce.Job: Job job_1454112721658_0011 running in uber mode : false\n",
      "16/01/30 02:29:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 02:29:30 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "16/01/30 02:29:31 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "16/01/30 02:29:33 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 02:29:34 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 02:29:35 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 02:29:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 02:29:59 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 02:30:01 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/01/30 02:30:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 02:30:04 INFO mapreduce.Job: Job job_1454112721658_0011 completed successfully\n",
      "16/01/30 02:30:05 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8735828\n",
      "\t\tFILE: Number of bytes written=17927758\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=2375\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=56358\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=48833\n",
      "\t\tTotal time spent by all map tasks (ms)=56358\n",
      "\t\tTotal time spent by all reduce tasks (ms)=48833\n",
      "\t\tTotal vcore-seconds taken by all map tasks=56358\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=48833\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=57710592\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=50004992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=8109992\n",
      "\t\tMap output materialized bytes=8735840\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce shuffle bytes=8735840\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=79\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1171\n",
      "\t\tCPU time spent (ms)=14030\n",
      "\t\tPhysical memory (bytes) snapshot=678158336\n",
      "\t\tVirtual memory (bytes) snapshot=10123059200\n",
      "\t\tTotal committed heap usage (bytes)=453255168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2375\n",
      "16/01/30 02:30:05 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_1_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_1_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=2 -D mapred.reduce.tasks=2 -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_2_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper and reducer counter are both 2, which is exactly what I specified in the parameters.  This is not always going to be the case though - we don't have direct control over how Hadoop is goint to split jobs.  The parameters is just a hint for the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.2 continues</b>Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, \n",
    "Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined \n",
    "Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your \n",
    "user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,combiner_count,1\\n\")\n",
    "from operator import itemgetter\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    if len(line.split('\\t')) != 2: continue\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 04:11:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_2_2_output\n",
      "16/01/30 04:11:31 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/combiner.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob7026719206050507258.jar tmpDir=null\n",
      "16/01/30 04:11:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 04:11:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 04:11:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 04:11:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 04:11:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 04:11:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 04:11:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454112721658_0018\n",
      "16/01/30 04:11:40 INFO impl.YarnClientImpl: Submitted application application_1454112721658_0018\n",
      "16/01/30 04:11:40 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454112721658_0018/\n",
      "16/01/30 04:11:40 INFO mapreduce.Job: Running job: job_1454112721658_0018\n",
      "16/01/30 04:11:58 INFO mapreduce.Job: Job job_1454112721658_0018 running in uber mode : false\n",
      "16/01/30 04:11:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 04:12:25 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "16/01/30 04:12:26 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "16/01/30 04:12:28 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 04:12:29 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 04:12:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 04:12:55 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 04:12:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 04:12:58 INFO mapreduce.Job: Job job_1454112721658_0018 completed successfully\n",
      "16/01/30 04:12:58 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4588\n",
      "\t\tFILE: Number of bytes written=467754\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=2908\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=65066\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=38749\n",
      "\t\tTotal time spent by all map tasks (ms)=65066\n",
      "\t\tTotal time spent by all reduce tasks (ms)=38749\n",
      "\t\tTotal vcore-seconds taken by all map tasks=65066\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=38749\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=66627584\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=39678976\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=8109992\n",
      "\t\tMap output materialized bytes=4600\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=312912\n",
      "\t\tCombine output records=146\n",
      "\t\tReduce input groups=98\n",
      "\t\tReduce shuffle bytes=4600\n",
      "\t\tReduce input records=146\n",
      "\t\tReduce output records=98\n",
      "\t\tSpilled Records=292\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=1158\n",
      "\t\tCPU time spent (ms)=12410\n",
      "\t\tPhysical memory (bytes) snapshot=677875712\n",
      "\t\tVirtual memory (bytes) snapshot=10129555456\n",
      "\t\tTotal committed heap usage (bytes)=453255168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tcombiner_count=4\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2908\n",
      "16/01/30 04:12:58 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_2_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x combiner.py\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_2_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=2 -D mapred.reduce.tasks=2 -mapper mapper.py -file /data/w261/hw3/mapper.py -combiner combiner.py -file /data/w261/hw3/combiner.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_2_2_output\n",
    "#!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar -D mapred.map.tasks=2 -D mapred.reduce.tasks=2 -mapper mapper.py -file /data/w261/hw3/mapper.py -reducer reducer.py -file /data/w261/hw3/reducer.py -input /w261/hw3/Consumer_Complaints.csv -output /w261/hw3/hw3_2_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiner Counter: 4<br/>\n",
    "Mapper Counter: 2<br/>\n",
    "Reducer Counter: 2<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>HW3.2 Continue</b>: Using a single reducer: What are the top 50 most frequent terms in your word count analysis? \n",
    "Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in \n",
    "alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"account opening\t16205\r\n",
      "\"making/receiving payments\t3226\r\n",
      "advertising and marketing\t1193\r\n",
      "arbitration\t168\r\n",
      "balance transfer\t502\r\n",
      "balance transfer fee\t95\r\n",
      "bankruptcy\t222\r\n",
      "can't contact lender\t221\r\n",
      "cash advance\t136\r\n",
      "cash advance fee\t104\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_2_output/* | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import csv\n",
    "# record mapper invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,mapper_count,1\\n\")\n",
    "\n",
    "total_count = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().lower()\n",
    "    if line == '' or line.startswith(\"complaint id\"): continue\n",
    "    lineArr = list(csv.reader(line.splitlines(), delimiter=',', quotechar='\"'))[0]\n",
    "    issue_text = lineArr[3]\n",
    "    total_count += 1\n",
    "    if issue_text.strip() == '': issue_text = 'blank'\n",
    "    print '%s\\t%s\\t%s' % (\"issue_words\", issue_text, 1)\n",
    "\n",
    "# print total word count from this mapper to calculate relative frequency\n",
    "print '%s\\t%s\\t%s' % (\"issue_words\", \"*\", total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# record reducer invokation\n",
    "sys.stderr.write(\"reporter:counter:custom_counters,reducer_count,1\\n\")\n",
    "total_count = 0\n",
    "current_issue = None\n",
    "current_count = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if line == '': continue\n",
    "    lineArr = line.split('\\t')\n",
    "    issue_text = lineArr[1]\n",
    "    issue_count = int(lineArr[2])\n",
    "    # aggregate total counts\n",
    "    if issue_text == \"*\":\n",
    "        total_count += issue_count\n",
    "    else:\n",
    "        if issue_text == current_issue:\n",
    "            current_count += issue_count\n",
    "        else:\n",
    "            if current_issue != None and total_count != 0:\n",
    "                print \"%s\\t%s\\t%.6f\" % (current_issue, current_count, float(current_count)/float(total_count))\n",
    "            current_issue = issue_text\n",
    "            current_count = issue_count\n",
    "if current_issue != None and total_count != 0:\n",
    "    print \"%s\\t%s\\t%.6f\" % (current_issue, current_count, float(current_count)/float(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 22:35:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /w261/hw3/hw3_2_3_output\n",
      "16/01/30 22:36:02 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/data/w261/hw3/mapper.py, /data/w261/hw3/reducer.py] [/usr/lib/hadoop/lib/hadoop-streaming-2.6.0.jar] /tmp/streamjob3516562719083472012.jar tmpDir=null\n",
      "16/01/30 22:36:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 22:36:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 22:36:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 22:36:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 22:36:09 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 22:36:09 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/30 22:36:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454188161308_0010\n",
      "16/01/30 22:36:10 INFO impl.YarnClientImpl: Submitted application application_1454188161308_0010\n",
      "16/01/30 22:36:11 INFO mapreduce.Job: The url to track the job: http://ip-172-31-55-170.ec2.internal:8088/proxy/application_1454188161308_0010/\n",
      "16/01/30 22:36:11 INFO mapreduce.Job: Running job: job_1454188161308_0010\n",
      "16/01/30 22:36:27 INFO mapreduce.Job: Job job_1454188161308_0010 running in uber mode : false\n",
      "16/01/30 22:36:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 22:36:52 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/01/30 22:36:55 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "16/01/30 22:36:58 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "16/01/30 22:36:59 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "16/01/30 22:37:02 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 22:37:03 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 22:37:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 22:37:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 22:37:20 INFO mapreduce.Job: Job job_1454188161308_0010 completed successfully\n",
      "16/01/30 22:37:21 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16198865\n",
      "\t\tFILE: Number of bytes written=32741682\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910095\n",
      "\t\tHDFS: Number of bytes written=3188\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=68832\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13977\n",
      "\t\tTotal time spent by all map tasks (ms)=68832\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13977\n",
      "\t\tTotal vcore-seconds taken by all map tasks=68832\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=13977\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=70483968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14312448\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312914\n",
      "\t\tMap output bytes=15573031\n",
      "\t\tMap output materialized bytes=16198871\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=81\n",
      "\t\tReduce shuffle bytes=16198871\n",
      "\t\tReduce input records=312914\n",
      "\t\tReduce output records=79\n",
      "\t\tSpilled Records=625828\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1061\n",
      "\t\tCPU time spent (ms)=17950\n",
      "\t\tPhysical memory (bytes) snapshot=578310144\n",
      "\t\tVirtual memory (bytes) snapshot=7588909056\n",
      "\t\tTotal committed heap usage (bytes)=392372224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tcustom_counters\n",
      "\t\tmapper_count=2\n",
      "\t\treducer_count=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3188\n",
      "16/01/30 22:37:21 INFO streaming.StreamJob: Output directory: /w261/hw3/hw3_2_3_output\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py;\n",
    "!hdfs dfs -rm -r /w261/hw3/hw3_2_3_output;\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/lib/hadoop-*streaming*.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3nr -k2\" \\\n",
    "-mapper mapper.py -file /data/w261/hw3/mapper.py \\\n",
    "-reducer reducer.py -file /data/w261/hw3/reducer.py \\\n",
    "-input /w261/hw3/Consumer_Complaints.csv \\\n",
    "-output /w261/hw3/hw3_2_3_output;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 frequent items and their relative frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan modification,collection,foreclosure\t70487\t0.225261\r\n",
      "loan servicing, payments, escrow account\t36767\t0.117499\r\n",
      "incorrect information on credit report\t29069\t0.092898\r\n",
      "cont'd attempts collect debt not owed\t17972\t0.057435\r\n",
      "account opening, closing, or management\t16205\t0.051788\r\n",
      "deposits and withdrawals\t10555\t0.033732\r\n",
      "communication tactics\t8671\t0.027711\r\n",
      "application, originator, mortgage broker\t8625\t0.027564\r\n",
      "disclosure verification of debt\t7655\t0.024464\r\n",
      "billing disputes\t6938\t0.022172\r\n",
      "other\t6273\t0.020047\r\n",
      "problems caused by my funds being low\t5663\t0.018098\r\n",
      "credit reporting company's investigation\t4858\t0.015525\r\n",
      "managing the loan or lease\t4560\t0.014573\r\n",
      "unable to get credit report/credit score\t4357\t0.013924\r\n",
      "settlement process and costs\t4350\t0.013902\r\n",
      "repaying your loan\t3844\t0.012285\r\n",
      "problems when you are unable to pay\t3821\t0.012211\r\n",
      "false statements or representation\t3621\t0.011572\r\n",
      "improper contact or sharing of info\t3489\t0.011150\r\n",
      "apr or interest rate\t3431\t0.010965\r\n",
      "identity theft / fraud / embezzlement\t3276\t0.010469\r\n",
      "making/receiving payments, sending money\t3226\t0.010310\r\n",
      "taking/threatening an illegal action\t2964\t0.009472\r\n",
      "closing/cancelling account\t2795\t0.008932\r\n",
      "credit decision / underwriting\t2774\t0.008865\r\n",
      "using a debit or atm card\t2422\t0.007740\r\n",
      "dealing with my lender or servicer\t1944\t0.006213\r\n",
      "late fee\t1797\t0.005743\r\n",
      "credit reporting\t1701\t0.005436\r\n",
      "can't repay my loan\t1647\t0.005263\r\n",
      "credit determination\t1490\t0.004762\r\n",
      "improper use of my credit report\t1477\t0.004720\r\n",
      "credit monitoring or identity protection\t1453\t0.004643\r\n",
      "customer service / customer relations\t1367\t0.004369\r\n",
      "credit card protection / debt protection\t1343\t0.004292\r\n",
      "taking out the loan or lease\t1242\t0.003969\r\n",
      "billing statement\t1220\t0.003899\r\n",
      "advertising and marketing\t1193\t0.003813\r\n",
      "payoff process\t1155\t0.003691\r\n",
      "credit line increase/decrease\t1149\t0.003672\r\n",
      "transaction issue\t1098\t0.003509\r\n",
      "other fee\t1075\t0.003435\r\n",
      "delinquent account\t1061\t0.003391\r\n",
      "collection practices\t1003\t0.003205\r\n",
      "rewards\t1002\t0.003202\r\n",
      "collection debt dispute\t904\t0.002889\r\n",
      "charged fees or interest i didn't expect\t807\t0.002579\r\n",
      "unsolicited issuance of credit card\t640\t0.002045\r\n",
      "fraud or scam\t566\t0.001809\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_3_output/part-00000 | sort -t$'\\t' -k2,2nr -k1,1 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 10 tokens and their counts and relative frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank\t4\t0.000013\r\n",
      "incorrect/missing disclosures or info\t64\t0.000205\r\n",
      "charged bank acct wrong day or amt\t71\t0.000227\r\n",
      "convenience checks\t75\t0.000240\r\n",
      "payment to acct not credited\t92\t0.000294\r\n",
      "balance transfer fee\t95\t0.000304\r\n",
      "wrong amount charged or received\t98\t0.000313\r\n",
      "cash advance fee\t104\t0.000332\r\n",
      "received a loan i didn't apply for\t118\t0.000377\r\n",
      "overlimit fee\t127\t0.000406\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /w261/hw3/hw3_2_3_output/part-00000 | sort -t$'\\t' -k2,2n -k1,1 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
