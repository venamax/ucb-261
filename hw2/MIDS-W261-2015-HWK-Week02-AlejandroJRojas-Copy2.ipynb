{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b>Homework 2</b></h1>\n",
    "<i>Alejandro J. Rojas<br>\n",
    "ale@ischool.berkeley.edu<br>\n",
    "W261: Machine Learning at Scale<br>\n",
    "Week: 02<br>\n",
    "Jan 26, 2016</i></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.0.  </h2>\n",
    "What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.1. Sort in Hadoop MapReduce</h2>\n",
    "Given as input: Records of the form '<'integer, “NA”'>', where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form '<'integer, “NA”'>' in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form '<'integer, “NA”'>'. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "N = 10000                           ### for a sample size of N\n",
    "random.seed(0)                      ### pick a random seed to replicate results\n",
    "\n",
    "\n",
    "input_file = open(\"numcount.txt\", \"w\") # writing file\n",
    "\n",
    "for i in range(N):\n",
    "    a = random.randint(0, 100)       ### Select a random integer from 0 to 100\n",
    "    b = ''\n",
    "    input_file.write(str(a))         \n",
    "    input_file.write(b)\n",
    "    input_file.write('\\n')\n",
    "\n",
    "   \n",
    "\n",
    "input_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:                         ### input comes from STDIN (standard input)\n",
    "    \n",
    "    number = line.strip()                      ### remove leading and trailing whitespace\n",
    "    print ('%s\\t%s' % (number, 1))             ### mapper out looks like 'number' \\t 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_number = None\n",
    "current_count = 0\n",
    "number = None\n",
    "numlist = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    number = line[0]                           ### integer generated randomly we got from mapper.py\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_number == number:                ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "    else:\n",
    "        if current_number:\n",
    "            \n",
    "            numlist.append((current_number,current_count))  ### store tuple in a list once totalize count per number\n",
    "\n",
    "        current_count = count                   ### set current count\n",
    "        current_number = number                 ### set current number\n",
    "\n",
    "\n",
    "if current_number == number:                    ### do not forget to output the last word if needed!\n",
    "    numlist.append((current_number,current_count))\n",
    "\n",
    "\n",
    "toplist = sorted(numlist,key=lambda record: record[1], reverse=True) ### sort list from largest count to smallest\n",
    "bottomlist = sorted(numlist,key=lambda record: record[1])  ### sort list from smalles to largest\n",
    "\n",
    "print '%25s' %'TOP 10', '%25s' % '', '%28s' %'BOTTOM 10'\n",
    "print '%20s' %'Number', '%10s' %'Count', '%20s' % '', '%20s' %'Number','%10s' %'Count'\n",
    "for i in range (10):\n",
    "    print '%20s%10s' % (toplist[i][0], toplist[i][1]),'%20s' % '', '%20s%10s' % (bottomlist[i][0], bottomlist[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"reducer.py\", line 49\n",
      "    print '%25s' %'TOP 10', '%25s' % '', '%28s' %'BOTTOM 10'\n",
      "               ^\n",
      "SyntaxError: Missing parentheses in call to 'print'\n",
      "  File \"mapper.py\", line 27\n",
      "    print '%s\\t%s' % (word, 1)\n",
      "                 ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!echo \"10 \\n 10\\n  5\\n 6\\n 8\\n 9 \\n 10 \\n 9 \\n 12 \\n 21 \\n 22 \\n 23 \\n 24 \\n 25\" | python mapper.py | sort -k1,1 | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run numcount in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>start yarn  and hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 12434. Stop it first.\n",
      "localhost: nodemanager running as process 13130. Stop it first.\n",
      "16/01/25 20:32:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 13875. Stop it first.\n",
      "localhost: datanode running as process 14563. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 15278. Stop it first.\n",
      "16/01/25 20:33:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 20:33:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 20:33:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 20:33:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                         ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload numcount.txt to hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 20:33:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put numcount.txt /user/venamax                 #### save source data file to hdfs            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar hadoopstreamingjarfile \\\n",
    "\n",
    "    -D stream.num.map.output.key.fields=n \\\n",
    "    -mapper mapperfile \\\n",
    "    -reducer reducerfile \\\n",
    "    -input inputfile \\\n",
    "    -output outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 20:33:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 20:33:39 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 20:33:39 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 20:33:39 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 20:33:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 20:33:39 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 20:33:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2145711988_0001\n",
      "16/01/25 20:33:39 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 20:33:39 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 20:33:39 INFO mapreduce.Job: Running job: job_local2145711988_0001\n",
      "16/01/25 20:33:39 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 20:33:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 20:33:39 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 20:33:39 INFO mapred.LocalJobRunner: Starting task: attempt_local2145711988_0001_m_000000_0\n",
      "16/01/25 20:33:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 20:33:40 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 20:33:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/numcount.txt:0+29121\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 20:33:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 20:33:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 20:33:40 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 20:33:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:40 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:40 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:40 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:40 INFO mapreduce.Job: Job job_local2145711988_0001 running in uber mode : false\n",
      "16/01/25 20:33:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: \n",
      "16/01/25 20:33:41 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 20:33:41 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 20:33:41 INFO mapred.MapTask: bufstart = 0; bufend = 49121; bufvoid = 104857600\n",
      "16/01/25 20:33:41 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "16/01/25 20:33:41 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 20:33:41 INFO mapred.Task: Task:attempt_local2145711988_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "16/01/25 20:33:41 INFO mapred.Task: Task 'attempt_local2145711988_0001_m_000000_0' done.\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: Finishing task: attempt_local2145711988_0001_m_000000_0\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: Starting task: attempt_local2145711988_0001_r_000000_0\n",
      "16/01/25 20:33:41 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 20:33:41 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 20:33:41 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 20:33:41 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3e3345d0\n",
      "16/01/25 20:33:41 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 20:33:41 INFO reduce.EventFetcher: attempt_local2145711988_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 20:33:41 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2145711988_0001_m_000000_0 decomp: 69123 len: 69127 to MEMORY\n",
      "16/01/25 20:33:41 INFO reduce.InMemoryMapOutput: Read 69123 bytes from map-output for attempt_local2145711988_0001_m_000000_0\n",
      "16/01/25 20:33:41 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 69123, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->69123\n",
      "16/01/25 20:33:41 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 20:33:41 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 20:33:41 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 20:33:41 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 69119 bytes\n",
      "16/01/25 20:33:41 INFO reduce.MergeManagerImpl: Merged 1 segments, 69123 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 20:33:41 INFO reduce.MergeManagerImpl: Merging 1 files, 69127 bytes from disk\n",
      "16/01/25 20:33:41 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 20:33:41 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 20:33:41 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 69119 bytes\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/25 20:33:41 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 20:33:41 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/25 20:33:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 20:33:41 INFO mapred.Task: Task:attempt_local2145711988_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 20:33:41 INFO mapred.Task: Task attempt_local2145711988_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 20:33:41 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2145711988_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/numcountOutput/_temporary/0/task_local2145711988_0001_r_000000\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "16/01/25 20:33:41 INFO mapred.Task: Task 'attempt_local2145711988_0001_r_000000_0' done.\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: Finishing task: attempt_local2145711988_0001_r_000000_0\n",
      "16/01/25 20:33:41 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 20:33:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 20:33:41 INFO mapreduce.Job: Job job_local2145711988_0001 completed successfully\n",
      "16/01/25 20:33:41 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=350078\n",
      "\t\tFILE: Number of bytes written=1007373\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=58242\n",
      "\t\tHDFS: Number of bytes written=1008\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=49121\n",
      "\t\tMap output materialized bytes=69127\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=69127\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=547356672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=29121\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1008\n",
      "16/01/25 20:33:41 INFO streaming.StreamJob: Output directory: numcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input numcount.txt -output numcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 20:33:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                   TOP 10                                              BOTTOM 10\t\n",
      "              Number      Count                                    Number      Count\t\n",
      "                  77       125                                         9        76\t\n",
      "                  42       122                                        88        77\t\n",
      "                  90       117                                        70        83\t\n",
      "                  26       116                                        15        85\t\n",
      "                  80       115                                        35        85\t\n",
      "                  14       114                                        41        85\t\n",
      "                  86       113                                        59        85\t\n",
      "                  45       112                                        73        85\t\n",
      "                  53       112                                         7        86\t\n",
      "                  13       111                                        23        87\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat numcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>stop yarn and hdfs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 21:13:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 21:13:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.2.  WORDCOUNT</h2>\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "       #NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = 'enronemail_1h.txt'               \n",
    "findwords = ['assistance', 'commercial', 'intelligence','call']\n",
    "counts = {}\n",
    "##filename = sys.argv[1]\n",
    "##findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "for keyword in findwords:  ### Initialize to zero all keywords to find\n",
    "    counts[keyword] = 0\n",
    "\n",
    "\n",
    "## open the input file\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line) \n",
    "        if len(record) == 4:                             ### Take only complete records\n",
    "            for keyword in findwords:                    ### For each word to find\n",
    "                for i in range (2,len(record)):          ### Compare it to the words from each email\n",
    "                    bagofwords = re.split(\" \",record[i]) ### Break each email records into words\n",
    "                    for word in bagofwords:\n",
    "                        neword = word.strip(',')         ### eliminate comas  \n",
    "                        if keyword in neword:\n",
    "                            counts[keyword] += 1\n",
    "\n",
    "\n",
    "for keyword in findwords:                                 ### output results in the form:                   \n",
    "    print ('%s\\t%s'% (keyword, str(counts[keyword])))     ### word to find, count\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "words = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mappe\n",
    "    word = line[0]\n",
    "    \n",
    "    try:\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_word == word:                     ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "    else:\n",
    "        if current_word:\n",
    "            \n",
    "            words[current_word] = current_count  ### store tuple in a list once totalize count per number\n",
    "  \n",
    "        current_count = count                    ### set current count\n",
    "        current_word = word                      ### set current word\n",
    "\n",
    "\n",
    "if current_word == word:                         ### do not forget to output the last word if needed!\n",
    "    words[current_word] = current_count \n",
    "\n",
    "for word in words:\n",
    "    print ('We found %s' %word, ' on %s'%words[word] , 'occassions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run numcount in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>start yarn  and hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 12434. Stop it first.\n",
      "localhost: nodemanager running as process 13130. Stop it first.\n",
      "16/01/25 16:46:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 13875. Stop it first.\n",
      "localhost: datanode running as process 14563. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 15278. Stop it first.\n",
      "16/01/25 16:46:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:23:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:23:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:23:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                         ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload enronemail_1h.txt to hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:23:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/venamax            #### save source data file to hdfs         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:23:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:23:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 19:23:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 19:23:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 19:23:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 19:23:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 19:23:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local722687238_0001\n",
      "16/01/25 19:23:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 19:23:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 19:23:49 INFO mapreduce.Job: Running job: job_local722687238_0001\n",
      "16/01/25 19:23:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 19:23:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 19:23:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 19:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local722687238_0001_m_000000_0\n",
      "16/01/25 19:23:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 19:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 19:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/enronemail_1h.txt:0+203981\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 19:23:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 19:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 19:23:49 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 19:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:23:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:23:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 19:23:50 INFO streaming.PipeMapRed: R/W/S=73/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "minRecWrittenToEnableSkip_=9223372036854775807 HOST=null\n",
      "USER=venamax\n",
      "HADOOP_USER=null\n",
      "last tool output: |assistance\t9|\n",
      "\n",
      "java.io.IOException: Broken pipe\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/01/25 19:23:50 INFO streaming.PipeMapRed: Records R/W=73/1\n",
      "16/01/25 19:23:50 WARN streaming.PipeMapRed: java.io.IOException: Stream closed\n",
      "16/01/25 19:23:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 19:23:50 WARN streaming.PipeMapRed: java.io.IOException: Stream closed\n",
      "16/01/25 19:23:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 19:23:50 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 19:23:50 WARN mapred.LocalJobRunner: job_local722687238_0001\n",
      "java.lang.Exception: java.io.IOException: Broken pipe\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\n",
      "Caused by: java.io.IOException: Broken pipe\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/01/25 19:23:50 INFO mapreduce.Job: Job job_local722687238_0001 running in uber mode : false\n",
      "16/01/25 19:23:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/25 19:23:50 INFO mapreduce.Job: Job job_local722687238_0001 failed with state FAILED due to: NA\n",
      "16/01/25 19:23:50 INFO mapreduce.Job: Counters: 0\n",
      "16/01/25 19:23:50 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output wordcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>stop yarn and hdfs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 15:11:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 15:11:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        line = line.strip()\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "            for i in range (2,len(record)):                ### Starting from Subject to the Content               \n",
    "                bagofwords = re.split(\" \" | \",\" ,record[i])### Collect all words present on each email                \n",
    "                for word in bagofwords: \n",
    "                    flag=0\n",
    "                    if word in findwords:\n",
    "                        flag=1                    \n",
    "                    print '%s\\t%s\\t%s\\t%s\\t%s' % (word, 1,record[0], record[1],flag) \n",
    "                                                            ### output: word, 1, id, spam truth and flag\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "\n",
    "current_word, word = None, None                 \n",
    "current_wordcount, current_spam_wordcount, current_ham_wordcount = 0,0,0\n",
    "current_id, record_id = None, None          \n",
    "current_y_true, y_true = None, None\n",
    "current_flag, flag = None,None\n",
    "\n",
    "sum_records, sum_spamrecords, sum_hamrecords = 0,0,0\n",
    "sum_spamwords, sum_hamwords = 0,0\n",
    "flagged_words = []\n",
    "\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    "\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    word = line[0]                             ### word we get from mapper.py\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "        email = line[2]                         ### id that identifies each email\n",
    "        y_true = line[3]\n",
    "        y_true = int(y_true)                    ### spam truth as an integer\n",
    "        flag = line[4]\n",
    "        flag = int(flag)                        ### flags if word is in the user specified list\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_word == word:                    ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: word) before it is passed to the reducer\n",
    "        \n",
    "        if current_word not in words.keys():\n",
    "            words[current_word]={'ham_count':0,'spam_count':0,'flag':flag}\n",
    "        if email not in emails.keys():\n",
    "            emails[current_email]={'y_true':y_true,'word_count':0,'words':[]}\n",
    "            sum_records +=1\n",
    "            if y_true == 1:\n",
    "                sum_spamrecords +=1\n",
    "            else\n",
    "                sum_hamrecords +=1\n",
    "    \n",
    "        if y_true == 1:                         ### if record where word is located is a spam\n",
    "            current_spamcount += count         ### add to spam count of that word\n",
    "            sum_spamwords += 1\n",
    "        else:\n",
    "            current_hamcount += count          ### if not add to ham count of thet word\n",
    "            sum_hamwords +=1 \n",
    "\n",
    "        emails[current_email]['word_count'] += 1\n",
    "        emails[current_email]['words'].append(current_word)### store words in email      \n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        if current_word:\n",
    "            if flag==1 and current_word not in flagged_words:\n",
    "                flagged_words.append(current_word)                \n",
    "            \n",
    "            words[current_word]['flag'] = flag  ### denote if current word is a word specified by the user list\n",
    "            words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "            words[current_word]['ham_count'] += current_hamcount ### update ham count for current word\n",
    "\n",
    "  \n",
    "            \n",
    "\n",
    "        current_count = count                   ### set current count\n",
    "        current_spamcount, current_hamcount = 0,0 ### initialize spam and ham wordcount\n",
    "        current_word = word                     ### set current number\n",
    "        current_email = email                   ### set current id of email\n",
    "        current_y_true = y_true                 ### set current spam truth\n",
    "        current_flag = flag                     ### set current flag \n",
    "        \n",
    "\n",
    "\n",
    "if current_word == word:                       ### do not forget to output the last word if needed!\n",
    "    emails[current_email]['word_count'] += 1\n",
    "    emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    words[current_word]['flag'] = flag  ### denote if current word is a word specified by the user list\n",
    "    words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "    words[current_word]['ham_count'] += current_hamcount ### update ham count for current word    \n",
    "    \n",
    "\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "prior_spam= sum_spamrecords/sum_records     \n",
    "prior_ham=sum_hamrecords/sum_records\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "            \n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(sum_spamwords)\n",
    "    word['p_ham']=(word['ham_count'])/(sum_hamwords)\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS', '%20s' %'CUMULATIVE ACCURACY'\n",
    "miss, sample_size = 0,0 \n",
    "\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    #p_spam=log(prior_spam)\n",
    "    #p_ham=log(prior_ham)\n",
    "    \n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "        if word in flagged_words:\n",
    "            try:\n",
    "                #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "                p_spam*=words[word]['p_spam']\n",
    "            except ValueError:\n",
    "                pass #This means that words that do not appear in a class will use the class prior\n",
    "            try:\n",
    "                #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "                p_ham*=words[word]['p_ham']\n",
    "            except ValueError:\n",
    "                pass\n",
    "    if p_spam>p_ham:\n",
    "        y_pred=1\n",
    "    else:\n",
    "        y_pred=0\n",
    "        \n",
    "    y_true = email['y_true']\n",
    "    if y_pred != y_true:\n",
    "        miss+= 1.0\n",
    "        \n",
    "    sample_size += 1.0\n",
    "    accuracy = ((sample_size-miss)/sample_size)*100\n",
    "                \n",
    "    print  '%30s' %email, '%10d' %y_true, '%10d' %y_pred, '%18.2f %%' % accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run Hadoop MapReduce Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> start up yarn and dfs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                       ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload enronmail_1h.txt file </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/venamax                 #### save source data file to hdfs     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input numcount.txt -output numcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.2.1</h2>  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.3. Multinomial NAIVE BAYES with NO Smoothing</h2>\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the log posterior probabilities (i.e., Pr(Class|Doc))) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.4 Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. </h2>\n",
    "Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.5. Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. </h2>How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm</h2>\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HHW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)</h2>\n",
    "—  Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "-  Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.7 OPTIONAL (note this exercise is a stretch HW and optional)</h2>\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "--- Line 1 contains the subject\n",
    "--- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.8 OPTIONAL</h2>\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    " -- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    " -- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.8.1 OPTIONAL</h2>\n",
    "—  Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "-  Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HOMEWORK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
