{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b>Homework 2</b></h1>\n",
    "<i>Alejandro J. Rojas<br>\n",
    "ale@ischool.berkeley.edu<br>\n",
    "W261: Machine Learning at Scale<br>\n",
    "Week: 02<br>\n",
    "Jan 26, 2016, 1:15 PM</i></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.0.  </h2>\n",
    "What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Race conditions </h2>\n",
    "Race conditions essentially refers to situations where the results of one process is dependant on the order of when other processes are executed. For instance any processes that involve transactions are usually in this category and they're managed by establishing locks in databases so that transactions are updated against the locked database and executed in order and not simultaneously.  In the context of paralalization when many processes occur simultaneosly, it is important to understand which processes can be embarassingly parallel and which can't so that we design data architecture to deal accordingly.\n",
    "\n",
    "<h2> Map Reduce </h2>\n",
    "Method essentially developed by Google in which processes run embarassingly parallel following two major steps. In the map stage, data is broken into chunks and mapped to a specific output. In the reduce stage, data from chunks is consolidated and a summary of that data is output.\n",
    "\n",
    "<h2> Hadoop </h2>\n",
    "Hadoop is an open source project that was developed to not only implement the Map Reduce programming model but also to serve as a distributed file system of storage under a paradigm of parallel computing running on a great number of 'cheap' servers that are prone to fail. This distributed files system provides fault tolerance against those failures by implementing replicates of all data. Data is broken into chunks that are replicated usually 3 times across a cluster of computer nodes.\n",
    "\n",
    "A simplified example of Hadoop can be as follows:\n",
    "\n",
    "Imagine a very viral video that is published on Facebook. This is a 1280 x 720 high resolution video that lasts about 5 minutes and it's a file of about 350 MB in size. Facebook stores the video as an object that is broken into 64MB chunks resulting roughly in about one chunk for everey minute of video. Each chunk is replicated by Facebook at least 3 times across its clusters of computer nodes.Facebook also serves this combined file on multiple servers depending on where the user wants to watch the video. If a user \"likes\" the video, then this information is attached to a log that is stored in the server where the like was collected. A \"Map\" job then assigns likes, comments, and shares to that video using a json format on each server where the video is served. A \"Reduce\" job consolidates all jsons from all the computer nodes were the video was served to then totalize and come up with the number of \"Likes\" that the video shows.\n",
    "\n",
    "In pseudocode we can see the following:\n",
    "\n",
    "User \"likes\" video object v\n",
    "\n",
    "<i>Map job </i>\n",
    "\n",
    "    inputs:\n",
    "        object_v = {object_id: video, publisher_id: publisher, date_of_publishing: date, ....}\n",
    "        user = {user_id: unique identifier, name: user name, lastname: user lastname, ...} \n",
    "    function get_like(object_id, user_id):\n",
    "    object_v[like]=[user]\n",
    "    \n",
    "        return object_v\n",
    "\n",
    "<i>Reduce job</i>\n",
    "    \n",
    "    function sum_likes(object_v):\n",
    "        for node in nodes:\n",
    "            for like in object_v:\n",
    "                sum_likes = sum_likes +1\n",
    "            \n",
    "         return sum_likes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.1. Sort in Hadoop MapReduce</h2>\n",
    "Given as input: Records of the form '<'integer, “NA”'>', where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form '<'integer, “NA”'>' in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form '<'integer, “NA”'>'. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "N = 10000                           ### for a sample size of N\n",
    "random.seed(0)                      ### pick a random seed to replicate results\n",
    "\n",
    "\n",
    "input_file = open(\"numcount.txt\", \"w\") # writing file\n",
    "\n",
    "for i in range(N):\n",
    "    a = random.randint(0, 100)       ### Select a random integer from 0 to 100\n",
    "    b = ''\n",
    "    input_file.write(str(a))         \n",
    "    input_file.write(b)\n",
    "    input_file.write('\\n')\n",
    "\n",
    "   \n",
    "\n",
    "input_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:                         ### input comes from STDIN (standard input)\n",
    "    \n",
    "    number = line.strip()                      ### remove leading and trailing whitespace\n",
    "    print ('%s\\t%s' % (number, 1))             ### mapper out looks like 'number' \\t 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_number = None\n",
    "current_count = 0\n",
    "number = None\n",
    "numlist = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    number = line[0]                           ### integer generated randomly we got from mapper.py\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_number == number:                ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "    else:\n",
    "        if current_number:\n",
    "            \n",
    "            numlist.append((current_number,current_count))  ### store tuple in a list once totalize count per number\n",
    "\n",
    "        current_count = count                   ### set current count\n",
    "        current_number = number                 ### set current number\n",
    "\n",
    "\n",
    "if current_number == number:                    ### do not forget to output the last word if needed!\n",
    "    numlist.append((current_number,current_count))\n",
    "\n",
    "\n",
    "toplist = sorted(numlist,key=lambda record: record[1], reverse=True) ### sort list from largest count to smallest\n",
    "bottomlist = sorted(numlist,key=lambda record: record[1])  ### sort list from smalles to largest\n",
    "\n",
    "print '%25s' %'TOP 10', '%25s' % '', '%28s' %'BOTTOM 10'\n",
    "print '%20s' %'Number', '%10s' %'Count', '%20s' % '', '%20s' %'Number','%10s' %'Count'\n",
    "for i in range (10):\n",
    "    print '%20s%10s' % (toplist[i][0], toplist[i][1]),'%20s' % '', '%20s%10s' % (bottomlist[i][0], bottomlist[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run numcount in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>start yarn  and hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-venamax-resourcemanager-VENAMAX.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-venamax-nodemanager-VENAMAX.local.out\n",
      "16/01/24 11:33:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-venamax-namenode-VENAMAX.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-venamax-datanode-VENAMAX.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-venamax-secondarynamenode-VENAMAX.local.out\n",
      "16/01/24 11:34:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 21:51:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 21:51:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 21:51:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                         ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload numcount.txt to hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 11:42:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put numcount.txt /user/venamax                 #### save source data file to hdfs            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar hadoopstreamingjarfile \\\n",
    "\n",
    "    -D stream.num.map.output.key.fields=n \\\n",
    "    -mapper mapperfile \\\n",
    "    -reducer reducerfile \\\n",
    "    -input inputfile \\\n",
    "    -output outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 11:42:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 11:42:51 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 11:42:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 11:42:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 11:42:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 11:42:52 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 11:42:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1147326746_0001\n",
      "16/01/24 11:42:53 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 11:42:53 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 11:42:53 INFO mapreduce.Job: Running job: job_local1147326746_0001\n",
      "16/01/24 11:42:53 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 11:42:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 11:42:53 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 11:42:53 INFO mapred.LocalJobRunner: Starting task: attempt_local1147326746_0001_m_000000_0\n",
      "16/01/24 11:42:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 11:42:53 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 11:42:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/numcount.txt:0+29121\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 11:42:53 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 11:42:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 11:42:53 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 11:42:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:53 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:54 INFO mapreduce.Job: Job job_local1147326746_0001 running in uber mode : false\n",
      "16/01/24 11:42:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: \n",
      "16/01/24 11:42:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 11:42:55 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 11:42:55 INFO mapred.MapTask: bufstart = 0; bufend = 49121; bufvoid = 104857600\n",
      "16/01/24 11:42:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "16/01/24 11:42:55 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 11:42:55 INFO mapred.Task: Task:attempt_local1147326746_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "16/01/24 11:42:55 INFO mapred.Task: Task 'attempt_local1147326746_0001_m_000000_0' done.\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local1147326746_0001_m_000000_0\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: Starting task: attempt_local1147326746_0001_r_000000_0\n",
      "16/01/24 11:42:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 11:42:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 11:42:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 11:42:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@32812234\n",
      "16/01/24 11:42:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 11:42:55 INFO reduce.EventFetcher: attempt_local1147326746_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 11:42:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1147326746_0001_m_000000_0 decomp: 69123 len: 69127 to MEMORY\n",
      "16/01/24 11:42:55 INFO reduce.InMemoryMapOutput: Read 69123 bytes from map-output for attempt_local1147326746_0001_m_000000_0\n",
      "16/01/24 11:42:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 69123, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->69123\n",
      "16/01/24 11:42:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 11:42:55 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 11:42:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 11:42:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 11:42:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 69119 bytes\n",
      "16/01/24 11:42:55 INFO reduce.MergeManagerImpl: Merged 1 segments, 69123 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 11:42:55 INFO reduce.MergeManagerImpl: Merging 1 files, 69127 bytes from disk\n",
      "16/01/24 11:42:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 11:42:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 11:42:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 69119 bytes\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/24 11:42:55 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 11:42:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/24 11:42:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 11:42:55 INFO mapred.Task: Task:attempt_local1147326746_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 11:42:55 INFO mapred.Task: Task attempt_local1147326746_0001_r_000000_0 is allowed to commit now\n",
      "16/01/24 11:42:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1147326746_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/numcountOutput/_temporary/0/task_local1147326746_0001_r_000000\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "16/01/24 11:42:55 INFO mapred.Task: Task 'attempt_local1147326746_0001_r_000000_0' done.\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local1147326746_0001_r_000000_0\n",
      "16/01/24 11:42:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 11:42:56 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 11:42:56 INFO mapreduce.Job: Job job_local1147326746_0001 completed successfully\n",
      "16/01/24 11:42:56 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=350072\n",
      "\t\tFILE: Number of bytes written=1007367\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=58242\n",
      "\t\tHDFS: Number of bytes written=1008\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=49121\n",
      "\t\tMap output materialized bytes=69127\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=69127\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=551550976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=29121\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1008\n",
      "16/01/24 11:42:56 INFO streaming.StreamJob: Output directory: numcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input numcount.txt -output numcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 11:43:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                   TOP 10                                              BOTTOM 10\t\n",
      "              Number      Count                                    Number      Count\t\n",
      "                  77       125                                         9        76\t\n",
      "                  42       122                                        88        77\t\n",
      "                  90       117                                        70        83\t\n",
      "                  26       116                                        15        85\t\n",
      "                  80       115                                        35        85\t\n",
      "                  14       114                                        41        85\t\n",
      "                  86       113                                        59        85\t\n",
      "                  45       112                                        73        85\t\n",
      "                  53       112                                         7        86\t\n",
      "                  13       111                                        23        87\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat numcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>stop yarn and hdfs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 21:13:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 21:13:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.2.  WORDCOUNT</h2>\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "       #NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = 'enronemail_1h.txt'               \n",
    "findwords = ['assistance']\n",
    "counts = {}\n",
    "##filename = sys.argv[1]\n",
    "##findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "\n",
    "for keyword in findwords:  ### Initialize to zero all keywords to find\n",
    "    counts[keyword] = 0\n",
    "\n",
    "for line in sys.stdin:                         ### input comes from STDIN (standard input)\n",
    "    record = re.split(r'\\t+', line) \n",
    "    if len(record) == 4:                             ### Take only complete records\n",
    "        for keyword in findwords:                    ### For each word to find\n",
    "            for i in range (2,len(record)):          ### Compare it to the words from each email\n",
    "                bagofwords = re.split(\" \",record[i]) ### Break each email records into words\n",
    "                for word in bagofwords:\n",
    "                    neword = word.strip(',')         ### eliminate comas  \n",
    "                    if keyword in neword:\n",
    "                        counts[keyword] += 1\n",
    "\n",
    "\n",
    "for keyword in findwords:                                 ### output results in the form:                   \n",
    "    print ('%s\\t%s'% (keyword, str(counts[keyword])))     ### word to find, count\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "words = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mappe\n",
    "    word = line[0]\n",
    "    \n",
    "    try:\n",
    "\n",
    "        count = line[1]\n",
    "        count = int(count)                      ### convert count (currently a string) to int\n",
    "    except ValueError:                          ### if count was not a number then silently                         \n",
    "        continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "    if current_word == word:                     ### this IF-switch only works because Hadoop sorts map output\n",
    "        current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "    else:\n",
    "        if current_word:\n",
    "            \n",
    "            words[current_word] = current_count  ### store tuple in a list once totalize count per number\n",
    "  \n",
    "        current_count = count                    ### set current count\n",
    "        current_word = word                      ### set current word\n",
    "\n",
    "\n",
    "if current_word == word:                         ### do not forget to output the last word if needed!\n",
    "    words[current_word] = current_count \n",
    "\n",
    "for word in words:\n",
    "    print 'We found \"%s\"' %word, ' on %s'%words[word] , 'occassions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run numcount in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>start yarn  and hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 12434. Stop it first.\n",
      "localhost: nodemanager running as process 13130. Stop it first.\n",
      "16/01/25 16:46:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 13875. Stop it first.\n",
      "localhost: datanode running as process 14563. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 15278. Stop it first.\n",
      "16/01/25 16:46:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:58:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:58:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:58:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                         ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload enronemail_1h.txt to hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:58:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/venamax            #### save source data file to hdfs         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:58:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:58:45 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 22:58:45 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 22:58:45 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 22:58:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 22:58:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 22:58:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1108989492_0001\n",
      "16/01/25 22:58:45 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 22:58:45 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 22:58:45 INFO mapreduce.Job: Running job: job_local1108989492_0001\n",
      "16/01/25 22:58:45 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 22:58:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 22:58:45 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 22:58:45 INFO mapred.LocalJobRunner: Starting task: attempt_local1108989492_0001_m_000000_0\n",
      "16/01/25 22:58:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 22:58:45 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 22:58:45 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/enronemail_1h.txt:0+203981\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 22:58:45 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 22:58:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 22:58:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 22:58:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 22:58:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 22:58:46 INFO mapreduce.Job: Job job_local1108989492_0001 running in uber mode : false\n",
      "16/01/25 22:58:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: \n",
      "16/01/25 22:58:47 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 22:58:47 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 22:58:47 INFO mapred.MapTask: bufstart = 0; bufend = 13; bufvoid = 104857600\n",
      "16/01/25 22:58:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "16/01/25 22:58:47 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 22:58:47 INFO mapred.Task: Task:attempt_local1108989492_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "16/01/25 22:58:47 INFO mapred.Task: Task 'attempt_local1108989492_0001_m_000000_0' done.\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108989492_0001_m_000000_0\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: Starting task: attempt_local1108989492_0001_r_000000_0\n",
      "16/01/25 22:58:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 22:58:47 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 22:58:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 22:58:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@45e6ddb1\n",
      "16/01/25 22:58:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 22:58:47 INFO reduce.EventFetcher: attempt_local1108989492_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 22:58:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1108989492_0001_m_000000_0 decomp: 17 len: 21 to MEMORY\n",
      "16/01/25 22:58:47 INFO reduce.InMemoryMapOutput: Read 17 bytes from map-output for attempt_local1108989492_0001_m_000000_0\n",
      "16/01/25 22:58:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 17, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->17\n",
      "16/01/25 22:58:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 22:58:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 22:58:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 22:58:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/25 22:58:47 INFO reduce.MergeManagerImpl: Merged 1 segments, 17 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 22:58:47 INFO reduce.MergeManagerImpl: Merging 1 files, 21 bytes from disk\n",
      "16/01/25 22:58:47 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 22:58:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 22:58:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/25 22:58:47 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 22:58:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/25 22:58:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 22:58:47 INFO mapred.Task: Task:attempt_local1108989492_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 22:58:47 INFO mapred.Task: Task attempt_local1108989492_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 22:58:47 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108989492_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/wordcountOutput/_temporary/0/task_local1108989492_0001_r_000000\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/01/25 22:58:47 INFO mapred.Task: Task 'attempt_local1108989492_0001_r_000000_0' done.\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108989492_0001_r_000000_0\n",
      "16/01/25 22:58:47 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 22:58:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 22:58:47 INFO mapreduce.Job: Job job_local1108989492_0001 completed successfully\n",
      "16/01/25 22:58:47 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=211878\n",
      "\t\tFILE: Number of bytes written=800091\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=41\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=13\n",
      "\t\tMap output materialized bytes=21\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=21\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=549453824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=41\n",
      "16/01/25 22:58:47 INFO streaming.StreamJob: Output directory: wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output wordcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:59:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "We found \"assistance\"  on 9 occassions.\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>stop yarn and hdfs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 15:11:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 15:11:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.2.1</h2>  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "             \n",
    "counts = {}\n",
    "\n",
    "for line in sys.stdin:                               ### input comes from STDIN (standard input)\n",
    "    record = re.split(r'\\t+', line) \n",
    "    if len(record) == 4:                             ### Take only complete records\n",
    "        for i in range (2,len(record)):              ###  words in subject and content from each email\n",
    "            bagofwords = re.split(\" \",record[i])     ### Break each email records into words\n",
    "            for word in bagofwords:\n",
    "                neword = word.strip(',')             ### eliminate comas and other clean up\n",
    "                neword = re.sub('^\\'+','',word)\n",
    "                neword = re.sub('^\\-+','',neword)\n",
    "                neword = re.sub('\\-+$','',neword)                    \n",
    "                neword = re.sub('\\'+','\\'',neword)\n",
    "                neword = re.sub('\\-+','-',neword)\n",
    "                if neword in counts.keys():           ### check if word has a prior count\n",
    "                    counts[neword] +=1                ### add one to the count\n",
    "                else:                                 ### if not\n",
    "                    counts[neword] = 1                ### initialize word to one\n",
    "\n",
    "for word in counts.keys():\n",
    "    print ('%s\\t%s'% (word, str(counts[word])))         ### word, count\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "words = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mappe\n",
    "    if len(line) == 2:\n",
    "        word = line[0]\n",
    "    \n",
    "        try:\n",
    "\n",
    "            count = line[1]\n",
    "            count = int(count)                      ### convert count (currently a string) to int\n",
    "        except ValueError:                          ### if count was not a number then silently                         \n",
    "            continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "        if current_word == word:                     ### this IF-switch only works because Hadoop sorts map output\n",
    "            current_count += count                  ### by key (here: number) before it is passed to the reducer\n",
    "        else:\n",
    "            if current_word:\n",
    "            \n",
    "                words[current_word] = current_count  ### store tuple in a list once totalize count per number\n",
    "  \n",
    "            current_count = count                    ### set current count\n",
    "            current_word = word                      ### set current word\n",
    "\n",
    "\n",
    "if current_word == word:                         ### do not forget to output the last word if needed!\n",
    "    words[current_word] = current_count \n",
    "\n",
    "\n",
    "\n",
    "topwords = sorted(words, key=words.__getitem__, reverse = True) ## sort \n",
    "topwordcount = [value for (key, value) in sorted(words.items(), reverse=True)]\n",
    "    \n",
    "for i in range(10):\n",
    "    word = topwords[i]\n",
    "    print 'We found \"%s\"' %word, ' on %s'%words[word] , 'occassions.'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:57:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 00:57:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:57:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                         ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload enronemail_1h.txt to hdfs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:57:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/venamax            #### save source data file to hdfs     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming command </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:57:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 00:57:42 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 00:57:42 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 00:57:42 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 00:57:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 00:57:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 00:57:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1864178331_0001\n",
      "16/01/26 00:57:42 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 00:57:42 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 00:57:42 INFO mapreduce.Job: Running job: job_local1864178331_0001\n",
      "16/01/26 00:57:42 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 00:57:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 00:57:42 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 00:57:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1864178331_0001_m_000000_0\n",
      "16/01/26 00:57:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 00:57:42 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 00:57:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 00:57:42 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/enronemail_1h.txt:0+203981\n",
      "16/01/26 00:57:42 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 00:57:43 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 00:57:43 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 00:57:43 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 00:57:43 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 00:57:43 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 00:57:43 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 00:57:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 00:57:43 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 00:57:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:57:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:57:43 INFO mapreduce.Job: Job job_local1864178331_0001 running in uber mode : false\n",
      "16/01/26 00:57:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 00:57:44 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: \n",
      "16/01/26 00:57:48 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 00:57:48 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 00:57:48 INFO mapred.MapTask: bufstart = 0; bufend = 75391; bufvoid = 104857600\n",
      "16/01/26 00:57:48 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26184528(104738112); length = 29869/6553600\n",
      "16/01/26 00:57:48 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 00:57:48 INFO mapred.Task: Task:attempt_local1864178331_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "16/01/26 00:57:48 INFO mapred.Task: Task 'attempt_local1864178331_0001_m_000000_0' done.\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: Finishing task: attempt_local1864178331_0001_m_000000_0\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: Starting task: attempt_local1864178331_0001_r_000000_0\n",
      "16/01/26 00:57:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 00:57:48 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 00:57:48 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 00:57:48 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5150e88d\n",
      "16/01/26 00:57:48 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 00:57:48 INFO reduce.EventFetcher: attempt_local1864178331_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 00:57:48 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1864178331_0001_m_000000_0 decomp: 90329 len: 90333 to MEMORY\n",
      "16/01/26 00:57:48 INFO reduce.InMemoryMapOutput: Read 90329 bytes from map-output for attempt_local1864178331_0001_m_000000_0\n",
      "16/01/26 00:57:48 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 90329, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->90329\n",
      "16/01/26 00:57:48 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 00:57:48 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 00:57:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 00:57:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90326 bytes\n",
      "16/01/26 00:57:48 INFO reduce.MergeManagerImpl: Merged 1 segments, 90329 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 00:57:48 INFO reduce.MergeManagerImpl: Merging 1 files, 90333 bytes from disk\n",
      "16/01/26 00:57:48 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 00:57:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 00:57:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90326 bytes\n",
      "16/01/26 00:57:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/26 00:57:48 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 00:57:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 00:57:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: Records R/W=7468/1\n",
      "16/01/26 00:57:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 00:57:49 INFO mapred.Task: Task:attempt_local1864178331_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 00:57:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 00:57:49 INFO mapred.Task: Task attempt_local1864178331_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 00:57:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1864178331_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/wordtopcountOutput/_temporary/0/task_local1864178331_0001_r_000000\n",
      "16/01/26 00:57:49 INFO mapred.LocalJobRunner: Records R/W=7468/1 > reduce\n",
      "16/01/26 00:57:49 INFO mapred.Task: Task 'attempt_local1864178331_0001_r_000000_0' done.\n",
      "16/01/26 00:57:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1864178331_0001_r_000000_0\n",
      "16/01/26 00:57:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 00:57:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 00:57:49 INFO mapreduce.Job: Job job_local1864178331_0001 completed successfully\n",
      "16/01/26 00:57:49 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=392502\n",
      "\t\tFILE: Number of bytes written=1071039\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=355\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=7468\n",
      "\t\tMap output bytes=75391\n",
      "\t\tMap output materialized bytes=90333\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7378\n",
      "\t\tReduce shuffle bytes=90333\n",
      "\t\tReduce input records=7468\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=14936\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=547356672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=355\n",
      "16/01/26 00:57:49 INFO streaming.StreamJob: Output directory: wordtopcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output wordtopcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:58:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "We found \"the\"  on 1201 occassions.\t\n",
      "We found \"to\"  on 888 occassions.\t\n",
      "We found \"and\"  on 607 occassions.\t\n",
      "We found \"of\"  on 534 occassions.\t\n",
      "We found \"a\"  on 506 occassions.\t\n",
      "We found \"in\"  on 399 occassions.\t\n",
      "We found \"your\"  on 379 occassions.\t\n",
      "We found \"you\"  on 375 occassions.\t\n",
      "We found \"@\"  on 360 occassions.\t\n",
      "We found \"for\"  on 353 occassions.\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordtopcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.2.1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.3. Multinomial NAIVE BAYES with NO Smoothing</h2>\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the log posterior probabilities (i.e., Pr(Class|Doc))) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "             \n",
    "counts = {}\n",
    "\n",
    "for line in sys.stdin:                               ### input comes from STDIN (standard input)\n",
    "    record = re.split(r'\\t+', line) \n",
    "    if len(record) == 4:                             ### Take only complete records\n",
    "        for i in range (2,len(record)):              ###  words in subject and content from each email\n",
    "            bagofwords = re.split(\" \",record[i])     ### Break each email records into words\n",
    "            for word in bagofwords:\n",
    "                neword = word.strip(',')             ### eliminate comas and other clean up\n",
    "                neword = re.sub('^\\'+','',word)\n",
    "                neword = re.sub('^\\-+','',neword)\n",
    "                neword = re.sub('\\-+$','',neword)   \n",
    "                neword = re.sub('\\!+$','',neword)  \n",
    "                neword = re.sub('^\\!+','',neword)  \n",
    "                neword = re.sub('\\'+','\\'',neword)\n",
    "                neword = re.sub('\\-+','-',neword)\n",
    "                print '%s\\t%s\\t%s\\t%s' % (neword, 1,record[0], record[1]) \n",
    "                                                     ### output: word, 1, id, spam truth\n",
    " \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "from itertools import groupby\n",
    "import math\n",
    "\n",
    "\n",
    "current_word, word = None, None                 \n",
    "current_email, email = None, None          \n",
    "current_y_true, y_true = None, None\n",
    "\n",
    "\n",
    "sum_records, sum_spamrecords, sum_hamrecords = 0,0,0\n",
    "sum_spamwords, sum_hamwords = 0,0\n",
    "spam_zeroes, ham_zeroes = 0,0\n",
    "\n",
    "\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    "conf_matrix = np.zeros((2,2))\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    if len(line) == 4:\n",
    "    \n",
    "        try:\n",
    "            word = line[0]                          ### word we get from mapper.py\n",
    "            count = line[1]                         ### corresponding count\n",
    "            count = int(count)                      ### convert count (currently a string) to int\n",
    "            email = line[2]                         ### id that identifies each email\n",
    "            y_true = line[3]                        ### spam truth\n",
    "            y_true = int(y_true)                    ### spam truth as an integer\n",
    "        except ValueError:                          ### any error then silently                         \n",
    "            continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "        if current_word == word:                    ### this IF-switch only works because Hadoop sorts map output\n",
    "            current_count += count                  ### by key (here: word) before it is passed to the reducer\n",
    "        \n",
    "            if current_word not in words.keys():     ### initialize word count of new words\n",
    "                words[current_word]={'ham_count':0,'spam_count':0}\n",
    "            \n",
    "            if current_email not in emails.keys():          ### initialize word counts and label of new email\n",
    "                emails[current_email]={'y_true':current_y_true,'word_count':0,'words':[]} \n",
    "                sum_records +=1.0\n",
    "                if current_y_true == 1:                     ### identified new email as either spam or ham\n",
    "                    sum_spamrecords +=1.0               \n",
    "\n",
    "            elif current_email in emails.keys():\n",
    "                emails[current_email]['word_count'] += 1\n",
    "                emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    \n",
    "            if current_y_true == 1:                         ### if record where word is located is a spam\n",
    "                current_spamcount += count         ### add to spam count of that word\n",
    "                sum_spamwords += 1.0\n",
    "            else:\n",
    "                current_hamcount += count          ### if not add to ham count of thet word\n",
    "                sum_hamwords +=1.0 \n",
    "\n",
    "        else:\n",
    "            if current_word:\n",
    "                try:\n",
    "                    if current_word in words.keys():\n",
    "                        words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "                        words[current_word]['ham_count'] += current_hamcount ### update ham count for current word\n",
    "                except ValueError:                          ### if count was not a number then silently                         \n",
    "                    continue                                ### ignore/discard this line\n",
    "                    \n",
    "                    \n",
    "            current_count = count                   ### set current count\n",
    "            current_word = word                     ### set current number\n",
    "            current_email = email                   ### set current id of email\n",
    "            current_y_true = y_true                 ### set current spam truth\n",
    "            current_spamcount, current_hamcount = 0,0\n",
    "        \n",
    "\n",
    "\n",
    "if current_word == word:                       ### do not forget to output the last word if needed!\n",
    "    emails[current_email]['word_count'] += 1\n",
    "    emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "    words[current_word]['ham_count'] += current_hamcount ### update ham count for current word    \n",
    "    \n",
    "\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "\n",
    "prior_spam= sum_spamrecords/sum_records     \n",
    "prior_ham= 1 - prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "prob_spamwords = sum_spamwords/(sum_spamwords+sum_hamwords)\n",
    "prob_hamwords = 1-prob_spamwords\n",
    "\n",
    "\n",
    "\n",
    "for k,word in words.iteritems():\n",
    "    #These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    #word['p_spam']=(word['spam_count']+1)/(spam_word_count+vocab_count)\n",
    "    #word['p_ham']=(word['ham_count']+1)/(ham_word_count+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    word['p_spam']=(word['spam_count'])/(sum_spamwords)\n",
    "    word['p_ham']=(word['ham_count'])/(sum_hamwords)\n",
    "    \n",
    "    if word['p_spam'] == 0 and word['p_ham'] == 0:\n",
    "        spam_zeroes +=1\n",
    "        ham_zeroes +=1\n",
    "        word['p_spam']= 1\n",
    "        word['p_ham'] = 1\n",
    "    \n",
    "    elif word['p_spam'] == 0:\n",
    "        spam_zeroes +=1\n",
    "        word['p_ham'] = min(0.5 ** (1/word['ham_count']),0.99999999) ### back-calculates probability \n",
    "        word['p_spam']= 1-word['p_ham']              ### of the sequence of events assuming  \n",
    "                                                     ### sequence is 50% likely to happen         \n",
    "    elif word['p_ham'] == 0:\n",
    "        ham_zeroes +=1\n",
    "        word['p_spam'] = min(0.5 ** (1/word['spam_count']),0.99999999) ### Adjust probability \n",
    "        word['p_ham']= 1-word['p_spam']               ### just as above\n",
    "    \n",
    "    word['p_spam'] = math.log(word['p_spam'])              ### convert to log scale\n",
    "    word['p_ham'] = math.log(word['p_ham'])               \n",
    "        \n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "\n",
    "print 'Analyzed %s' % sum_records, 'using a vocabulary of %s' % vocab_count , 'terms'\n",
    "print 'On %s'%spam_zeroes , 'times we found spam probabilities that were zero'\n",
    "print 'On %s'%ham_zeroes, 'times we found not spam probabilities that were zero'\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS', '%20s' %'CUMULATIVE ACCURACY', '%10s'%'P-SPAM', '%10s'%'P-HAM'\n",
    "miss, sample_size = 0,0 \n",
    "prob_spam_list, prob_ham_list = [], []\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    p_spam= math.log(prior_spam)\n",
    "    p_ham= math.log(prior_ham)\n",
    "    \n",
    "    #p_spam=prior_spam\n",
    "    #p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "\n",
    "        try:\n",
    "            #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "            p_spam+=words[word]['p_spam']\n",
    "        except ValueError:\n",
    "            pass #This means that words that do not appear in a class will use the class prior\n",
    "        try:\n",
    "            #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "            p_ham+=words[word]['p_ham']\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if p_spam>p_ham:\n",
    "        y_pred=1\n",
    "        \n",
    "    else:\n",
    "        y_pred=0\n",
    "    y_true = email['y_true']\n",
    "    if y_true == 1:\n",
    "        prob_spam_list.append(math.exp(p_spam))       ### update probability list\n",
    "        if y_pred == 1:\n",
    "            conf_matrix[0][0] +=1                     ### update confusion matrix\n",
    "        else:\n",
    "            conf_matrix[0][1] +=1\n",
    "            \n",
    "    else:\n",
    "        prob_ham_list.append(math.exp(p_spam))\n",
    "        if y_pred == 0:\n",
    "            conf_matrix[1][1] +=1\n",
    "        else:\n",
    "            conf_matrix[1][0] +=1\n",
    "    \n",
    "    \n",
    "    if y_pred != y_true:\n",
    "        miss+= 1.0\n",
    "        \n",
    "    sample_size += 1.0\n",
    "    accuracy = ((sample_size-miss)/sample_size)*100\n",
    "    prob_spam = math.exp(p_spam)\n",
    "    prob_ham = math.exp(p_ham)\n",
    "    print  '%30s' %j, '%10d' %y_true, '%10d' %y_pred, '%18.2f %%' % accuracy, '%10.6f'%prob_spam, '%10.6f'%prob_ham\n",
    "\n",
    "print 'See confusion matrix. Top Left is predicted correctly as spam. Bottom right predicted correctly as ham:'\n",
    "print conf_matrix\n",
    "print 'Error rate: %4.2f'%(100-accuracy),'%%'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run Hadoop MapReduce Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> start up yarn and dfs </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh       ### start up yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh        ### start up dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> remove files from prior runs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:13:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 18:13:49 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/venamax\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/venamax                           ### remove prior files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> create folder </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:13:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/venamax                       ### create hdfs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> upload enronmail_1h.txt file </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:13:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/venamax                 #### save source data file to hdfs     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:14:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 18:14:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 18:14:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 18:14:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 18:14:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 18:14:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 18:14:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local226499316_0001\n",
      "16/01/26 18:14:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 18:14:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 18:14:07 INFO mapreduce.Job: Running job: job_local226499316_0001\n",
      "16/01/26 18:14:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 18:14:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:14:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 18:14:07 INFO mapred.LocalJobRunner: Starting task: attempt_local226499316_0001_m_000000_0\n",
      "16/01/26 18:14:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:14:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 18:14:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/enronemail_1h.txt:0+203981\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 18:14:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 18:14:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 18:14:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 18:14:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:07 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:07 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "16/01/26 18:14:08 INFO mapreduce.Job: Job job_local226499316_0001 running in uber mode : false\n",
      "16/01/26 18:14:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: \n",
      "16/01/26 18:14:08 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 18:14:08 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 18:14:08 INFO mapred.MapTask: bufstart = 0; bufend = 1172828; bufvoid = 104857600\n",
      "16/01/26 18:14:08 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26072600(104290400); length = 141797/6553600\n",
      "16/01/26 18:14:08 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 18:14:08 INFO mapred.Task: Task:attempt_local226499316_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "16/01/26 18:14:08 INFO mapred.Task: Task 'attempt_local226499316_0001_m_000000_0' done.\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local226499316_0001_m_000000_0\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: Starting task: attempt_local226499316_0001_r_000000_0\n",
      "16/01/26 18:14:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:14:08 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 18:14:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 18:14:08 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1b9cd035\n",
      "16/01/26 18:14:08 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 18:14:08 INFO reduce.EventFetcher: attempt_local226499316_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 18:14:08 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local226499316_0001_m_000000_0 decomp: 1243730 len: 1243734 to MEMORY\n",
      "16/01/26 18:14:08 INFO reduce.InMemoryMapOutput: Read 1243730 bytes from map-output for attempt_local226499316_0001_m_000000_0\n",
      "16/01/26 18:14:08 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1243730, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1243730\n",
      "16/01/26 18:14:08 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:14:08 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 18:14:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 18:14:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1243727 bytes\n",
      "16/01/26 18:14:08 INFO reduce.MergeManagerImpl: Merged 1 segments, 1243730 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 18:14:08 INFO reduce.MergeManagerImpl: Merging 1 files, 1243734 bytes from disk\n",
      "16/01/26 18:14:08 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 18:14:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 18:14:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1243727 bytes\n",
      "16/01/26 18:14:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/26 18:14:08 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 18:14:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 18:14:09 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:14:10 INFO streaming.PipeMapRed: Records R/W=35450/1\n",
      "16/01/26 18:14:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 18:14:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 18:14:10 INFO mapred.Task: Task:attempt_local226499316_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 18:14:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:14:10 INFO mapred.Task: Task attempt_local226499316_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 18:14:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local226499316_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/NBNoSmoothtOutput/_temporary/0/task_local226499316_0001_r_000000\n",
      "16/01/26 18:14:10 INFO mapred.LocalJobRunner: Records R/W=35450/1 > reduce\n",
      "16/01/26 18:14:10 INFO mapred.Task: Task 'attempt_local226499316_0001_r_000000_0' done.\n",
      "16/01/26 18:14:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local226499316_0001_r_000000_0\n",
      "16/01/26 18:14:10 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 18:14:11 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 18:14:11 INFO mapreduce.Job: Job job_local226499316_0001 completed successfully\n",
      "16/01/26 18:14:11 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2699304\n",
      "\t\tFILE: Number of bytes written=4528242\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=9732\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=35450\n",
      "\t\tMap output bytes=1172828\n",
      "\t\tMap output materialized bytes=1243734\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7275\n",
      "\t\tReduce shuffle bytes=1243734\n",
      "\t\tReduce input records=35450\n",
      "\t\tReduce output records=104\n",
      "\t\tSpilled Records=70900\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=545259520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9732\n",
      "16/01/26 18:14:11 INFO streaming.StreamJob: Output directory: NBNoSmoothtOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output NBNoSmoothtOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:14:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Analyzed 96.0 using a vocabulary of 3077 terms\t\n",
      "On 1330 times we found spam probabilities that were zero\t\n",
      "On 1747 times we found not spam probabilities that were zero\t\n",
      "                            ID      TRUTH      CLASS  CUMULATIVE ACCURACY     P-SPAM      P-HAM\t\n",
      "            0010.2003-12-18.GP          1          0               0.00 %   0.447917   0.552083\t\n",
      "     0010.2001-06-28.SA_and_HP          1          1              50.00 %   0.000000   0.000000\t\n",
      "          0001.2000-01-17.beck          0          0              66.67 %   0.000000   0.000017\t\n",
      "      0018.1999-12-14.kaminski          0          0              75.00 %   0.000000   0.276041\t\n",
      "      0005.1999-12-12.kaminski          0          0              80.00 %   0.000000   0.138021\t\n",
      "     0011.2001-06-29.SA_and_HP          1          1              83.33 %   0.000000   0.000000\t\n",
      "            0008.2004-08-01.BG          1          1              85.71 %   0.000000   0.000000\t\n",
      "        0009.1999-12-14.farmer          0          0              87.50 %   0.000000   0.034505\t\n",
      "            0017.2003-12-18.GP          1          1              88.89 %   0.447917   0.000000\t\n",
      "     0011.2001-06-28.SA_and_HP          1          1              90.00 %   0.000000   0.000000\t\n",
      "     0015.2001-07-05.SA_and_HP          1          1              90.91 %   0.001750   0.000000\t\n",
      "       0015.2001-02-12.kitchen          0          0              91.67 %   0.000000   0.000000\t\n",
      "      0017.1999-12-14.kaminski          0          0              92.31 %   0.000000   0.276042\t\n",
      "          0012.2000-01-17.beck          0          0              92.86 %   0.000000   0.552081\t\n",
      "          0003.2000-01-17.beck          0          0              93.33 %   0.000000   0.008626\t\n",
      "     0004.2001-06-12.SA_and_HP          1          1              93.75 %   0.013997   0.000000\t\n",
      "     0008.2001-06-12.SA_and_HP          1          1              94.12 %   0.006999   0.000000\t\n",
      "       0007.2001-02-09.kitchen          0          0              94.44 %   0.000000   0.002157\t\n",
      "            0016.2004-08-01.BG          1          1              94.74 %   0.055990   0.000000\t\n",
      "        0016.1999-12-15.farmer          0          0              95.00 %   0.000000   0.034505\t\n",
      "            0013.2004-08-01.BG          1          1              95.24 %   0.003499   0.000000\t\n",
      "            0005.2003-12-18.GP          1          1              95.45 %   0.000000   0.000000\t\n",
      "       0012.2001-02-09.kitchen          0          0              95.65 %   0.000000   0.138021\t\n",
      "        0011.1999-12-14.farmer          0          0              95.83 %   0.000000   0.004313\t\n",
      "       0009.2001-02-09.kitchen          0          0              96.00 %   0.000000   0.000000\t\n",
      "       0006.2001-02-08.kitchen          0          0              96.15 %   0.000000   0.000000\t\n",
      "            0014.2003-12-19.GP          1          0              92.59 %   0.223958   0.276042\t\n",
      "        0010.1999-12-14.farmer          0          0              92.86 %   0.000000   0.004313\t\n",
      "            0010.2004-08-01.BG          1          1              93.10 %   0.000014   0.000000\t\n",
      "      0014.1999-12-14.kaminski          0          0              93.33 %   0.000000   0.004313\t\n",
      "      0006.1999-12-13.kaminski          0          0              93.55 %   0.000000   0.034505\t\n",
      "        0005.1999-12-14.farmer          0          0              93.75 %   0.000000   0.008626\t\n",
      "       0003.2001-02-08.kitchen          0          0              93.94 %   0.000000   0.002157\t\n",
      "       0001.2001-02-07.kitchen          0          0              94.12 %   0.000000   0.008626\t\n",
      "       0008.2001-02-09.kitchen          0          0              94.29 %   0.000000   0.000000\t\n",
      "            0017.2004-08-02.BG          1          1              94.44 %   0.000001   0.000000\t\n",
      "            0007.2003-12-18.GP          1          1              94.59 %   0.001750   0.000000\t\n",
      "            0014.2004-08-01.BG          1          1              94.74 %   0.027995   0.000000\t\n",
      "            0006.2003-12-18.GP          1          1              94.87 %   0.003499   0.000000\t\n",
      "     0016.2001-07-05.SA_and_HP          1          1              95.00 %   0.001750   0.000000\t\n",
      "         0005.2000-06-06.lokay          0          0              95.12 %   0.000000   0.138021\t\n",
      "     0014.2001-07-04.SA_and_HP          1          1              95.24 %   0.000014   0.000000\t\n",
      "      0001.2001-04-02.williams          0          0              95.35 %   0.000000   0.001078\t\n",
      "         0012.2000-06-08.lokay          0          0              95.45 %   0.000000   0.069010\t\n",
      "        0014.1999-12-15.farmer          0          0              95.56 %   0.000000   0.002157\t\n",
      "         0009.2000-06-07.lokay          0          0              95.65 %   0.000000   0.000270\t\n",
      "        0001.1999-12-10.farmer          0          0              95.74 %   0.447917   0.552083\t\n",
      "     0008.2001-06-25.SA_and_HP          1          1              95.83 %   0.000219   0.000000\t\n",
      "      0017.2001-04-03.williams          0          0              95.92 %   0.000000   0.276042\t\n",
      "       0014.2001-02-12.kitchen          0          0              96.00 %   0.000000   0.001078\t\n",
      "     0016.2001-07-06.SA_and_HP          1          1              96.08 %   0.000000   0.000000\t\n",
      "        0015.1999-12-15.farmer          0          0              96.15 %   0.000000   0.000017\t\n",
      "      0004.1999-12-10.kaminski          0          0              96.23 %   0.000000   0.276042\t\n",
      "            0011.2004-08-01.BG          1          1              96.30 %   0.055990   0.000000\t\n",
      "            0004.2004-08-01.BG          1          1              96.36 %   0.111979   0.000000\t\n",
      "            0018.2003-12-18.GP          1          1              96.43 %   0.000001   0.000000\t\n",
      "        0007.1999-12-14.farmer          0          0              96.49 %   0.000000   0.001078\t\n",
      "            0016.2003-12-19.GP          1          1              96.55 %   0.000000   0.000000\t\n",
      "        0004.1999-12-14.farmer          0          0              96.61 %   0.000000   0.017253\t\n",
      "            0015.2003-12-19.GP          1          1              96.67 %   0.001750   0.000000\t\n",
      "            0006.2004-08-01.BG          1          1              96.72 %   0.013997   0.000000\t\n",
      "            0009.2003-12-18.GP          1          1              96.77 %   0.000000   0.000000\t\n",
      "        0002.1999-12-13.farmer          0          0              96.83 %   0.000000   0.000270\t\n",
      "            0008.2003-12-18.GP          1          1              96.88 %   0.027995   0.000000\t\n",
      "      0010.1999-12-14.kaminski          0          0              96.92 %   0.447917   0.552083\t\n",
      "          0007.2000-01-17.beck          0          0              96.97 %   0.000000   0.552082\t\n",
      "        0003.1999-12-14.farmer          0          0              97.01 %   0.000000   0.552083\t\n",
      "            0003.2004-08-01.BG          1          1              97.06 %   0.006999   0.000000\t\n",
      "            0017.2004-08-01.BG          1          1              97.10 %   0.223958   0.000000\t\n",
      "     0013.2001-06-30.SA_and_HP          1          1              97.14 %   0.000000   0.000000\t\n",
      "      0003.1999-12-10.kaminski          0          0              97.18 %   0.000000   0.276042\t\n",
      "        0012.1999-12-14.farmer          0          0              97.22 %   0.000000   0.000000\t\n",
      "      0009.1999-12-13.kaminski          0          0              97.26 %   0.000000   0.000135\t\n",
      "     0018.2001-07-13.SA_and_HP          1          1              97.30 %   0.000109   0.000000\t\n",
      "       0002.2001-02-07.kitchen          0          0              97.33 %   0.000000   0.138021\t\n",
      "            0007.2004-08-01.BG          1          1              97.37 %   0.003499   0.000000\t\n",
      "      0012.1999-12-14.kaminski          0          0              97.40 %   0.000000   0.069010\t\n",
      "     0005.2001-06-23.SA_and_HP          1          1              97.44 %   0.447917   0.000000\t\n",
      "      0013.1999-12-14.kaminski          0          0              97.47 %   0.000000   0.069010\t\n",
      "      0007.1999-12-13.kaminski          0          0              97.50 %   0.000000   0.034505\t\n",
      "          0017.2000-01-17.beck          0          0              97.53 %   0.000000   0.552082\t\n",
      "     0006.2001-06-25.SA_and_HP          1          1              97.56 %   0.223958   0.000000\t\n",
      "      0006.2001-04-03.williams          0          0              97.59 %   0.000000   0.552083\t\n",
      "       0005.2001-02-08.kitchen          0          0              97.62 %   0.000000   0.017253\t\n",
      "            0002.2003-12-18.GP          1          1              97.65 %   0.055989   0.000000\t\n",
      "            0003.2003-12-18.GP          1          1              97.67 %   0.111979   0.000000\t\n",
      "      0013.2001-04-03.williams          0          0              97.70 %   0.000000   0.138021\t\n",
      "      0004.2001-04-02.williams          0          0              97.73 %   0.000000   0.138021\t\n",
      "       0010.2001-02-09.kitchen          0          0              97.75 %   0.000000   0.000000\t\n",
      "        0013.1999-12-14.farmer          0          0              97.78 %   0.000000   0.000000\t\n",
      "      0015.1999-12-14.kaminski          0          0              97.80 %   0.000000   0.069010\t\n",
      "            0012.2003-12-19.GP          1          1              97.83 %   0.223958   0.000000\t\n",
      "       0016.2001-02-12.kitchen          0          0              97.85 %   0.000000   0.000135\t\n",
      "            0002.2004-08-01.BG          1          1              97.87 %   0.055990   0.000000\t\n",
      "     0002.2001-05-25.SA_and_HP          1          1              97.89 %   0.111979   0.000000\t\n",
      "            0011.2003-12-18.GP          1          1              97.92 %   0.055990   0.000000\t\n",
      "See confusion matrix. Top Left is predicted correctly as spam. Bottom right predicted correctly as ham:\t\n",
      "[[ 41.   2.]\t\n",
      " [  0.  53.]]\t\n",
      "Error rate: 2.08 %%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat NBNoSmoothtOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Report</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling each zero probability with a special treatment we reach an accuracy of 98%. Our special treatment was as follows:\n",
    "if a word shows a zero probability for any class, then count the number of times it appears in the other class. We can then back calculate the probability of each single event assuming that the probability of getting the sequence we observe is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a new figure and set the figsize argument so we get square-ish plots of the 4 features.\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "\n",
    "for prob in prob_spam_list:\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.hist(prob, 10)\n",
    "    plt.title('Probabilities of Spam Class')\n",
    "\n",
    "for prob in prob_ham_list:\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.hist(prob, 10)\n",
    "    plt.title('Probabilities of Ham Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.4 Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. </h2>\n",
    "Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "from itertools import groupby\n",
    "import math\n",
    "\n",
    "\n",
    "current_word, word = None, None                 \n",
    "current_email, email = None, None          \n",
    "current_y_true, y_true = None, None\n",
    "\n",
    "\n",
    "sum_records, sum_spamrecords, sum_hamrecords = 0,0,0\n",
    "sum_spamwords, sum_hamwords = 0,0\n",
    "spam_zeroes, ham_zeroes = 0,0\n",
    "\n",
    "\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    "conf_matrix = np.zeros((2,2))\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    if len(line) == 4:\n",
    "    \n",
    "        try:\n",
    "            word = line[0]                          ### word we get from mapper.py\n",
    "            count = line[1]                         ### corresponding count\n",
    "            count = int(count)                      ### convert count (currently a string) to int\n",
    "            email = line[2]                         ### id that identifies each email\n",
    "            y_true = line[3]                        ### spam truth\n",
    "            y_true = int(y_true)                    ### spam truth as an integer\n",
    "        except ValueError:                          ### any error then silently                         \n",
    "            continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "        if current_word == word:                    ### this IF-switch only works because Hadoop sorts map output\n",
    "            current_count += count                  ### by key (here: word) before it is passed to the reducer\n",
    "        \n",
    "            if current_word not in words.keys():     ### initialize word count of new words\n",
    "                words[current_word]={'ham_count':0,'spam_count':0}\n",
    "            \n",
    "            if current_email not in emails.keys():          ### initialize word counts and label of new email\n",
    "                emails[current_email]={'y_true':current_y_true,'word_count':0,'words':[]} \n",
    "                sum_records +=1.0\n",
    "                if current_y_true == 1:                     ### identified new email as either spam or ham\n",
    "                    sum_spamrecords +=1.0               \n",
    "\n",
    "            elif current_email in emails.keys():\n",
    "                emails[current_email]['word_count'] += 1\n",
    "                emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    \n",
    "            if current_y_true == 1:                         ### if record where word is located is a spam\n",
    "                current_spamcount += count         ### add to spam count of that word\n",
    "                sum_spamwords += 1.0\n",
    "            else:\n",
    "                current_hamcount += count          ### if not add to ham count of thet word\n",
    "                sum_hamwords +=1.0 \n",
    "\n",
    "        else:\n",
    "            if current_word:\n",
    "                try:\n",
    "                    if current_word in words.keys():\n",
    "                        words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "                        words[current_word]['ham_count'] += current_hamcount ### update ham count for current word\n",
    "                except ValueError:                          ### if count was not a number then silently                         \n",
    "                    continue                                ### ignore/discard this line\n",
    "                    \n",
    "                    \n",
    "            current_count = count                   ### set current count\n",
    "            current_word = word                     ### set current number\n",
    "            current_email = email                   ### set current id of email\n",
    "            current_y_true = y_true                 ### set current spam truth\n",
    "            current_spamcount, current_hamcount = 0,0\n",
    "        \n",
    "\n",
    "\n",
    "if current_word == word:                       ### do not forget to output the last word if needed!\n",
    "    emails[current_email]['word_count'] += 1\n",
    "    emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "    words[current_word]['ham_count'] += current_hamcount ### update ham count for current word    \n",
    "    \n",
    "\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "\n",
    "prior_spam= sum_spamrecords/sum_records     \n",
    "prior_ham= 1 - prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "prob_spamwords = sum_spamwords/(sum_spamwords+sum_hamwords)\n",
    "prob_hamwords = 1-prob_spamwords\n",
    "\n",
    "\n",
    "\n",
    "for k,word in words.iteritems():\n",
    "    ##These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    word['p_spam']=(word['spam_count']+1)/(sum_spamwords+vocab_count)\n",
    "    word['p_ham']=(word['ham_count']+1)/(sum_hamwords+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    #word['p_spam']=(word['spam_count'])/(sum_spamwords)\n",
    "    #word['p_ham']=(word['ham_count'])/(sum_hamwords)\n",
    "    \n",
    "    if word['p_spam'] == 0 and word['p_ham'] == 0:\n",
    "        spam_zeroes +=1\n",
    "        ham_zeroes +=1\n",
    "        word['p_spam']= 1\n",
    "        word['p_ham'] = 1\n",
    "    \n",
    "    elif word['p_spam'] == 0:\n",
    "        spam_zeroes +=1\n",
    "        word['p_ham'] = min(0.5 ** (1/word['ham_count']),0.99999999) ### back-calculates probability \n",
    "        word['p_spam']= 1-word['p_ham']              ### of the sequence of events assuming  \n",
    "                                                     ### sequence is 50% likely to happen         \n",
    "    elif word['p_ham'] == 0:\n",
    "        ham_zeroes +=1\n",
    "        word['p_spam'] = min(0.5 ** (1/word['spam_count']),0.99999999) ### Adjust probability \n",
    "        word['p_ham']= 1-word['p_spam']               ### just as above\n",
    "    \n",
    "    word['p_spam'] = math.log(word['p_spam'])              ### convert to log scale\n",
    "    word['p_ham'] = math.log(word['p_ham'])               \n",
    "        \n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "\n",
    "print 'Analyzed %s' % sum_records, 'using a vocabulary of %s' % vocab_count , 'terms'\n",
    "print 'On %s'%spam_zeroes , 'times we found spam probabilities that were zero'\n",
    "print 'On %s'%ham_zeroes, 'times we found not spam probabilities that were zero'\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS', '%20s' %'CUMULATIVE ACCURACY', '%10s'%'P-SPAM', '%10s'%'P-HAM'\n",
    "miss, sample_size = 0,0 \n",
    "prob_spam_list, prob_ham_list = [], []\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    p_spam= math.log(prior_spam)\n",
    "    p_ham= math.log(prior_ham)\n",
    "    \n",
    "    #p_spam=prior_spam\n",
    "    #p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "\n",
    "        try:\n",
    "            #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "            p_spam+=words[word]['p_spam']\n",
    "        except ValueError:\n",
    "            pass #This means that words that do not appear in a class will use the class prior\n",
    "        try:\n",
    "            #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "            p_ham+=words[word]['p_ham']\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if p_spam>p_ham:\n",
    "        y_pred=1\n",
    "        \n",
    "    else:\n",
    "        y_pred=0\n",
    "    y_true = email['y_true']\n",
    "    if y_true == 1:\n",
    "        prob_spam_list.append(math.exp(p_spam))\n",
    "        if y_pred == 1:\n",
    "            conf_matrix[0][0] +=1                     ### update confusion matrix\n",
    "        else:\n",
    "            conf_matrix[0][1] +=1\n",
    "            \n",
    "    else:\n",
    "        prob_ham_list.append(math.exp(p_spam))\n",
    "        if y_pred == 0:\n",
    "            conf_matrix[1][1] +=1\n",
    "        else:\n",
    "            conf_matrix[1][0] +=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    if y_pred != y_true:\n",
    "        miss+= 1.0\n",
    "        \n",
    "    sample_size += 1.0\n",
    "    accuracy = ((sample_size-miss)/sample_size)*100\n",
    "    prob_spam = math.exp(p_spam)\n",
    "    prob_ham = math.exp(p_ham)\n",
    "    print  '%30s' %j, '%10d' %y_true, '%10d' %y_pred, '%18.2f %%' % accuracy, '%10.6f'%prob_spam, '%10.6f'%prob_ham\n",
    "print 'See confusion matrix. Top Left is predicted correctly as spam. Bottom right predicted correctly as ham:'\n",
    "print conf_matrix\n",
    "print 'Error rate: %4.2f'%(100-accuracy),'%%'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hadoop streaming </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:15:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 18:15:02 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 18:15:02 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 18:15:02 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 18:15:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 18:15:02 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 18:15:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1778598902_0001\n",
      "16/01/26 18:15:03 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 18:15:03 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 18:15:03 INFO mapreduce.Job: Running job: job_local1778598902_0001\n",
      "16/01/26 18:15:03 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 18:15:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:15:03 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 18:15:03 INFO mapred.LocalJobRunner: Starting task: attempt_local1778598902_0001_m_000000_0\n",
      "16/01/26 18:15:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:15:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 18:15:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/enronemail_1h.txt:0+203981\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 18:15:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 18:15:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 18:15:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 18:15:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:04 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "16/01/26 18:15:04 INFO mapreduce.Job: Job job_local1778598902_0001 running in uber mode : false\n",
      "16/01/26 18:15:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 18:15:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 18:15:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 18:15:04 INFO mapred.LocalJobRunner: \n",
      "16/01/26 18:15:04 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 18:15:04 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 18:15:04 INFO mapred.MapTask: bufstart = 0; bufend = 1172828; bufvoid = 104857600\n",
      "16/01/26 18:15:04 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26072600(104290400); length = 141797/6553600\n",
      "16/01/26 18:15:05 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 18:15:05 INFO mapred.Task: Task:attempt_local1778598902_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "16/01/26 18:15:05 INFO mapred.Task: Task 'attempt_local1778598902_0001_m_000000_0' done.\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local1778598902_0001_m_000000_0\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 18:15:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: Starting task: attempt_local1778598902_0001_r_000000_0\n",
      "16/01/26 18:15:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:15:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 18:15:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 18:15:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5150e88d\n",
      "16/01/26 18:15:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 18:15:05 INFO reduce.EventFetcher: attempt_local1778598902_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 18:15:05 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1778598902_0001_m_000000_0 decomp: 1243730 len: 1243734 to MEMORY\n",
      "16/01/26 18:15:05 INFO reduce.InMemoryMapOutput: Read 1243730 bytes from map-output for attempt_local1778598902_0001_m_000000_0\n",
      "16/01/26 18:15:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1243730, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1243730\n",
      "16/01/26 18:15:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:15:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 18:15:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 18:15:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1243727 bytes\n",
      "16/01/26 18:15:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 1243730 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 18:15:05 INFO reduce.MergeManagerImpl: Merging 1 files, 1243734 bytes from disk\n",
      "16/01/26 18:15:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 18:15:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 18:15:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1243727 bytes\n",
      "16/01/26 18:15:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:15:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/26 18:15:05 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 18:15:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 18:15:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:06 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:07 INFO streaming.PipeMapRed: Records R/W=35450/1\n",
      "16/01/26 18:15:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 18:15:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 18:15:08 INFO mapred.Task: Task:attempt_local1778598902_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 18:15:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:15:08 INFO mapred.Task: Task attempt_local1778598902_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 18:15:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1778598902_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/NBLaplaceSmoothOutput/_temporary/0/task_local1778598902_0001_r_000000\n",
      "16/01/26 18:15:08 INFO mapred.LocalJobRunner: Records R/W=35450/1 > reduce\n",
      "16/01/26 18:15:08 INFO mapred.Task: Task 'attempt_local1778598902_0001_r_000000_0' done.\n",
      "16/01/26 18:15:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1778598902_0001_r_000000_0\n",
      "16/01/26 18:15:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 18:15:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 18:15:08 INFO mapreduce.Job: Job job_local1778598902_0001 completed successfully\n",
      "16/01/26 18:15:08 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2699304\n",
      "\t\tFILE: Number of bytes written=4531266\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=9726\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=35450\n",
      "\t\tMap output bytes=1172828\n",
      "\t\tMap output materialized bytes=1243734\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7275\n",
      "\t\tReduce shuffle bytes=1243734\n",
      "\t\tReduce input records=35450\n",
      "\t\tReduce output records=104\n",
      "\t\tSpilled Records=70900\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=544210944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9726\n",
      "16/01/26 18:15:08 INFO streaming.StreamJob: Output directory: NBLaplaceSmoothOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output NBLaplaceSmoothOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>show the results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:15:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Analyzed 96.0 using a vocabulary of 3077 terms\t\n",
      "On 0 times we found spam probabilities that were zero\t\n",
      "On 0 times we found not spam probabilities that were zero\t\n",
      "                            ID      TRUTH      CLASS  CUMULATIVE ACCURACY     P-SPAM      P-HAM\t\n",
      "            0010.2003-12-18.GP          1          0               0.00 %   0.447917   0.552083\t\n",
      "     0010.2001-06-28.SA_and_HP          1          1              50.00 %   0.000000   0.000000\t\n",
      "          0001.2000-01-17.beck          0          0              66.67 %   0.000000   0.000000\t\n",
      "      0018.1999-12-14.kaminski          0          0              75.00 %   0.000000   0.000000\t\n",
      "      0005.1999-12-12.kaminski          0          0              80.00 %   0.000000   0.000000\t\n",
      "     0011.2001-06-29.SA_and_HP          1          1              83.33 %   0.000000   0.000000\t\n",
      "            0008.2004-08-01.BG          1          1              85.71 %   0.000000   0.000000\t\n",
      "        0009.1999-12-14.farmer          0          0              87.50 %   0.000000   0.000000\t\n",
      "            0017.2003-12-18.GP          1          1              88.89 %   0.000000   0.000000\t\n",
      "     0011.2001-06-28.SA_and_HP          1          1              90.00 %   0.000000   0.000000\t\n",
      "     0015.2001-07-05.SA_and_HP          1          1              90.91 %   0.000000   0.000000\t\n",
      "       0015.2001-02-12.kitchen          0          0              91.67 %   0.000000   0.000000\t\n",
      "      0017.1999-12-14.kaminski          0          0              92.31 %   0.000000   0.000000\t\n",
      "          0012.2000-01-17.beck          0          0              92.86 %   0.000000   0.000000\t\n",
      "          0003.2000-01-17.beck          0          0              93.33 %   0.000000   0.000000\t\n",
      "     0004.2001-06-12.SA_and_HP          1          1              93.75 %   0.000000   0.000000\t\n",
      "     0008.2001-06-12.SA_and_HP          1          1              94.12 %   0.000000   0.000000\t\n",
      "       0007.2001-02-09.kitchen          0          0              94.44 %   0.000000   0.000000\t\n",
      "            0016.2004-08-01.BG          1          1              94.74 %   0.000000   0.000000\t\n",
      "        0016.1999-12-15.farmer          0          0              95.00 %   0.000000   0.000000\t\n",
      "            0013.2004-08-01.BG          1          1              95.24 %   0.000000   0.000000\t\n",
      "            0005.2003-12-18.GP          1          1              95.45 %   0.000000   0.000000\t\n",
      "       0012.2001-02-09.kitchen          0          0              95.65 %   0.000000   0.000000\t\n",
      "        0011.1999-12-14.farmer          0          0              95.83 %   0.000000   0.000000\t\n",
      "       0009.2001-02-09.kitchen          0          0              96.00 %   0.000000   0.000000\t\n",
      "       0006.2001-02-08.kitchen          0          0              96.15 %   0.000000   0.000000\t\n",
      "            0014.2003-12-19.GP          1          1              96.30 %   0.000048   0.000046\t\n",
      "        0010.1999-12-14.farmer          0          0              96.43 %   0.000000   0.000000\t\n",
      "            0010.2004-08-01.BG          1          1              96.55 %   0.000000   0.000000\t\n",
      "      0014.1999-12-14.kaminski          0          0              96.67 %   0.000000   0.000000\t\n",
      "      0006.1999-12-13.kaminski          0          0              96.77 %   0.000000   0.000000\t\n",
      "        0005.1999-12-14.farmer          0          0              96.88 %   0.000000   0.000000\t\n",
      "       0003.2001-02-08.kitchen          0          0              96.97 %   0.000000   0.000000\t\n",
      "       0001.2001-02-07.kitchen          0          0              97.06 %   0.000000   0.000000\t\n",
      "       0008.2001-02-09.kitchen          0          0              97.14 %   0.000000   0.000000\t\n",
      "            0017.2004-08-02.BG          1          1              97.22 %   0.000000   0.000000\t\n",
      "            0007.2003-12-18.GP          1          1              97.30 %   0.000000   0.000000\t\n",
      "            0014.2004-08-01.BG          1          1              97.37 %   0.000000   0.000000\t\n",
      "            0006.2003-12-18.GP          1          1              97.44 %   0.000000   0.000000\t\n",
      "     0016.2001-07-05.SA_and_HP          1          1              97.50 %   0.000000   0.000000\t\n",
      "         0005.2000-06-06.lokay          0          0              97.56 %   0.000000   0.000000\t\n",
      "     0014.2001-07-04.SA_and_HP          1          1              97.62 %   0.000000   0.000000\t\n",
      "      0001.2001-04-02.williams          0          0              97.67 %   0.000000   0.000000\t\n",
      "         0012.2000-06-08.lokay          0          0              97.73 %   0.000000   0.000000\t\n",
      "        0014.1999-12-15.farmer          0          0              97.78 %   0.000000   0.000000\t\n",
      "         0009.2000-06-07.lokay          0          0              97.83 %   0.000000   0.000000\t\n",
      "        0001.1999-12-10.farmer          0          0              97.87 %   0.447917   0.552083\t\n",
      "     0008.2001-06-25.SA_and_HP          1          1              97.92 %   0.000000   0.000000\t\n",
      "      0017.2001-04-03.williams          0          0              97.96 %   0.000000   0.000000\t\n",
      "       0014.2001-02-12.kitchen          0          0              98.00 %   0.000000   0.000000\t\n",
      "     0016.2001-07-06.SA_and_HP          1          1              98.04 %   0.000000   0.000000\t\n",
      "        0015.1999-12-15.farmer          0          0              98.08 %   0.000000   0.000000\t\n",
      "      0004.1999-12-10.kaminski          0          0              98.11 %   0.000000   0.000000\t\n",
      "            0011.2004-08-01.BG          1          1              98.15 %   0.000000   0.000000\t\n",
      "            0004.2004-08-01.BG          1          1              98.18 %   0.000000   0.000000\t\n",
      "            0018.2003-12-18.GP          1          1              98.21 %   0.000000   0.000000\t\n",
      "        0007.1999-12-14.farmer          0          0              98.25 %   0.000000   0.000000\t\n",
      "            0016.2003-12-19.GP          1          1              98.28 %   0.000000   0.000000\t\n",
      "        0004.1999-12-14.farmer          0          0              98.31 %   0.000000   0.000000\t\n",
      "            0015.2003-12-19.GP          1          1              98.33 %   0.000000   0.000000\t\n",
      "            0006.2004-08-01.BG          1          1              98.36 %   0.000000   0.000000\t\n",
      "            0009.2003-12-18.GP          1          1              98.39 %   0.000000   0.000000\t\n",
      "        0002.1999-12-13.farmer          0          0              98.41 %   0.000000   0.000000\t\n",
      "            0008.2003-12-18.GP          1          1              98.44 %   0.000000   0.000000\t\n",
      "      0010.1999-12-14.kaminski          0          0              98.46 %   0.447917   0.552083\t\n",
      "          0007.2000-01-17.beck          0          0              98.48 %   0.000000   0.000000\t\n",
      "        0003.1999-12-14.farmer          0          0              98.51 %   0.000000   0.000000\t\n",
      "            0003.2004-08-01.BG          1          1              98.53 %   0.000000   0.000000\t\n",
      "            0017.2004-08-01.BG          1          1              98.55 %   0.000000   0.000000\t\n",
      "     0013.2001-06-30.SA_and_HP          1          1              98.57 %   0.000000   0.000000\t\n",
      "      0003.1999-12-10.kaminski          0          0              98.59 %   0.000000   0.000000\t\n",
      "        0012.1999-12-14.farmer          0          0              98.61 %   0.000000   0.000000\t\n",
      "      0009.1999-12-13.kaminski          0          0              98.63 %   0.000000   0.000000\t\n",
      "     0018.2001-07-13.SA_and_HP          1          1              98.65 %   0.000000   0.000000\t\n",
      "       0002.2001-02-07.kitchen          0          0              98.67 %   0.000000   0.000000\t\n",
      "            0007.2004-08-01.BG          1          1              98.68 %   0.000000   0.000000\t\n",
      "      0012.1999-12-14.kaminski          0          0              98.70 %   0.000000   0.000000\t\n",
      "     0005.2001-06-23.SA_and_HP          1          1              98.72 %   0.000000   0.000000\t\n",
      "      0013.1999-12-14.kaminski          0          0              98.73 %   0.000000   0.000000\t\n",
      "      0007.1999-12-13.kaminski          0          0              98.75 %   0.000000   0.000000\t\n",
      "          0017.2000-01-17.beck          0          0              98.77 %   0.000000   0.000000\t\n",
      "     0006.2001-06-25.SA_and_HP          1          1              98.78 %   0.000000   0.000000\t\n",
      "      0006.2001-04-03.williams          0          0              98.80 %   0.000000   0.000000\t\n",
      "       0005.2001-02-08.kitchen          0          0              98.81 %   0.000000   0.000000\t\n",
      "            0002.2003-12-18.GP          1          1              98.82 %   0.000000   0.000000\t\n",
      "            0003.2003-12-18.GP          1          1              98.84 %   0.000000   0.000000\t\n",
      "      0013.2001-04-03.williams          0          0              98.85 %   0.000000   0.000000\t\n",
      "      0004.2001-04-02.williams          0          0              98.86 %   0.000000   0.000000\t\n",
      "       0010.2001-02-09.kitchen          0          0              98.88 %   0.000000   0.000000\t\n",
      "        0013.1999-12-14.farmer          0          0              98.89 %   0.000000   0.000000\t\n",
      "      0015.1999-12-14.kaminski          0          0              98.90 %   0.000000   0.000000\t\n",
      "            0012.2003-12-19.GP          1          1              98.91 %   0.000000   0.000000\t\n",
      "       0016.2001-02-12.kitchen          0          0              98.92 %   0.000000   0.000000\t\n",
      "            0002.2004-08-01.BG          1          1              98.94 %   0.000000   0.000000\t\n",
      "     0002.2001-05-25.SA_and_HP          1          1              98.95 %   0.000000   0.000000\t\n",
      "            0011.2003-12-18.GP          1          1              98.96 %   0.000000   0.000000\t\n",
      "See confusion matrix. Top Left is predicted correctly as spam. Bottom right predicted correctly as ham:\t\n",
      "[[ 42.   1.]\t\n",
      " [  0.  53.]]\t\n",
      "Error rate: 1.04 %%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat NBLaplaceSmoothOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Report</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Laplace allowed us to improve accuracy to 99% from 98%. The difference was on a single record that my prior algorithm predicted as ham. However, even in that prediction the estimated p_spam and p_ham were very close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.4</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.5. Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. </h2>How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "from itertools import groupby\n",
    "import math\n",
    "\n",
    "\n",
    "current_word, word = None, None                 \n",
    "current_email, email = None, None          \n",
    "current_y_true, y_true = None, None\n",
    "\n",
    "\n",
    "sum_records, sum_spamrecords, sum_hamrecords = 0,0,0\n",
    "sum_spamwords, sum_hamwords = 0,0\n",
    "spam_zeroes, ham_zeroes = 0,0\n",
    "\n",
    "\n",
    "emails={} #Associative array to hold email data\n",
    "words={} #Associative array for word data\n",
    "conf_matrix = np.zeros((2,2))\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    line = line.strip()                        ### remove leading and trailing whitespace\n",
    "    line = line.split('\\t')                    ### parse the input we got from mapper.py \n",
    "    if len(line) == 4:\n",
    "    \n",
    "        try:\n",
    "            word = line[0]                          ### word we get from mapper.py\n",
    "            count = line[1]                         ### corresponding count\n",
    "            count = int(count)                      ### convert count (currently a string) to int\n",
    "            email = line[2]                         ### id that identifies each email\n",
    "            y_true = line[3]                        ### spam truth\n",
    "            y_true = int(y_true)                    ### spam truth as an integer\n",
    "        except ValueError:                          ### any error then silently                         \n",
    "            continue                                ### ignore/discard this line\n",
    "\n",
    "                                                \n",
    "                                                \n",
    "        if current_word == word:                    ### this IF-switch only works because Hadoop sorts map output\n",
    "            current_count += count                  ### by key (here: word) before it is passed to the reducer\n",
    "        \n",
    "            if current_word not in words.keys():     ### initialize word count of new words\n",
    "                words[current_word]={'ham_count':0,'spam_count':0}\n",
    "            \n",
    "            if current_email not in emails.keys():          ### initialize word counts and label of new email\n",
    "                emails[current_email]={'y_true':current_y_true,'word_count':0,'words':[]} \n",
    "                sum_records +=1.0\n",
    "                if current_y_true == 1:                     ### identified new email as either spam or ham\n",
    "                    sum_spamrecords +=1.0               \n",
    "\n",
    "            elif current_email in emails.keys():\n",
    "                emails[current_email]['word_count'] += 1\n",
    "                emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    \n",
    "            if current_y_true == 1:                         ### if record where word is located is a spam\n",
    "                current_spamcount += count         ### add to spam count of that word\n",
    "                sum_spamwords += 1.0\n",
    "            else:\n",
    "                current_hamcount += count          ### if not add to ham count of thet word\n",
    "                sum_hamwords +=1.0 \n",
    "\n",
    "        else:\n",
    "            if current_word:\n",
    "                try:\n",
    "                    if current_word in words.keys():\n",
    "                        words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "                        words[current_word]['ham_count'] += current_hamcount ### update ham count for current word\n",
    "                except ValueError:                          ### if count was not a number then silently                         \n",
    "                    continue                                ### ignore/discard this line\n",
    "                    \n",
    "                    \n",
    "            current_count = count                   ### set current count\n",
    "            current_word = word                     ### set current number\n",
    "            current_email = email                   ### set current id of email\n",
    "            current_y_true = y_true                 ### set current spam truth\n",
    "            current_spamcount, current_hamcount = 0,0\n",
    "        \n",
    "\n",
    "\n",
    "if current_word == word:                       ### do not forget to output the last word if needed!\n",
    "    emails[current_email]['word_count'] += 1\n",
    "    emails[current_email]['words'].append(current_word)### store words in email  \n",
    "    words[current_word]['spam_count'] += current_spamcount ### update spam count for current word \n",
    "    words[current_word]['ham_count'] += current_hamcount ### update ham count for current word    \n",
    "    \n",
    "\n",
    "\n",
    "#Calculate stats for entire corpus\n",
    "\n",
    "prior_spam= sum_spamrecords/sum_records     \n",
    "prior_ham= 1 - prior_spam\n",
    "vocab_count=len(words)#number of unique words in the total vocabulary\n",
    "prob_spamwords = sum_spamwords/(sum_spamwords+sum_hamwords)\n",
    "prob_hamwords = 1-prob_spamwords\n",
    "\n",
    "\n",
    "\n",
    "for k,word in words.iteritems():\n",
    "    ##These versions calculate conditional probabilities WITH Laplace smoothing.  \n",
    "    if (word['spam_count'] + word['ham_count']) < 3:\n",
    "        word['p_spam']= 1\n",
    "        word['p_ham'] = 1\n",
    "    else:\n",
    "        word['p_spam']=(word['spam_count']+1)/(sum_spamwords+vocab_count)\n",
    "        word['p_ham']=(word['ham_count']+1)/(sum_hamwords+vocab_count)\n",
    "    \n",
    "    #Compute conditional probabilities WITHOUT Laplace smoothing\n",
    "    #word['p_spam']=(word['spam_count'])/(sum_spamwords)\n",
    "    #word['p_ham']=(word['ham_count'])/(sum_hamwords)\n",
    "    \n",
    "    if word['p_spam'] == 0 and word['p_ham'] == 0:\n",
    "        spam_zeroes +=1\n",
    "        ham_zeroes +=1\n",
    "        word['p_spam']= 1\n",
    "        word['p_ham'] = 1\n",
    "    \n",
    "    elif word['p_spam'] == 0:\n",
    "        spam_zeroes +=1\n",
    "        word['p_ham'] = min(0.5 ** (1/word['ham_count']),0.99999999) ### back-calculates probability \n",
    "        word['p_spam']= 1-word['p_ham']              ### of the sequence of events assuming  \n",
    "                                                     ### sequence is 50% likely to happen         \n",
    "    elif word['p_ham'] == 0:\n",
    "        ham_zeroes +=1\n",
    "        word['p_spam'] = min(0.5 ** (1/word['spam_count']),0.99999999) ### Adjust probability \n",
    "        word['p_ham']= 1-word['p_spam']               ### just as above\n",
    "    \n",
    "    word['p_spam'] = math.log(word['p_spam'])              ### convert to log scale\n",
    "    word['p_ham'] = math.log(word['p_ham'])               \n",
    "        \n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "\n",
    "print 'Analyzed %s' % sum_records, 'using a vocabulary of %s' % vocab_count , 'terms'\n",
    "print 'On %s'%spam_zeroes , 'times we found spam probabilities that were zero'\n",
    "print 'On %s'%ham_zeroes, 'times we found not spam probabilities that were zero'\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS', '%20s' %'CUMULATIVE ACCURACY', '%10s'%'P-SPAM', '%10s'%'P-HAM'\n",
    "miss, sample_size = 0,0 \n",
    "prob_spam_list, prob_ham_list = [], []\n",
    "for j,email in emails.iteritems():\n",
    "    \n",
    "    #Log versions - no longer used\n",
    "    p_spam= math.log(prior_spam)\n",
    "    p_ham= math.log(prior_ham)\n",
    "    \n",
    "    #p_spam=prior_spam\n",
    "    #p_ham=prior_ham\n",
    "    \n",
    "    for word in email['words']:\n",
    "\n",
    "        try:\n",
    "            #p_spam+=log(words[word]['p_spam']) #Log version - no longer used\n",
    "            p_spam+=words[word]['p_spam']\n",
    "        except ValueError:\n",
    "            pass #This means that words that do not appear in a class will use the class prior\n",
    "        try:\n",
    "            #p_ham+=log(words[word]['p_ham']) #Log version - no longer used\n",
    "            p_ham+=words[word]['p_ham']\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if p_spam>p_ham:\n",
    "        y_pred=1\n",
    "        \n",
    "    else:\n",
    "        y_pred=0\n",
    "    y_true = email['y_true']\n",
    "    if y_true == 1:\n",
    "        prob_spam_list.append(math.exp(p_spam))\n",
    "        if y_pred == 1:\n",
    "            conf_matrix[0][0] +=1                     ### update confusion matrix\n",
    "        else:\n",
    "            conf_matrix[0][1] +=1\n",
    "    else:\n",
    "        prob_ham_list.append(math.exp(p_spam))\n",
    "        if y_pred == 0:\n",
    "            conf_matrix[1][1] +=1\n",
    "        else:\n",
    "            conf_matrix[1][0] +=1\n",
    "    \n",
    "    \n",
    "    if y_pred != y_true:\n",
    "        miss+= 1.0\n",
    "        \n",
    "    sample_size += 1.0\n",
    "    accuracy = ((sample_size-miss)/sample_size)*100\n",
    "    prob_spam = math.exp(p_spam)\n",
    "    prob_ham = math.exp(p_ham)\n",
    "    print  '%30s' %j, '%10d' %y_true, '%10d' %y_pred, '%18.2f %%' % accuracy, '%10.6f'%prob_spam, '%10.6f'%prob_ham\n",
    "\n",
    "print 'See confusion matrix. Top Left is predicted correctly as spam. Bottom right predicted correctly as ham:'\n",
    "print conf_matrix\n",
    "print 'Error rate: %4.2f'%(100-accuracy),'%%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hadoop Streaming </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:15:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 18:15:51 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/26 18:15:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/26 18:15:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/26 18:15:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 18:15:52 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/26 18:15:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local197341096_0001\n",
      "16/01/26 18:15:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/26 18:15:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/26 18:15:52 INFO mapreduce.Job: Running job: job_local197341096_0001\n",
      "16/01/26 18:15:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/26 18:15:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:15:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/26 18:15:52 INFO mapred.LocalJobRunner: Starting task: attempt_local197341096_0001_m_000000_0\n",
      "16/01/26 18:15:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:15:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 18:15:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/venamax/enronemail_1h.txt:0+203981\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/26 18:15:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/26 18:15:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./mapper.py]\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/26 18:15:52 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/26 18:15:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:52 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "16/01/26 18:15:53 INFO mapreduce.Job: Job job_local197341096_0001 running in uber mode : false\n",
      "16/01/26 18:15:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: \n",
      "16/01/26 18:15:53 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/26 18:15:53 INFO mapred.MapTask: Spilling map output\n",
      "16/01/26 18:15:53 INFO mapred.MapTask: bufstart = 0; bufend = 1172828; bufvoid = 104857600\n",
      "16/01/26 18:15:53 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26072600(104290400); length = 141797/6553600\n",
      "16/01/26 18:15:53 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/26 18:15:53 INFO mapred.Task: Task:attempt_local197341096_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "16/01/26 18:15:53 INFO mapred.Task: Task 'attempt_local197341096_0001_m_000000_0' done.\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local197341096_0001_m_000000_0\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: Starting task: attempt_local197341096_0001_r_000000_0\n",
      "16/01/26 18:15:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/26 18:15:53 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/26 18:15:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/26 18:15:53 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@bed9885\n",
      "16/01/26 18:15:53 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/26 18:15:53 INFO reduce.EventFetcher: attempt_local197341096_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/26 18:15:53 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local197341096_0001_m_000000_0 decomp: 1243730 len: 1243734 to MEMORY\n",
      "16/01/26 18:15:53 INFO reduce.InMemoryMapOutput: Read 1243730 bytes from map-output for attempt_local197341096_0001_m_000000_0\n",
      "16/01/26 18:15:53 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1243730, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1243730\n",
      "16/01/26 18:15:53 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:15:53 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/26 18:15:53 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 18:15:53 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1243727 bytes\n",
      "16/01/26 18:15:53 INFO reduce.MergeManagerImpl: Merged 1 segments, 1243730 bytes to disk to satisfy reduce memory limit\n",
      "16/01/26 18:15:53 INFO reduce.MergeManagerImpl: Merging 1 files, 1243734 bytes from disk\n",
      "16/01/26 18:15:53 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/26 18:15:53 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/26 18:15:53 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1243727 bytes\n",
      "16/01/26 18:15:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/venamax/ucb-261/hw2/./reducer.py]\n",
      "16/01/26 18:15:53 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/26 18:15:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 18:15:54 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/26 18:15:55 INFO streaming.PipeMapRed: Records R/W=35450/1\n",
      "16/01/26 18:15:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/26 18:15:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/26 18:15:56 INFO mapred.Task: Task:attempt_local197341096_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/26 18:15:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/26 18:15:56 INFO mapred.Task: Task attempt_local197341096_0001_r_000000_0 is allowed to commit now\n",
      "16/01/26 18:15:56 INFO output.FileOutputCommitter: Saved output of task 'attempt_local197341096_0001_r_000000_0' to hdfs://localhost:9000/user/venamax/NBLaplaceSmoothLess3Output/_temporary/0/task_local197341096_0001_r_000000\n",
      "16/01/26 18:15:56 INFO mapred.LocalJobRunner: Records R/W=35450/1 > reduce\n",
      "16/01/26 18:15:56 INFO mapred.Task: Task 'attempt_local197341096_0001_r_000000_0' done.\n",
      "16/01/26 18:15:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local197341096_0001_r_000000_0\n",
      "16/01/26 18:15:56 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/26 18:15:56 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 18:15:56 INFO mapreduce.Job: Job job_local197341096_0001 completed successfully\n",
      "16/01/26 18:15:56 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2699304\n",
      "\t\tFILE: Number of bytes written=4528278\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=9726\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=35450\n",
      "\t\tMap output bytes=1172828\n",
      "\t\tMap output materialized bytes=1243734\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7275\n",
      "\t\tReduce shuffle bytes=1243734\n",
      "\t\tReduce input records=35450\n",
      "\t\tReduce output records=104\n",
      "\t\tSpilled Records=70900\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=545259520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9726\n",
      "16/01/26 18:15:56 INFO streaming.StreamJob: Output directory: NBLaplaceSmoothLess3Output\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output NBLaplaceSmoothLess3Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> show results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:16:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Analyzed 96.0 using a vocabulary of 3077 terms\t\n",
      "On 0 times we found spam probabilities that were zero\t\n",
      "On 0 times we found not spam probabilities that were zero\t\n",
      "                            ID      TRUTH      CLASS  CUMULATIVE ACCURACY     P-SPAM      P-HAM\t\n",
      "            0010.2003-12-18.GP          1          0               0.00 %   0.447917   0.552083\t\n",
      "     0010.2001-06-28.SA_and_HP          1          1              50.00 %   0.000000   0.000000\t\n",
      "          0001.2000-01-17.beck          0          0              66.67 %   0.000000   0.000000\t\n",
      "      0018.1999-12-14.kaminski          0          0              75.00 %   0.000000   0.000000\t\n",
      "      0005.1999-12-12.kaminski          0          0              80.00 %   0.000000   0.000000\t\n",
      "     0011.2001-06-29.SA_and_HP          1          1              83.33 %   0.000000   0.000000\t\n",
      "            0008.2004-08-01.BG          1          1              85.71 %   0.000000   0.000000\t\n",
      "        0009.1999-12-14.farmer          0          0              87.50 %   0.000000   0.000000\t\n",
      "            0017.2003-12-18.GP          1          1              88.89 %   0.000000   0.000000\t\n",
      "     0011.2001-06-28.SA_and_HP          1          1              90.00 %   0.000000   0.000000\t\n",
      "     0015.2001-07-05.SA_and_HP          1          1              90.91 %   0.000000   0.000000\t\n",
      "       0015.2001-02-12.kitchen          0          0              91.67 %   0.000000   0.000000\t\n",
      "      0017.1999-12-14.kaminski          0          0              92.31 %   0.000000   0.000000\t\n",
      "          0012.2000-01-17.beck          0          0              92.86 %   0.000000   0.000000\t\n",
      "          0003.2000-01-17.beck          0          0              93.33 %   0.000000   0.000000\t\n",
      "     0004.2001-06-12.SA_and_HP          1          1              93.75 %   0.000000   0.000000\t\n",
      "     0008.2001-06-12.SA_and_HP          1          1              94.12 %   0.000000   0.000000\t\n",
      "       0007.2001-02-09.kitchen          0          0              94.44 %   0.000000   0.000000\t\n",
      "            0016.2004-08-01.BG          1          1              94.74 %   0.000000   0.000000\t\n",
      "        0016.1999-12-15.farmer          0          0              95.00 %   0.000000   0.000000\t\n",
      "            0013.2004-08-01.BG          1          1              95.24 %   0.000000   0.000000\t\n",
      "            0005.2003-12-18.GP          1          1              95.45 %   0.000000   0.000000\t\n",
      "       0012.2001-02-09.kitchen          0          0              95.65 %   0.000000   0.000000\t\n",
      "        0011.1999-12-14.farmer          0          0              95.83 %   0.000000   0.000000\t\n",
      "       0009.2001-02-09.kitchen          0          0              96.00 %   0.000000   0.000000\t\n",
      "       0006.2001-02-08.kitchen          0          0              96.15 %   0.000000   0.000000\t\n",
      "            0014.2003-12-19.GP          1          0              92.59 %   0.447917   0.552083\t\n",
      "        0010.1999-12-14.farmer          0          0              92.86 %   0.000000   0.000000\t\n",
      "            0010.2004-08-01.BG          1          1              93.10 %   0.000000   0.000000\t\n",
      "      0014.1999-12-14.kaminski          0          0              93.33 %   0.000000   0.000000\t\n",
      "      0006.1999-12-13.kaminski          0          0              93.55 %   0.000000   0.000000\t\n",
      "        0005.1999-12-14.farmer          0          0              93.75 %   0.000000   0.000000\t\n",
      "       0003.2001-02-08.kitchen          0          0              93.94 %   0.000000   0.000000\t\n",
      "       0001.2001-02-07.kitchen          0          0              94.12 %   0.000000   0.000000\t\n",
      "       0008.2001-02-09.kitchen          0          0              94.29 %   0.000000   0.000000\t\n",
      "            0017.2004-08-02.BG          1          1              94.44 %   0.000000   0.000000\t\n",
      "            0007.2003-12-18.GP          1          0              91.89 %   0.447917   0.552083\t\n",
      "            0014.2004-08-01.BG          1          1              92.11 %   0.000000   0.000000\t\n",
      "            0006.2003-12-18.GP          1          1              92.31 %   0.000000   0.000000\t\n",
      "     0016.2001-07-05.SA_and_HP          1          1              92.50 %   0.000000   0.000000\t\n",
      "         0005.2000-06-06.lokay          0          0              92.68 %   0.000000   0.000000\t\n",
      "     0014.2001-07-04.SA_and_HP          1          1              92.86 %   0.000000   0.000000\t\n",
      "      0001.2001-04-02.williams          0          0              93.02 %   0.000000   0.000000\t\n",
      "         0012.2000-06-08.lokay          0          0              93.18 %   0.000000   0.000000\t\n",
      "        0014.1999-12-15.farmer          0          0              93.33 %   0.000000   0.000000\t\n",
      "         0009.2000-06-07.lokay          0          0              93.48 %   0.000000   0.000000\t\n",
      "        0001.1999-12-10.farmer          0          0              93.62 %   0.447917   0.552083\t\n",
      "     0008.2001-06-25.SA_and_HP          1          1              93.75 %   0.000000   0.000000\t\n",
      "      0017.2001-04-03.williams          0          0              93.88 %   0.000000   0.000000\t\n",
      "       0014.2001-02-12.kitchen          0          0              94.00 %   0.000000   0.000000\t\n",
      "     0016.2001-07-06.SA_and_HP          1          1              94.12 %   0.000000   0.000000\t\n",
      "        0015.1999-12-15.farmer          0          0              94.23 %   0.000000   0.000000\t\n",
      "      0004.1999-12-10.kaminski          0          0              94.34 %   0.000000   0.000000\t\n",
      "            0011.2004-08-01.BG          1          1              94.44 %   0.000000   0.000000\t\n",
      "            0004.2004-08-01.BG          1          1              94.55 %   0.000000   0.000000\t\n",
      "            0018.2003-12-18.GP          1          1              94.64 %   0.000000   0.000000\t\n",
      "        0007.1999-12-14.farmer          0          0              94.74 %   0.000000   0.000000\t\n",
      "            0016.2003-12-19.GP          1          1              94.83 %   0.000000   0.000000\t\n",
      "        0004.1999-12-14.farmer          0          0              94.92 %   0.000000   0.000000\t\n",
      "            0015.2003-12-19.GP          1          1              95.00 %   0.000000   0.000000\t\n",
      "            0006.2004-08-01.BG          1          1              95.08 %   0.000000   0.000000\t\n",
      "            0009.2003-12-18.GP          1          1              95.16 %   0.000000   0.000000\t\n",
      "        0002.1999-12-13.farmer          0          0              95.24 %   0.000000   0.000000\t\n",
      "            0008.2003-12-18.GP          1          1              95.31 %   0.000000   0.000000\t\n",
      "      0010.1999-12-14.kaminski          0          0              95.38 %   0.447917   0.552083\t\n",
      "          0007.2000-01-17.beck          0          0              95.45 %   0.000000   0.000000\t\n",
      "        0003.1999-12-14.farmer          0          0              95.52 %   0.447917   0.552083\t\n",
      "            0003.2004-08-01.BG          1          1              95.59 %   0.000000   0.000000\t\n",
      "            0017.2004-08-01.BG          1          1              95.65 %   0.000000   0.000000\t\n",
      "     0013.2001-06-30.SA_and_HP          1          1              95.71 %   0.000000   0.000000\t\n",
      "      0003.1999-12-10.kaminski          0          0              95.77 %   0.000000   0.000000\t\n",
      "        0012.1999-12-14.farmer          0          0              95.83 %   0.000000   0.000000\t\n",
      "      0009.1999-12-13.kaminski          0          0              95.89 %   0.000000   0.000000\t\n",
      "     0018.2001-07-13.SA_and_HP          1          1              95.95 %   0.000000   0.000000\t\n",
      "       0002.2001-02-07.kitchen          0          0              96.00 %   0.000000   0.000000\t\n",
      "            0007.2004-08-01.BG          1          1              96.05 %   0.000000   0.000000\t\n",
      "      0012.1999-12-14.kaminski          0          0              96.10 %   0.000000   0.000000\t\n",
      "     0005.2001-06-23.SA_and_HP          1          0              94.87 %   0.447917   0.552083\t\n",
      "      0013.1999-12-14.kaminski          0          0              94.94 %   0.000000   0.000000\t\n",
      "      0007.1999-12-13.kaminski          0          0              95.00 %   0.000000   0.000000\t\n",
      "          0017.2000-01-17.beck          0          0              95.06 %   0.000000   0.000000\t\n",
      "     0006.2001-06-25.SA_and_HP          1          0              93.90 %   0.447917   0.552083\t\n",
      "      0006.2001-04-03.williams          0          0              93.98 %   0.000000   0.000000\t\n",
      "       0005.2001-02-08.kitchen          0          0              94.05 %   0.000000   0.000000\t\n",
      "            0002.2003-12-18.GP          1          1              94.12 %   0.000000   0.000000\t\n",
      "            0003.2003-12-18.GP          1          1              94.19 %   0.000000   0.000000\t\n",
      "      0013.2001-04-03.williams          0          0              94.25 %   0.000000   0.000000\t\n",
      "      0004.2001-04-02.williams          0          0              94.32 %   0.447917   0.552083\t\n",
      "       0010.2001-02-09.kitchen          0          0              94.38 %   0.000000   0.000000\t\n",
      "        0013.1999-12-14.farmer          0          0              94.44 %   0.000000   0.000000\t\n",
      "      0015.1999-12-14.kaminski          0          0              94.51 %   0.000000   0.000000\t\n",
      "            0012.2003-12-19.GP          1          1              94.57 %   0.000000   0.000000\t\n",
      "       0016.2001-02-12.kitchen          0          0              94.62 %   0.000000   0.000000\t\n",
      "            0002.2004-08-01.BG          1          1              94.68 %   0.000000   0.000000\t\n",
      "     0002.2001-05-25.SA_and_HP          1          1              94.74 %   0.000000   0.000000\t\n",
      "            0011.2003-12-18.GP          1          1              94.79 %   0.000000   0.000000\t\n",
      "See confusion matrix. Top Left is predicted correctly as spam. Bottom right predicted correctly as ham:\t\n",
      "[[ 38.   5.]\t\n",
      " [  0.  53.]]\t\n",
      "Error rate: 5.21 %%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat NBLaplaceSmoothLess3Output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Report </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rate went up because we're now discarding some words that are not that frequent but are great predictors. So I would not recommend to drop terms that are not that frequent. We should use the frequency to estimate how likely it is that term is a good predictor. That's what I did on 2.3. I adjusted zero probabilities by their frequency taking into account that as a term has higher frequency it is more likely to appear on both ham and spam lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.5</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm</h2>\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christmas tree farm pictures NA\n",
      "0\n",
      "0001.1999-12-10.farmer\n",
      "re: rankings  thank you.\n",
      "0\n",
      "0001.1999-12-10.kaminski\n",
      "leadership development pilot  sally:  what timing, ask and you shall receive. as per our discussion, listed below  is an update on the leadership pilot. your vendor selection team will  receive an update and even more information later in the week.  on the lunch & learn for energy operations, the audience and focus will be  your group. we are ready to start up when appropriate.  thank you for your time today. please call me if you have any questions at  x 33597.  ----------------------forwarded by julie armstrong/corp/enron on 01/17/2000  06:44 pm---------------------------  from: susan runkel @ ect 01/17/2000 03:22 pm  to: cindy skinner/hou/ect @ ect, brad mcsherry/hou/ect @ ect, norma  villarreal/hou/ect @ ect, kimberly rizzi/hou/ect @ ect, fran l mayes/hou/ect @ ect,  gary buck/hou/ect @ ect, robert jones/corp/enron @ enron, sheila  walton/hou/ect @ ect, philip conn/corp/enron @ enron, mary overgaard/pdx/ect @ ect,  kim melodick/hou/ect @ ect, valeria a hope/hou/ect @ ect  cc: david oxley/hou/ect @ ect, susan carrera/hou/ect @ ect, jane  allen/hou/ect @ ect, christine shenkman/enron_development @ enron_development,  kathryn mclean/hou/ect @ ect, gracie s presas/hou/ect @ ect, janice  riedel/hou/ect @ ect, julie armstrong/corp/enron @ enron  subject: leadership development pilot  good news regarding the ena leadership curriculum! through the help of a  vendor selection team from eops, we've chosen southwest performance group and  wilson learning products as one of our primary vendors for the leadership  curriculum and programs. we are ready to conduct a pilot on february 8-10 of  six modules. the purpose of the pilot is to evaluate for fine-tuning the  wilson learning materials and facilitators and to present just a portion of  the leadership curriculum.  in order to evaluate the materials thoroughly, it would be great to get a  cross-section of ena to attend. we are asking that you invite several  supervisors from your client groups to participate in any of the courses  listed below. the sessions will be held in room 560 and times are listed  below. also attached is a description of the modules. all are designed for  supervisors only, with the exception being \" communicating effectively \". this  is open to any employee. as a benefit in attending the pilot, i will pick up  the cost., so there will be no charge back for their attendance.  we are currently completing the curriculum design and will have information  on the full curriculum available in february. this will include options  other than \" classrom setting \" for development.  please respond back to gracie presas by february 1 with your names. if you  have further questions, please contact me at 3-7394. we are really excited  that we have this available and hope that your clients will find it to be  valuable.  the following are half-day sessions. supervisors may sign up for any or all  depending on their need. it would be helpful if supervisors attend a minimum  of two modules.  date module time target audience  feb. 8 meeting leadership challenges 8-12 am supervisors with less  than 6 months experience  working styles 1-5 pm any supervisor  feb. 9 coaching to performance 8-12 am any supervisor  motivating for results 1-5 pm any supervisor  feb. 10 communicating effectively 8-12 am any employee  delegating and directing 1-5 pm any supervisor\n",
      "0\n",
      "0001.2000-01-17.beck\n",
      "key hr issues going forward  a) year end reviews-report needs generating like mid-year documenting business unit performance on review completion-david to john;  b) work out or plan generation for the nim/issues employees-david to john;  c) hpl transition issues-ongoing.  officially transferred.  regards  delainey\n",
      "0\n",
      "0001.2001-02-07.kitchen\n",
      "re: quasi  good morning,  i'd love to go get some coffee with you, but remember that annoying project that mike etringer wants me to work on for him? this morning i am kinda under some pressure to hurry up and try to get some stuff figured out so i really don't have much spare time right now. ja would flip out if i left for coffee now. maybe later this afternoon? or tomorrow morning? anyhow, another ride sounds really cool. i had lots of fun. and yes, it would be cooler if i didn't have to worry about work. let me know when you have extra time to go for a ride.  my weekend was pretty fun. i weed-wacked (is that a word?) my yard for the first time. it looks so bad. i so don't know anything about lawn care. also i planted some herbs and stuff in my yard which i am sure my dog will destroy, but it s worth a try. oh yeah, i also bought a snowboard. it's pretty cool. i bought some step-in switch boots, too. cool, eh?  so i'll talk to you later. have a great day.\n",
      "0\n",
      "0001.2001-04-02.williams\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import sys\n",
    "import re\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "message, spam_truth, email_ids = [], [], []\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "filename = 'enronemail_1h.txt'\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for num,line in enumerate(myfile.readlines()):\n",
    "        fields=line.split('\\t') #parse line into separate fields\n",
    "        subject_and_body=\" \".join(fields[-2:]).strip()#parse the subject and body fields from the line, and combine into one string\n",
    "        if len(fields) == 4:\n",
    "            message.append(subject_and_body)\n",
    "            spam_truth.append(int(fields[1]))\n",
    "            email_ids.append(fields[0])\n",
    "\n",
    "for i in range (5):              ### print out data to make sure it displays expected format\n",
    "    print (message[i])\n",
    "    print (spam_truth[i])\n",
    "    print (email_ids[i])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 5319)\n"
     ]
    }
   ],
   "source": [
    "### Vectorize train_data and call it matrix X\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "train = vectorizer.fit_transform(message)\n",
    "print (train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(spam_truth)\n",
    "print (labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciKit-Learn Metrics:\n",
      "F1: 1.0\n",
      "Confusion matrix:\n",
      "[[55  0]\n",
      " [ 0 43]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:664: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/venamax/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#### Let's now define a MultiNomial classifier testing certain alphas and finding the optimal\n",
    "alphas = [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "max_mn_f1, max_alpha = 0,0\n",
    "for i in range(len(alphas)):\n",
    "    mn = MultinomialNB(alpha=alphas[i])\n",
    "    mn.fit(train,labels)\n",
    "    y_true = labels\n",
    "    y_pred = mn.predict(train)\n",
    "    if metrics.f1_score(y_true, y_pred, average ='weighted') > max_mn_f1:\n",
    "        max_mn_f1 = metrics.f1_score(y_true, y_pred, average ='weighted')\n",
    "        max_alpha = alphas[i]\n",
    "\n",
    "mn = MultinomialNB(alpha=max_alpha)\n",
    "mn.fit(train,labels)\n",
    "y_true = labels\n",
    "y_pred = mn.predict(train)\n",
    "print ('SciKit-Learn Metrics:')\n",
    "print ('F1:',(metrics.f1_score(y_true, y_pred, average ='weighted')))\n",
    "print ('Confusion matrix:')\n",
    "print ((confusion_matrix(y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HW 2.6</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HHW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)</h2>\n",
    "—  Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "-  Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.7 OPTIONAL (note this exercise is a stretch HW and optional)</h2>\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "--- Line 1 contains the subject\n",
    "--- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.8 OPTIONAL</h2>\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    " -- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    " -- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW2.8.1 OPTIONAL</h2>\n",
    "—  Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "-  Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>=====================\n",
    "END OF HOMEWORK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
