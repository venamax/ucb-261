{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b>Homework 1</b></h1>\n",
    "<i>completed by Alejandro J. Rojas</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.0.0.</h2> Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.0.1.</h2>In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HW1.1.</h2> Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########  Done  #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.6</h2> Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.2.</h2>Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "records = 0\n",
    "words = 0\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "#We set to look for the word on each line assuming different ways in which it could be present\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)\n",
    "        records = records + 1\n",
    "        for i in range (len(record)):\n",
    "            bagofwords = re.split(\" \",record[i])\n",
    "            for word in bagofwords:\n",
    "                words = words + 1\n",
    "                for keyword in findwords:\n",
    "                    if keyword in word:\n",
    "                        count = count + 1\n",
    "            \n",
    "\n",
    "##print '# of Records analized',records\n",
    "##print '# of Words analized', words\n",
    "##print '# of Ocurrences', count\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                sum = sum + int(line)\n",
    "                \n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"found [$numOfInstances] [$wordlist]\" ## Report  how many were found\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pNaiveBayes.sh m wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [10] [assistance]\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.3.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            ########## Variables to collect and measure #########\n",
    "            records = 0                                    ### Each record corresponds to a unique email\n",
    "            words = 0                                      ### Words written in all emails incluidng Subject \n",
    "            spam_records, spam_words, spam_count = 0,0,0   ### Spam email count, words in spam email, user-specified word count\n",
    "            ham_records, ham_words, ham_count = 0, 0, 0    ### Same as above but for not spam emails\n",
    "\n",
    "\n",
    "            records += 1                                   ### add one the the total sum of emails\n",
    "            if int(record[1]) == 1:                        ### If the email is labeled as spam\n",
    "                spam_records += 1                          ### add one to the email spam count\n",
    "                for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                    bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email                \n",
    "                    for word in bagofwords:                ### For each word\n",
    "                        words += 1                         ### add one to the total sum of words\n",
    "                        spam_words += 1                    ### add one to the total sum of spam words  \n",
    "                        for keyword in findwords:          ### for each word specified by user\n",
    "                            if keyword in word:            ### If there's a match then\n",
    "                                spam_count += 1            ### add one to the user specified word count as spam\n",
    "                                \n",
    "            else:                                          ### If email is not labeled as spam\n",
    "                ham_records +=1                            ### add one to the email ham count\n",
    "                for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                    bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email                \n",
    "                    for word in bagofwords:                ### For each word\n",
    "                        words += 1                         ### add one to the total sum of words\n",
    "                        ham_words += 1                     ### add one to the total sum of ham words  \n",
    "                        for keyword in findwords:          ### for each word specified by user\n",
    "                            if keyword in word:            ### If there's a match then\n",
    "                                ham_count += 1             ### add one to the user specified word count as ham\n",
    "                    \n",
    "            record_id = record[0]\n",
    "            truth = record[1]\n",
    "            print spam_count, \" \", spam_words, \" \",  spam_records, \" \",  \\\n",
    "                  ham_count, \" \", ham_words, \" \",  ham_records, \" \", \\\n",
    "                  words, \" \", records, \" \",  record_id, \" \", truth      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.3-1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum_spam_records, sum_spam_words, sum_spam_count = 0,0,0\n",
    "sum_ham_records, sum_ham_words, sum_ham_count = 0,0,0\n",
    "sum_records,sum_words = 0,0\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                factors = re.split(\" \", line)\n",
    "                sum_spam_count += int(factors[0])           ## sum up every time the word was found in a spam\n",
    "                sum_spam_words += int(factors[3])           ## sum up all words from spams\n",
    "                sum_spam_records+= int(factors[6])          ## sum up all emails labeled as spam\n",
    "                sum_ham_count  += int(factors[9])           ## sum up every time the word was found in a ham\n",
    "                sum_ham_words += int(factors[12])           ## sum up all words from hams\n",
    "                sum_ham_records += int(factors[15])         ## sum up all emails labeled as ham\n",
    "                sum_words += int(factors[18])               ## sum all words from all emails\n",
    "                sum_records += int(factors[21])             ## sum all emails\n",
    "                \n",
    "\n",
    "prior_spam = float(sum_spam_records)/float(sum_records)     ## prior prob of a spam email\n",
    "prior_ham = float(sum_ham_records)/float(sum_records)       ## prior prob of a ham email\n",
    "prob_word_spam = float(sum_spam_count)/float(sum_spam_words)## prob of word given that email is spam\n",
    "prob_word_ham = float(sum_ham_count)/float(sum_ham_words)   ## prob of word given that email is ham\n",
    "\n",
    "##check_prior = prior_spam + prior_ham                        ## check priors -> sum to 1\n",
    "##check_words = float(sum_words)/float(sum_spam_words+sum_ham_words) ## check probabilities of a word -> sum to 1\n",
    "##check_spam = prob_word_spam*float(sum_spam_words)/float(sum_spam_count) ## check spam counts -> sum to 1\n",
    "##check_ham = prob_word_ham*float(sum_ham_words)/float(sum_ham_count) ## check ham count -> sum to 1\n",
    "sum_count = sum_spam_count+sum_ham_count\n",
    "\n",
    "print \"Summary of Data\"\n",
    "print '%4s'%sum_records ,'emails examined, containing %6s'%sum_words, 'words, we found %3s'%sum_count ,'matches.'\n",
    "\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS'\n",
    "for file in filenames:                                      \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                data = re.split(\" \", line)\n",
    "                record_id = data[24]\n",
    "                y_true = data[27][0]\n",
    "                count = int(data[0]) + int(data[9])\n",
    "                p_spam = prior_spam*prob_word_spam**count\n",
    "                p_ham = prior_ham*prob_word_ham**count\n",
    "                if p_spam > p_ham:\n",
    "                    y_pred = 1\n",
    "                else:\n",
    "                    y_pred = 0\n",
    "                    \n",
    "                print  '%30s' %record_id, '%10s' %y_true, '%10s' %y_pred\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"NB Classifier based on word(s): $wordlist\" ## Print out words \n",
    "echo \"$numOfInstances\" ## Print out output data\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier based on word(s): assistance\r\n",
      "Summary of Data\r\n",
      "  98 emails examined, containing  35352 words, we found   9 matches.\r\n",
      "                            ID      TRUTH      CLASS\r\n",
      "        0001.1999-12-10.farmer          0          0\r\n",
      "      0001.1999-12-10.kaminski          0          0\r\n",
      "          0001.2000-01-17.beck          0          0\r\n",
      "       0001.2001-02-07.kitchen          0          0\r\n",
      "      0001.2001-04-02.williams          0          0\r\n",
      "        0002.1999-12-13.farmer          0          0\r\n",
      "       0002.2001-02-07.kitchen          0          0\r\n",
      "     0002.2001-05-25.SA_and_HP          1          0\r\n",
      "            0002.2003-12-18.GP          1          0\r\n",
      "            0002.2004-08-01.BG          1          1\r\n",
      "      0003.1999-12-10.kaminski          0          0\r\n",
      "        0003.1999-12-14.farmer          0          0\r\n",
      "          0003.2000-01-17.beck          0          0\r\n",
      "       0003.2001-02-08.kitchen          0          0\r\n",
      "            0003.2003-12-18.GP          1          0\r\n",
      "            0003.2004-08-01.BG          1          0\r\n",
      "      0004.1999-12-10.kaminski          0          1\r\n",
      "        0004.1999-12-14.farmer          0          0\r\n",
      "      0004.2001-04-02.williams          0          0\r\n",
      "     0004.2001-06-12.SA_and_HP          1          0\r\n",
      "            0004.2004-08-01.BG          1          0\r\n",
      "      0005.1999-12-12.kaminski          0          1\r\n",
      "        0005.1999-12-14.farmer          0          0\r\n",
      "         0005.2000-06-06.lokay          0          0\r\n",
      "       0005.2001-02-08.kitchen          0          0\r\n",
      "     0005.2001-06-23.SA_and_HP          1          0\r\n",
      "            0005.2003-12-18.GP          1          0\r\n",
      "      0006.1999-12-13.kaminski          0          0\r\n",
      "       0006.2001-02-08.kitchen          0          0\r\n",
      "      0006.2001-04-03.williams          0          0\r\n",
      "     0006.2001-06-25.SA_and_HP          1          0\r\n",
      "            0006.2003-12-18.GP          1          0\r\n",
      "            0006.2004-08-01.BG          1          0\r\n",
      "      0007.1999-12-13.kaminski          0          0\r\n",
      "        0007.1999-12-14.farmer          0          0\r\n",
      "          0007.2000-01-17.beck          0          0\r\n",
      "       0007.2001-02-09.kitchen          0          0\r\n",
      "            0007.2003-12-18.GP          1          0\r\n",
      "            0007.2004-08-01.BG          1          0\r\n",
      "       0008.2001-02-09.kitchen          0          0\r\n",
      "     0008.2001-06-12.SA_and_HP          1          0\r\n",
      "     0008.2001-06-25.SA_and_HP          1          0\r\n",
      "            0008.2003-12-18.GP          1          0\r\n",
      "            0008.2004-08-01.BG          1          0\r\n",
      "      0009.1999-12-13.kaminski          0          0\r\n",
      "        0009.1999-12-14.farmer          0          0\r\n",
      "         0009.2000-06-07.lokay          0          0\r\n",
      "       0009.2001-02-09.kitchen          0          0\r\n",
      "            0009.2003-12-18.GP          1          0\r\n",
      "        0010.1999-12-14.farmer          0          0\r\n",
      "      0010.1999-12-14.kaminski          0          0\r\n",
      "       0010.2001-02-09.kitchen          0          0\r\n",
      "     0010.2001-06-28.SA_and_HP          1          1\r\n",
      "            0010.2003-12-18.GP          1          0\r\n",
      "            0010.2004-08-01.BG          1          0\r\n",
      "        0011.1999-12-14.farmer          0          0\r\n",
      "     0011.2001-06-28.SA_and_HP          1          0\r\n",
      "     0011.2001-06-29.SA_and_HP          1          0\r\n",
      "            0011.2003-12-18.GP          1          0\r\n",
      "            0011.2004-08-01.BG          1          0\r\n",
      "        0012.1999-12-14.farmer          0          0\r\n",
      "      0012.1999-12-14.kaminski          0          0\r\n",
      "          0012.2000-01-17.beck          0          0\r\n",
      "         0012.2000-06-08.lokay          0          0\r\n",
      "       0012.2001-02-09.kitchen          0          0\r\n",
      "            0012.2003-12-19.GP          1          0\r\n",
      "        0013.1999-12-14.farmer          0          0\r\n",
      "      0013.1999-12-14.kaminski          0          0\r\n",
      "      0013.2001-04-03.williams          0          0\r\n",
      "     0013.2001-06-30.SA_and_HP          1          0\r\n",
      "            0013.2004-08-01.BG          1          1\r\n",
      "      0014.1999-12-14.kaminski          0          0\r\n",
      "        0014.1999-12-15.farmer          0          0\r\n",
      "       0014.2001-02-12.kitchen          0          0\r\n",
      "     0014.2001-07-04.SA_and_HP          1          0\r\n",
      "            0014.2003-12-19.GP          1          0\r\n",
      "            0014.2004-08-01.BG          1          0\r\n",
      "      0015.1999-12-14.kaminski          0          0\r\n",
      "        0015.1999-12-15.farmer          0          0\r\n",
      "         0015.2000-06-09.lokay          0          0\r\n",
      "       0015.2001-02-12.kitchen          0          0\r\n",
      "     0015.2001-07-05.SA_and_HP          1          0\r\n",
      "            0015.2003-12-19.GP          1          0\r\n",
      "        0016.1999-12-15.farmer          0          0\r\n",
      "       0016.2001-02-12.kitchen          0          0\r\n",
      "     0016.2001-07-05.SA_and_HP          1          0\r\n",
      "     0016.2001-07-06.SA_and_HP          1          0\r\n",
      "            0016.2003-12-19.GP          1          0\r\n",
      "            0016.2004-08-01.BG          1          0\r\n",
      "      0017.1999-12-14.kaminski          0          0\r\n",
      "          0017.2000-01-17.beck          0          0\r\n",
      "      0017.2001-04-03.williams          0          0\r\n",
      "            0017.2003-12-18.GP          1          0\r\n",
      "            0017.2004-08-01.BG          1          0\r\n",
      "            0017.2004-08-02.BG          1          0\r\n",
      "      0018.1999-12-14.kaminski          0          0\r\n",
      "     0018.2001-07-13.SA_and_HP          1          1\r\n",
      "            0018.2003-12-18.GP          1          1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.4.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier based on word(s): assistance valium enlargementWithATypo\r\n",
      "Summary of Data\r\n",
      "  98 emails examined, containing   35352 words, we found   12 matches.\r\n",
      "                            ID      TRUTH      CLASS\r\n",
      "        0001.1999-12-10.farmer          0          0\r\n",
      "      0001.1999-12-10.kaminski          0          0\r\n",
      "          0001.2000-01-17.beck          0          0\r\n",
      "       0001.2001-02-07.kitchen          0          0\r\n",
      "      0001.2001-04-02.williams          0          0\r\n",
      "        0002.1999-12-13.farmer          0          0\r\n",
      "       0002.2001-02-07.kitchen          0          0\r\n",
      "     0002.2001-05-25.SA_and_HP          1          0\r\n",
      "            0002.2003-12-18.GP          1          0\r\n",
      "            0002.2004-08-01.BG          1          1\r\n",
      "      0003.1999-12-10.kaminski          0          0\r\n",
      "        0003.1999-12-14.farmer          0          0\r\n",
      "          0003.2000-01-17.beck          0          0\r\n",
      "       0003.2001-02-08.kitchen          0          0\r\n",
      "            0003.2003-12-18.GP          1          0\r\n",
      "            0003.2004-08-01.BG          1          0\r\n",
      "      0004.1999-12-10.kaminski          0          1\r\n",
      "        0004.1999-12-14.farmer          0          0\r\n",
      "      0004.2001-04-02.williams          0          0\r\n",
      "     0004.2001-06-12.SA_and_HP          1          0\r\n",
      "            0004.2004-08-01.BG          1          0\r\n",
      "      0005.1999-12-12.kaminski          0          1\r\n",
      "        0005.1999-12-14.farmer          0          0\r\n",
      "         0005.2000-06-06.lokay          0          0\r\n",
      "       0005.2001-02-08.kitchen          0          0\r\n",
      "     0005.2001-06-23.SA_and_HP          1          0\r\n",
      "            0005.2003-12-18.GP          1          0\r\n",
      "      0006.1999-12-13.kaminski          0          0\r\n",
      "       0006.2001-02-08.kitchen          0          0\r\n",
      "      0006.2001-04-03.williams          0          0\r\n",
      "     0006.2001-06-25.SA_and_HP          1          0\r\n",
      "            0006.2003-12-18.GP          1          0\r\n",
      "            0006.2004-08-01.BG          1          0\r\n",
      "      0007.1999-12-13.kaminski          0          0\r\n",
      "        0007.1999-12-14.farmer          0          0\r\n",
      "          0007.2000-01-17.beck          0          0\r\n",
      "       0007.2001-02-09.kitchen          0          0\r\n",
      "            0007.2003-12-18.GP          1          0\r\n",
      "            0007.2004-08-01.BG          1          0\r\n",
      "       0008.2001-02-09.kitchen          0          0\r\n",
      "     0008.2001-06-12.SA_and_HP          1          0\r\n",
      "     0008.2001-06-25.SA_and_HP          1          0\r\n",
      "            0008.2003-12-18.GP          1          0\r\n",
      "            0008.2004-08-01.BG          1          0\r\n",
      "      0009.1999-12-13.kaminski          0          0\r\n",
      "        0009.1999-12-14.farmer          0          0\r\n",
      "         0009.2000-06-07.lokay          0          0\r\n",
      "       0009.2001-02-09.kitchen          0          0\r\n",
      "            0009.2003-12-18.GP          1          1\r\n",
      "        0010.1999-12-14.farmer          0          0\r\n",
      "      0010.1999-12-14.kaminski          0          0\r\n",
      "       0010.2001-02-09.kitchen          0          0\r\n",
      "     0010.2001-06-28.SA_and_HP          1          1\r\n",
      "            0010.2003-12-18.GP          1          0\r\n",
      "            0010.2004-08-01.BG          1          0\r\n",
      "        0011.1999-12-14.farmer          0          0\r\n",
      "     0011.2001-06-28.SA_and_HP          1          0\r\n",
      "     0011.2001-06-29.SA_and_HP          1          0\r\n",
      "            0011.2003-12-18.GP          1          0\r\n",
      "            0011.2004-08-01.BG          1          0\r\n",
      "        0012.1999-12-14.farmer          0          0\r\n",
      "      0012.1999-12-14.kaminski          0          0\r\n",
      "          0012.2000-01-17.beck          0          0\r\n",
      "         0012.2000-06-08.lokay          0          0\r\n",
      "       0012.2001-02-09.kitchen          0          0\r\n",
      "            0012.2003-12-19.GP          1          0\r\n",
      "        0013.1999-12-14.farmer          0          0\r\n",
      "      0013.1999-12-14.kaminski          0          0\r\n",
      "      0013.2001-04-03.williams          0          0\r\n",
      "     0013.2001-06-30.SA_and_HP          1          0\r\n",
      "            0013.2004-08-01.BG          1          1\r\n",
      "      0014.1999-12-14.kaminski          0          0\r\n",
      "        0014.1999-12-15.farmer          0          0\r\n",
      "       0014.2001-02-12.kitchen          0          0\r\n",
      "     0014.2001-07-04.SA_and_HP          1          0\r\n",
      "            0014.2003-12-19.GP          1          0\r\n",
      "            0014.2004-08-01.BG          1          0\r\n",
      "      0015.1999-12-14.kaminski          0          0\r\n",
      "        0015.1999-12-15.farmer          0          0\r\n",
      "         0015.2000-06-09.lokay          0          0\r\n",
      "       0015.2001-02-12.kitchen          0          0\r\n",
      "     0015.2001-07-05.SA_and_HP          1          0\r\n",
      "            0015.2003-12-19.GP          1          0\r\n",
      "        0016.1999-12-15.farmer          0          0\r\n",
      "       0016.2001-02-12.kitchen          0          0\r\n",
      "     0016.2001-07-05.SA_and_HP          1          0\r\n",
      "     0016.2001-07-06.SA_and_HP          1          0\r\n",
      "            0016.2003-12-19.GP          1          1\r\n",
      "            0016.2004-08-01.BG          1          0\r\n",
      "      0017.1999-12-14.kaminski          0          0\r\n",
      "          0017.2000-01-17.beck          0          0\r\n",
      "      0017.2001-04-03.williams          0          0\r\n",
      "            0017.2003-12-18.GP          1          0\r\n",
      "            0017.2004-08-01.BG          1          1\r\n",
      "            0017.2004-08-02.BG          1          0\r\n",
      "      0018.1999-12-14.kaminski          0          0\r\n",
      "     0018.2001-07-13.SA_and_HP          1          1\r\n",
      "            0018.2003-12-18.GP          1          1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.5.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = sys.argv[1]\n",
    "findwords = []\n",
    "countwords = []\n",
    "\n",
    "\n",
    "def line_preprocessor(s):\n",
    "    elim_array = ['\\s,\\s', \n",
    "                 '\\s&\\s', \n",
    "                 '\\s\"\\s', \n",
    "                  '\\s|\\s', \n",
    "                '\\s\\\\s',\n",
    "                  '\\s?\\s', \n",
    "                  ' \\s-\\s', \n",
    "                  '\\s!\\s', \n",
    "                 '\\d'\n",
    "                  ]\n",
    "    for elim_word in  elim_array:    \n",
    "        s = re.sub(elim_word,'', s, flags=re.IGNORECASE)\n",
    "    return s\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        record = line_preprocessor(line)                   ### Preprocess line\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            for i in range (2,len(record)):                ### Starting from Subject to the Content               \n",
    "                bagofwords = re.split(\" \",record[i])       ### Collect all words present on each email               \n",
    "                for word in bagofwords:                \n",
    "                    findwords.append(word)                 ### Add each word to list\n",
    "    findwords = sorted(set(findwords))                     ### Eliminate repeats and sort\n",
    "\n",
    "with open (filename, \"r\") as myfile:    \n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            record_wordlist =[]                            ### initiate a list of words that show up in email\n",
    "            for i in range (2,len(record)):                ### Starting from Subject to the Content               \n",
    "                bagofwords = re.split(\" \",record[i])       ### Collect all words present on each email               \n",
    "                for word in bagofwords:\n",
    "                    record_wordlist.append(word)           ### place each word on the list\n",
    "            \n",
    "            for keyword in findwords:                     \n",
    "                count = 0        \n",
    "                for word in record_wordlist:               ### review each word in the email\n",
    "                    if keyword in word:                    ### add one every time we find a word in the email\n",
    "                        count += 1\n",
    "                    print record[0], '\\t', record[1],'\\t', word,'\\t',count ### print out each word tuple \n",
    "                    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "word_tuple, id_list, word_list = [], [], []\n",
    "sum_spam_records, sum_ham_records, sum_records = 0,0,0\n",
    "sum_spam_words, sum_ham_words, sum_words = 0,0,0\n",
    "wordcount_spam, wordcount_ham, wordcount = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                data = re.split(r'\\t+', line)\n",
    "                word_tuple.append((data[0],data[1],data[2],data[3]))\n",
    "                id_list.append(data[0])\n",
    "                word_list.append(data[2])\n",
    "                sum_words += int(data[3])\n",
    "                \n",
    "unique_ids = sorted(set(id_list))\n",
    "vocabulary = sorted(set(word_list))\n",
    "for email in unique_ids:\n",
    "    for i in range(len(word_tuple)):\n",
    "        if email == word_tuple[i][0]:\n",
    "            y_true = { 'email': word_tuple[i][0],\n",
    "                      'truth': word_tuple[i][1]\n",
    "                      }\n",
    "    y_true_list.append(y_true) \n",
    "\n",
    "for i in range(len(word_tuple)):\n",
    "    if word_tuple[i][1] == 1:\n",
    "        spam_ids.append(word_tuple[i][0])\n",
    "        sum_spam_words += word_tuple[i][3]\n",
    "    else:\n",
    "        ham_ids.append(word_tuple[i][0])\n",
    "        sum_ham_words += word_tuple[i][3]\n",
    "\n",
    "sum_spam_records = len(set(spam_ids))\n",
    "sum_ham_records = len(set(ham_ids))\n",
    "sum_records = len(unique_ids)\n",
    "\n",
    "prior_spam = float(sum_spam_records)/float(sum_records)     ## prior prob of a spam email\n",
    "prior_ham = float(sum_ham_records)/float(sum_records)       ## prior prob of a ham email\n",
    "\n",
    "for word in vocabulary:\n",
    "    count, spam_count, ham_count = 0,0,0\n",
    "    for i in range(len(word_tuple)):\n",
    "        if word == word_tuple[i][2]:\n",
    "            count+= word_tuple[i][3]\n",
    "            if word_tuple[i][1] == 1:\n",
    "                spam_count += 1\n",
    "            else:\n",
    "                ham_count += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    wordcount.append(count)\n",
    "    wordcount_spam.append(spam_count)\n",
    "    wordcount_ham.append(ham_count)\n",
    "    sum_words = sum(wordcount)\n",
    "    sum_spam_words = sum(wordcount_spam)\n",
    "    sum_ham_word =sum(wordcount_ham)\n",
    "\n",
    "for email in unique_ids:\n",
    "    wordcount_in_email =[]\n",
    "    for word in vocabulary:\n",
    "        found = False\n",
    "        for i in range(len(word_tuple)):\n",
    "            if email == word_tuple[i][0]:            \n",
    "                if word == word_tuple[i][2] :\n",
    "                    wordcount_in_email.append(word_tuple[i][3])\n",
    "                    found = True\n",
    "        if found == False:\n",
    "             wordcount_in_email.append(0)\n",
    "    \n",
    "    for i in range(len(vocabulary)):\n",
    "        cond_prob_spam += float(wordcount_spam[i])/float(sum_spam_words)\n",
    "        cond_prob_ham += float(wordcount_ham[i])/float(sum_ham_words)\n",
    "        p_spam += prior_spam*cond_prob_spam**float(wordcount_in_email[i])\n",
    "        p_ham += prior_ham*cond_prob_ham**float(wordcount_in_email[i])\n",
    "    \n",
    "    if p_spam > p_ham:\n",
    "        y_pred = 1\n",
    "    else:\n",
    "        y_pred = 0\n",
    "    \n",
    "\n",
    "\n",
    "##check_prior = prior_spam + prior_ham                        ## check priors -> sum to 1\n",
    "##check_words = float(sum_words)/float(sum_spam_words+sum_ham_words) ## check probabilities of a word -> sum to 1\n",
    "##check_spam = prob_word_spam*float(sum_spam_words)/float(sum_spam_count) ## check spam counts -> sum to 1\n",
    "##check_ham = prob_word_ham*float(sum_ham_words)/float(sum_ham_count) ## check ham count -> sum to 1\n",
    "##print word_tuple\n",
    "\n",
    "print \"Summary of Data\"\n",
    "print '%4s'%sum_records ,'emails examined, containing %6s'%sum_words, 'using a dictionary of %6s' %size_dict\n",
    "\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS'\n",
    "for file in filenames:                                      \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                data = re.split(\" - \", line)\n",
    "                record_id = data[0]\n",
    "                y_true = data[1][0]\n",
    "                count_vector = data[2]\n",
    "                cond_prob_spam = []\n",
    "                cond_prob_ham = []\n",
    "                for i in range (len(wordcount)):\n",
    "                    prob_spam = float((count_vector[i]+1))/float(word_count_spam[i])**int(wordcount[i])\n",
    "                    prob_ham = float((count_vector[i]+1))/float(word_count_ham[i])**int(wordcount[i])\n",
    "                    p_spam += prior_spam*prob_spam\n",
    "                    p_ham += prior_ham*prob_ham\n",
    "                   \n",
    "                if p_spam > p_ham:\n",
    "                    y_pred = 1\n",
    "                else:\n",
    "                    y_pred = 0\n",
    "                    \n",
    "                print  '%30s' %record_id, '%10s' %y_true, '%10s' %y_pred\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"NB Classifier\" ## Print out words \n",
    "echo \"$numOfInstances\" ## Print out output data\n",
    "## clean up the data chunks and temporary count files\n",
    "##\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/pexpect/__init__.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize)\u001b[0m\n\u001b[1;32m   1465\u001b[0m         return self.expect_loop(searcher_re(pattern_list),\n\u001b[0;32m-> 1466\u001b[0;31m                 timeout, searchwindowsize)\n\u001b[0m\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/pexpect/__init__.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, searcher, timeout, searchwindowsize)\u001b[0m\n\u001b[1;32m   1534\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m                 \u001b[0mfreshlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/pexpect/__init__.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/pexpect/__init__.py\u001b[0m in \u001b[0;36m__select\u001b[0;34m(self, iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-20807ca32cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pNaiveBayes.sh 5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2385\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2387\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/pexpect/__init__.py\u001b[0m in \u001b[0;36msendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         automatically appended. Returns number of bytes written. '''\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinesep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/venamax/anaconda/lib/python3.5/site-packages/pexpect/__init__.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         log. '''\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelaybeforesend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coerce_send_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
