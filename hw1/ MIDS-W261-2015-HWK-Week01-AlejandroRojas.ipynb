{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b>Homework 1</b></h1>\n",
    "<i>completed by Alejandro J. Rojas</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.0.0.</h2> Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.0.1.</h2>In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HW1.1.</h2> Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########  Done  #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.6</h2> Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.2.</h2>Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "records = 0\n",
    "words = 0\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "#We set to look for the word on each line assuming different ways in which it could be present\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)\n",
    "        records = records + 1\n",
    "        for i in range (len(record)):\n",
    "            bagofwords = re.split(\" \",record[i])\n",
    "            for word in bagofwords:\n",
    "                words = words + 1\n",
    "                for keyword in findwords:\n",
    "                    if keyword in word:\n",
    "                        count = count + 1\n",
    "            \n",
    "\n",
    "##print '# of Records analized',records\n",
    "##print '# of Words analized', words\n",
    "##print '# of Ocurrences', count\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                sum = sum + int(line)\n",
    "                \n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"found [$numOfInstances] [$wordlist]\" ## Report  how many were found\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pNaiveBayes.sh m wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [10] [assistance]\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.3.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            ########## Variables to collect and measure #########\n",
    "            records = 0                                    ### Each record corresponds to a unique email\n",
    "            words = 0                                      ### Words written in all emails incluidng Subject \n",
    "            spam_records, spam_words, spam_count = 0,0,0   ### Spam email count, words in spam email, user-specified word count\n",
    "            ham_records, ham_words, ham_count = 0, 0, 0    ### Same as above but for not spam emails\n",
    "\n",
    "\n",
    "            records += 1                                   ### add one the the total sum of emails\n",
    "            if int(record[1]) == 1:                        ### If the email is labeled as spam\n",
    "                spam_records += 1                          ### add one to the email spam count\n",
    "                for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                    bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email                \n",
    "                    for word in bagofwords:                ### For each word\n",
    "                        words += 1                         ### add one to the total sum of words\n",
    "                        spam_words += 1                    ### add one to the total sum of spam words  \n",
    "                        for keyword in findwords:          ### for each word specified by user\n",
    "                            if keyword in word:            ### If there's a match then\n",
    "                                spam_count += 1            ### add one to the user specified word count as spam\n",
    "                                \n",
    "            else:                                          ### If email is not labeled as spam\n",
    "                ham_records +=1                            ### add one to the email ham count\n",
    "                for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                    bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email                \n",
    "                    for word in bagofwords:                ### For each word\n",
    "                        words += 1                         ### add one to the total sum of words\n",
    "                        ham_words += 1                     ### add one to the total sum of ham words  \n",
    "                        for keyword in findwords:          ### for each word specified by user\n",
    "                            if keyword in word:            ### If there's a match then\n",
    "                                ham_count += 1             ### add one to the user specified word count as ham\n",
    "                    \n",
    "\n",
    "            print spam_count, \" \", spam_words, \" \", spam_records, \" \", ham_count, \" \", ham_words, \" \", ham_records, \" \", words, \" \", records, \" \",record[0], \" \", record[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.3-1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum_spam_records, sum_spam_words, sum_spam_count = 0,0,0\n",
    "sum_ham_records, sum_ham_words, sum_ham_count = 0,0,0\n",
    "sum_records,sum_words = 0,0\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                factors = re.split(\" \", line)\n",
    "                sum_spam_count += int(factors[0])           ## sum up every time the word was found in a spam\n",
    "                sum_spam_words += int(factors[3])           ## sum up all words from spams\n",
    "                sum_spam_records+= int(factors[6])          ## sum up all emails labeled as spam\n",
    "                sum_ham_count  += int(factors[9])           ## sum up every time the word was found in a ham\n",
    "                sum_ham_words += int(factors[12])           ## sum up all words from hams\n",
    "                sum_ham_records += int(factors[15])         ## sum up all emails labeled as ham\n",
    "                sum_words += int(factors[18])               ## sum all words from all emails\n",
    "                sum_records += int(factors[21])             ## sum all emails\n",
    "                \n",
    "\n",
    "prior_spam = float(sum_spam_records)/float(sum_records)     ## prior prob of a spam email\n",
    "prior_ham = float(sum_ham_records)/float(sum_records)       ## prior prob of a ham email\n",
    "prob_word_spam = float(sum_spam_count)/float(sum_spam_words)## prob of word given that email is spam\n",
    "prob_word_ham = float(sum_ham_count)/float(sum_ham_words)   ## prob of word given that email is ham\n",
    "\n",
    "##check_prior = prior_spam + prior_ham                        ## check priors -> sum to 1\n",
    "##check_words = float(sum_words)/float(sum_spam_words+sum_ham_words) ## check probabilities of a word -> sum to 1\n",
    "##check_spam = prob_word_spam*float(sum_spam_words)/float(sum_spam_count) ## check spam counts -> sum to 1\n",
    "##check_ham = prob_word_ham*float(sum_ham_words)/float(sum_ham_count) ## check ham count -> sum to 1\n",
    "sum_count = sum_spam_count+sum_ham_count\n",
    "\n",
    "print \"Summary of Data\"\n",
    "print '%4s'%sum_records ,'emails examined, containing %6s'%sum_words, 'words, we found %3s'%sum_count ,'matches.'\n",
    "\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS'\n",
    "for file in filenames:                                      \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                data = re.split(\" \", line)\n",
    "                record_id = data[24]\n",
    "                y_true = data[27][0]\n",
    "                count = int(data[0]) + int(data[9])\n",
    "                p_spam = prior_spam*prob_word_spam**count\n",
    "                p_ham = prior_ham*prob_word_ham**count\n",
    "                if p_spam > p_ham:\n",
    "                    y_pred = 1\n",
    "                else:\n",
    "                    y_pred = 0\n",
    "                    \n",
    "                print  '%30s' %record_id, '%10s' %y_true, '%10s' %y_pred\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"NB Classifier based on word(s): $wordlist\" ## Print out words \n",
    "echo \"$numOfInstances\" ## Print out output data\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier based on word(s): assistance\r\n",
      "Summary of Data\r\n",
      "  98 emails examined, containing  35352 words, we found   9 matches.\r\n",
      "                            ID      TRUTH      CLASS\r\n",
      "        0001.1999-12-10.farmer          0          0\r\n",
      "      0001.1999-12-10.kaminski          0          0\r\n",
      "          0001.2000-01-17.beck          0          0\r\n",
      "       0001.2001-02-07.kitchen          0          0\r\n",
      "      0001.2001-04-02.williams          0          0\r\n",
      "        0002.1999-12-13.farmer          0          0\r\n",
      "       0002.2001-02-07.kitchen          0          0\r\n",
      "     0002.2001-05-25.SA_and_HP          1          0\r\n",
      "            0002.2003-12-18.GP          1          0\r\n",
      "            0002.2004-08-01.BG          1          1\r\n",
      "      0003.1999-12-10.kaminski          0          0\r\n",
      "        0003.1999-12-14.farmer          0          0\r\n",
      "          0003.2000-01-17.beck          0          0\r\n",
      "       0003.2001-02-08.kitchen          0          0\r\n",
      "            0003.2003-12-18.GP          1          0\r\n",
      "            0003.2004-08-01.BG          1          0\r\n",
      "      0004.1999-12-10.kaminski          0          1\r\n",
      "        0004.1999-12-14.farmer          0          0\r\n",
      "      0004.2001-04-02.williams          0          0\r\n",
      "     0004.2001-06-12.SA_and_HP          1          0\r\n",
      "            0004.2004-08-01.BG          1          0\r\n",
      "      0005.1999-12-12.kaminski          0          1\r\n",
      "        0005.1999-12-14.farmer          0          0\r\n",
      "         0005.2000-06-06.lokay          0          0\r\n",
      "       0005.2001-02-08.kitchen          0          0\r\n",
      "     0005.2001-06-23.SA_and_HP          1          0\r\n",
      "            0005.2003-12-18.GP          1          0\r\n",
      "      0006.1999-12-13.kaminski          0          0\r\n",
      "       0006.2001-02-08.kitchen          0          0\r\n",
      "      0006.2001-04-03.williams          0          0\r\n",
      "     0006.2001-06-25.SA_and_HP          1          0\r\n",
      "            0006.2003-12-18.GP          1          0\r\n",
      "            0006.2004-08-01.BG          1          0\r\n",
      "      0007.1999-12-13.kaminski          0          0\r\n",
      "        0007.1999-12-14.farmer          0          0\r\n",
      "          0007.2000-01-17.beck          0          0\r\n",
      "       0007.2001-02-09.kitchen          0          0\r\n",
      "            0007.2003-12-18.GP          1          0\r\n",
      "            0007.2004-08-01.BG          1          0\r\n",
      "       0008.2001-02-09.kitchen          0          0\r\n",
      "     0008.2001-06-12.SA_and_HP          1          0\r\n",
      "     0008.2001-06-25.SA_and_HP          1          0\r\n",
      "            0008.2003-12-18.GP          1          0\r\n",
      "            0008.2004-08-01.BG          1          0\r\n",
      "      0009.1999-12-13.kaminski          0          0\r\n",
      "        0009.1999-12-14.farmer          0          0\r\n",
      "         0009.2000-06-07.lokay          0          0\r\n",
      "       0009.2001-02-09.kitchen          0          0\r\n",
      "            0009.2003-12-18.GP          1          0\r\n",
      "        0010.1999-12-14.farmer          0          0\r\n",
      "      0010.1999-12-14.kaminski          0          0\r\n",
      "       0010.2001-02-09.kitchen          0          0\r\n",
      "     0010.2001-06-28.SA_and_HP          1          1\r\n",
      "            0010.2003-12-18.GP          1          0\r\n",
      "            0010.2004-08-01.BG          1          0\r\n",
      "        0011.1999-12-14.farmer          0          0\r\n",
      "     0011.2001-06-28.SA_and_HP          1          0\r\n",
      "     0011.2001-06-29.SA_and_HP          1          0\r\n",
      "            0011.2003-12-18.GP          1          0\r\n",
      "            0011.2004-08-01.BG          1          0\r\n",
      "        0012.1999-12-14.farmer          0          0\r\n",
      "      0012.1999-12-14.kaminski          0          0\r\n",
      "          0012.2000-01-17.beck          0          0\r\n",
      "         0012.2000-06-08.lokay          0          0\r\n",
      "       0012.2001-02-09.kitchen          0          0\r\n",
      "            0012.2003-12-19.GP          1          0\r\n",
      "        0013.1999-12-14.farmer          0          0\r\n",
      "      0013.1999-12-14.kaminski          0          0\r\n",
      "      0013.2001-04-03.williams          0          0\r\n",
      "     0013.2001-06-30.SA_and_HP          1          0\r\n",
      "            0013.2004-08-01.BG          1          1\r\n",
      "      0014.1999-12-14.kaminski          0          0\r\n",
      "        0014.1999-12-15.farmer          0          0\r\n",
      "       0014.2001-02-12.kitchen          0          0\r\n",
      "     0014.2001-07-04.SA_and_HP          1          0\r\n",
      "            0014.2003-12-19.GP          1          0\r\n",
      "            0014.2004-08-01.BG          1          0\r\n",
      "      0015.1999-12-14.kaminski          0          0\r\n",
      "        0015.1999-12-15.farmer          0          0\r\n",
      "         0015.2000-06-09.lokay          0          0\r\n",
      "       0015.2001-02-12.kitchen          0          0\r\n",
      "     0015.2001-07-05.SA_and_HP          1          0\r\n",
      "            0015.2003-12-19.GP          1          0\r\n",
      "        0016.1999-12-15.farmer          0          0\r\n",
      "       0016.2001-02-12.kitchen          0          0\r\n",
      "     0016.2001-07-05.SA_and_HP          1          0\r\n",
      "     0016.2001-07-06.SA_and_HP          1          0\r\n",
      "            0016.2003-12-19.GP          1          0\r\n",
      "            0016.2004-08-01.BG          1          0\r\n",
      "      0017.1999-12-14.kaminski          0          0\r\n",
      "          0017.2000-01-17.beck          0          0\r\n",
      "      0017.2001-04-03.williams          0          0\r\n",
      "            0017.2003-12-18.GP          1          0\r\n",
      "            0017.2004-08-01.BG          1          0\r\n",
      "            0017.2004-08-02.BG          1          0\r\n",
      "      0018.1999-12-14.kaminski          0          0\r\n",
      "     0018.2001-07-13.SA_and_HP          1          1\r\n",
      "            0018.2003-12-18.GP          1          1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.4.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier based on word(s): assistance valium enlargementWithATypo\r\n",
      "Summary of Data\r\n",
      "  98 emails examined, containing   35352 words, we found   12 matches.\r\n",
      "                            ID      TRUTH      CLASS\r\n",
      "        0001.1999-12-10.farmer          0          0\r\n",
      "      0001.1999-12-10.kaminski          0          0\r\n",
      "          0001.2000-01-17.beck          0          0\r\n",
      "       0001.2001-02-07.kitchen          0          0\r\n",
      "      0001.2001-04-02.williams          0          0\r\n",
      "        0002.1999-12-13.farmer          0          0\r\n",
      "       0002.2001-02-07.kitchen          0          0\r\n",
      "     0002.2001-05-25.SA_and_HP          1          0\r\n",
      "            0002.2003-12-18.GP          1          0\r\n",
      "            0002.2004-08-01.BG          1          1\r\n",
      "      0003.1999-12-10.kaminski          0          0\r\n",
      "        0003.1999-12-14.farmer          0          0\r\n",
      "          0003.2000-01-17.beck          0          0\r\n",
      "       0003.2001-02-08.kitchen          0          0\r\n",
      "            0003.2003-12-18.GP          1          0\r\n",
      "            0003.2004-08-01.BG          1          0\r\n",
      "      0004.1999-12-10.kaminski          0          1\r\n",
      "        0004.1999-12-14.farmer          0          0\r\n",
      "      0004.2001-04-02.williams          0          0\r\n",
      "     0004.2001-06-12.SA_and_HP          1          0\r\n",
      "            0004.2004-08-01.BG          1          0\r\n",
      "      0005.1999-12-12.kaminski          0          1\r\n",
      "        0005.1999-12-14.farmer          0          0\r\n",
      "         0005.2000-06-06.lokay          0          0\r\n",
      "       0005.2001-02-08.kitchen          0          0\r\n",
      "     0005.2001-06-23.SA_and_HP          1          0\r\n",
      "            0005.2003-12-18.GP          1          0\r\n",
      "      0006.1999-12-13.kaminski          0          0\r\n",
      "       0006.2001-02-08.kitchen          0          0\r\n",
      "      0006.2001-04-03.williams          0          0\r\n",
      "     0006.2001-06-25.SA_and_HP          1          0\r\n",
      "            0006.2003-12-18.GP          1          0\r\n",
      "            0006.2004-08-01.BG          1          0\r\n",
      "      0007.1999-12-13.kaminski          0          0\r\n",
      "        0007.1999-12-14.farmer          0          0\r\n",
      "          0007.2000-01-17.beck          0          0\r\n",
      "       0007.2001-02-09.kitchen          0          0\r\n",
      "            0007.2003-12-18.GP          1          0\r\n",
      "            0007.2004-08-01.BG          1          0\r\n",
      "       0008.2001-02-09.kitchen          0          0\r\n",
      "     0008.2001-06-12.SA_and_HP          1          0\r\n",
      "     0008.2001-06-25.SA_and_HP          1          0\r\n",
      "            0008.2003-12-18.GP          1          0\r\n",
      "            0008.2004-08-01.BG          1          0\r\n",
      "      0009.1999-12-13.kaminski          0          0\r\n",
      "        0009.1999-12-14.farmer          0          0\r\n",
      "         0009.2000-06-07.lokay          0          0\r\n",
      "       0009.2001-02-09.kitchen          0          0\r\n",
      "            0009.2003-12-18.GP          1          1\r\n",
      "        0010.1999-12-14.farmer          0          0\r\n",
      "      0010.1999-12-14.kaminski          0          0\r\n",
      "       0010.2001-02-09.kitchen          0          0\r\n",
      "     0010.2001-06-28.SA_and_HP          1          1\r\n",
      "            0010.2003-12-18.GP          1          0\r\n",
      "            0010.2004-08-01.BG          1          0\r\n",
      "        0011.1999-12-14.farmer          0          0\r\n",
      "     0011.2001-06-28.SA_and_HP          1          0\r\n",
      "     0011.2001-06-29.SA_and_HP          1          0\r\n",
      "            0011.2003-12-18.GP          1          0\r\n",
      "            0011.2004-08-01.BG          1          0\r\n",
      "        0012.1999-12-14.farmer          0          0\r\n",
      "      0012.1999-12-14.kaminski          0          0\r\n",
      "          0012.2000-01-17.beck          0          0\r\n",
      "         0012.2000-06-08.lokay          0          0\r\n",
      "       0012.2001-02-09.kitchen          0          0\r\n",
      "            0012.2003-12-19.GP          1          0\r\n",
      "        0013.1999-12-14.farmer          0          0\r\n",
      "      0013.1999-12-14.kaminski          0          0\r\n",
      "      0013.2001-04-03.williams          0          0\r\n",
      "     0013.2001-06-30.SA_and_HP          1          0\r\n",
      "            0013.2004-08-01.BG          1          1\r\n",
      "      0014.1999-12-14.kaminski          0          0\r\n",
      "        0014.1999-12-15.farmer          0          0\r\n",
      "       0014.2001-02-12.kitchen          0          0\r\n",
      "     0014.2001-07-04.SA_and_HP          1          0\r\n",
      "            0014.2003-12-19.GP          1          0\r\n",
      "            0014.2004-08-01.BG          1          0\r\n",
      "      0015.1999-12-14.kaminski          0          0\r\n",
      "        0015.1999-12-15.farmer          0          0\r\n",
      "         0015.2000-06-09.lokay          0          0\r\n",
      "       0015.2001-02-12.kitchen          0          0\r\n",
      "     0015.2001-07-05.SA_and_HP          1          0\r\n",
      "            0015.2003-12-19.GP          1          0\r\n",
      "        0016.1999-12-15.farmer          0          0\r\n",
      "       0016.2001-02-12.kitchen          0          0\r\n",
      "     0016.2001-07-05.SA_and_HP          1          0\r\n",
      "     0016.2001-07-06.SA_and_HP          1          0\r\n",
      "            0016.2003-12-19.GP          1          1\r\n",
      "            0016.2004-08-01.BG          1          0\r\n",
      "      0017.1999-12-14.kaminski          0          0\r\n",
      "          0017.2000-01-17.beck          0          0\r\n",
      "      0017.2001-04-03.williams          0          0\r\n",
      "            0017.2003-12-18.GP          1          0\r\n",
      "            0017.2004-08-01.BG          1          1\r\n",
      "            0017.2004-08-02.BG          1          0\r\n",
      "      0018.1999-12-14.kaminski          0          0\r\n",
      "     0018.2001-07-13.SA_and_HP          1          1\r\n",
      "            0018.2003-12-18.GP          1          1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.5.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = sys.argv[1]\n",
    "findwords = []\n",
    "countwords = []\n",
    "\n",
    "\n",
    "def empty_preprocessor(s):\n",
    "\n",
    "    elim_array = ['\\s,\\s', \n",
    "                 '\\s&\\s', \n",
    "                 '\\s\"\\s', \n",
    "                  '\\|\\s', \n",
    "                  '\\sTO\\s', \n",
    "                  ' \\sFOR\\s', \n",
    "                  '\\sTHAT\\s', \n",
    "                  '\\sWAS\\s', \n",
    "                 '\\sIS\\s', \n",
    "                 '\\sLIKE\\s'\n",
    "                  ]\n",
    "    for elim_word in  elim_array:    \n",
    "        s = re.sub(elim_word,' ', s, flags=re.IGNORECASE)\n",
    "    return s\n",
    "\n",
    "### Let's now change some and shorten some words\n",
    "def better_preprocessor(s):\n",
    "    s = re.sub('\\d', 'num_token ', s)\n",
    "    s = re.sub('\\s?\\s | \\s!\\s' , 'exp_token ', s )\n",
    "    s = re.sub('\\ly' , '', s )\n",
    "    return s  \n",
    "\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        record = empty_preprocessor(line)\n",
    "        record = better_preprocessor(line)\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email               \n",
    "                for word in bagofwords:                \n",
    "                    findwords.append(word)             ### Add each word to list\n",
    "    findwords = sorted(set(findwords))                 ### Eliminate repeats and sort\n",
    "\n",
    "with open (filename, \"r\") as myfile:    \n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            record_wordlist =[]\n",
    "            for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email               \n",
    "                for word in bagofwords:\n",
    "                    record_wordlist.append(word)\n",
    "            \n",
    "            for keyword in findwords:\n",
    "                count = 0\n",
    "                for word in record_wordlist:\n",
    "                    if keyword in word:\n",
    "                        count += 1\n",
    "                    countwords.append((word,count))\n",
    "                \n",
    "            print record[0], '\\t', record[1], '\\t'\n",
    "            for i in range(len(countwords)):\n",
    "                print countwords[i][0], '\\t', countwords[i][1],'\\t'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum_spam_records, sum_ham_records, sum_records = 0,0,0\n",
    "sum_spam_words, sum_ham_words, sum_words = 0,0,0\n",
    "wordcount_spam, wordcount_ham, wordcount = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                factors = re.split(r'\\t+', line)\n",
    "                for i in range(len(factors[2])):\n",
    "                    item = unicode(factors[2][i], 'utf-8')\n",
    "                    if item.isnumeric():\n",
    "                        word_tuple = {\n",
    "                            'word' : factors[3][i],\n",
    "                            'count': factors[2][i]\n",
    "                        }\n",
    "                        wordcount.append(word_tuple)\n",
    "                        if int(factors[1]) == 1:\n",
    "                            wordcount_spam.append(word_tuple)\n",
    "                        else: \n",
    "                            wordcount_ham.append(word_tuple)\n",
    "                         \n",
    "\n",
    "        print factors[0], factors [1], wordcount\n",
    "###   for i in range (len(wordcount_spam)):           \n",
    "##        sum_spam_words += int(wordcount_spam[i]) \n",
    "##    for i in range (len(wordcount_ham)):\n",
    "##        sum_ham_words += int(wordcount_ham[i])\n",
    "##    for i in range (len(wordcount)):\n",
    "##        sum_words += int(wordcount[i][0])\n",
    "                    \n",
    "        \n",
    "  \n",
    "    \n",
    "\n",
    "prior_spam = float(sum_spam_records)/float(sum_records)     ## prior prob of a spam email\n",
    "prior_ham = float(sum_ham_records)/float(sum_records)       ## prior prob of a ham email\n",
    "\n",
    "\n",
    "##check_prior = prior_spam + prior_ham                        ## check priors -> sum to 1\n",
    "##check_words = float(sum_words)/float(sum_spam_words+sum_ham_words) ## check probabilities of a word -> sum to 1\n",
    "##check_spam = prob_word_spam*float(sum_spam_words)/float(sum_spam_count) ## check spam counts -> sum to 1\n",
    "##check_ham = prob_word_ham*float(sum_ham_words)/float(sum_ham_count) ## check ham count -> sum to 1\n",
    "##print word_tuple\n",
    "\n",
    "print \"Summary of Data\"\n",
    "print '%4s'%sum_records ,'emails examined, containing %6s'%sum_words, 'using a dictionary of %6s' %size_dict\n",
    "\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS'\n",
    "for file in filenames:                                      \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                data = re.split(\" - \", line)\n",
    "                record_id = data[0]\n",
    "                y_true = data[1][0]\n",
    "                count_vector = data[2]\n",
    "                cond_prob_spam = []\n",
    "                cond_prob_ham = []\n",
    "                for i in range (len(wordcount)):\n",
    "                    prob_spam = float((count_vector[i]+1))/float(word_count_spam[i])**int(wordcount[i])\n",
    "                    prob_ham = float((count_vector[i]+1))/float(word_count_ham[i])**int(wordcount[i])\n",
    "                    p_spam += prior_spam*prob_spam\n",
    "                    p_ham += prior_ham*prob_ham\n",
    "                   \n",
    "                if p_spam > p_ham:\n",
    "                    y_pred = 1\n",
    "                else:\n",
    "                    y_pred = 0\n",
    "                    \n",
    "                print  '%30s' %record_id, '%10s' %y_true, '%10s' %y_pred\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"NB Classifier\" ## Print out words \n",
    "echo \"$numOfInstances\" ## Print out output data\n",
    "## clean up the data chunks and temporary count files\n",
    "##\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
