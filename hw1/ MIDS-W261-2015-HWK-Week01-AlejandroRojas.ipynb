{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b>Homework 1</b></h1>\n",
    "<i>Alejandro J. Rojas<br>\n",
    "ale@ischool.berkeley.edu<br>\n",
    "W261: Machine Learning at Scale<br>\n",
    "Week: 01<br>\n",
    "Jan 21, 2016</i></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.0.0.</h2> Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term big data is asoociated to datasets that cannot be processed, stored and transformed using traditional applications and tools because of high volume, high velocity and high variety. By high volume, we mean datasets that not only require high storage capacity, usually beyond 1T, but also datasets that are too big to have a decent processing and thoughput time. By high velocity, we mean data that requires real-time processing with throughput speeds that can be bursty. High variety includes data that comes from different formats, some structured some that are not, that all need to be ingested and transformed to be processed. Big data is simply changing the way we used to collect and analyze data at the time that it opens opportunity to increase the scale, scope and intimacy of the analyses that we are now able to do.\n",
    "\n",
    "The social web is a leading source of big data applications given our ability to log almost anything that the user does when interacting to an application. In my field, I've seen how online videos are increasingly the way users consume media. A video, per se, is an unstructured data item and its interactions are usually captured by leading social media platforms like Facebook, Twitter and Youtube in the form of JSON, a semi unstructured format that can capture user interactions such as likes, shares and comments. Across the internet, the amount of videos being upload and downstream is exploding making it a challenge to measure real-time, the media consuming habits of our target users. Big data can help in providing insights from all of this information so that we can better predict the taste of users visiting our site properties to serve them content they like.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.0.1.</h2>In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any dataset T that contains n independent variables (x1, x2, ..xn) and one dependent variable y_true, we can observe the following:\n",
    "\n",
    "If we try to estimate y as a function of x:\n",
    "    \n",
    "    y_pred = f(x)\n",
    "\n",
    "The estimate of our function will produce an error shown as:\n",
    "\n",
    "   <img src=\"error.png\">\n",
    "\n",
    "\n",
    "This error varies as we increase the complexity of our models as the following chart shows:\n",
    "\n",
    "<img src=\"Prediction-Error.png\">\n",
    "\n",
    "\n",
    "The source of this error can be divided into three types:\n",
    "    \n",
    "    bias \n",
    "    variance\n",
    "    irreducible error\n",
    "\n",
    "and can be derived mathematically the following way\n",
    "\n",
    "<img src=\"mathematicalerrors.jpg\">\n",
    "\n",
    "Bias error is introduced by us when we try to simplify the dynamics that we observe in the data, for instace by using a linear function to estimate y.\n",
    "\n",
    "As we try to better fit the underlying data, we can try implementing nonlinear functions.\n",
    "\n",
    "As the order of the polynomial regression increases, our function f(x) will more closely match the underlying portion of the dataset T and consequently we reduced our bias error.\n",
    "\n",
    "However, if we randomly applied our high-ordered polynomial f(x) to another portion of dataset T, we will find that our error will increase because we introduced variance error by overfitting the prior dataset.\n",
    "\n",
    "So as a rule of thumb, we can say that\n",
    "\n",
    "as the degree of the predictive polynomial function f(x) increases:\n",
    "\n",
    "    bias error is reduced\n",
    "    variance error is increased\n",
    "    \n",
    "the trick is to find the optimal point where the sum of these two errors are at the minimum.Even at that point, our function(x) will still show some error that will be irreducible because it comes from imprecisions in the way data was collected or other type of noise present in the dataset T.\n",
    "\n",
    "\n",
    "In this chart you can see how each of these errors varies as we bootstrap 50 samples of dataset T:\n",
    "\n",
    "<img src=\"bootstrapping.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HW1.1.</h2> Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <----------------------------------End of HW1.1------------------------------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.2.</h2>Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "records = 0\n",
    "words = 0\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)\n",
    "        records = records + 1\n",
    "        for i in range (len(record)):\n",
    "            bagofwords = re.split(\" \",record[i]) ### Break each email records into words\n",
    "            for word in bagofwords:\n",
    "                words = words + 1\n",
    "                for keyword in findwords:\n",
    "                    if keyword in word:\n",
    "                        count = count + 1       ### Add one the count of found words\n",
    "            \n",
    "\n",
    "##print '# of Records analized',records\n",
    "##print '# of Words analized', words\n",
    "##print '# of Ocurrences', count\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                sum = sum + int(line)             ### Add counts present on all mapper produced files\n",
    "                \n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"found [$numOfInstances] [$wordlist]\" ## Report  how many were found\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pNaiveBayes.sh m wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [10] [assistance]\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <----------------------------------End of HW1.2------------------------------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.3.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 9,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "########## Collect user input  ###############\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "\n",
    "    for line in myfile.readlines():\n",
    "        record = re.split(r'\\t+', line)                    ### Each email is a record with 4 components\n",
    "                                                           ### 1) ID 2) Spam Truth 3) Subject 4) Content\n",
    "        if len(record)==4:                                 ### Take only complete records\n",
    "\n",
    "            ########## Variables to collect and measure #########\n",
    "            records = 0                                    ### Each record corresponds to a unique email\n",
    "            words = 0                                      ### Words written in all emails incluidng Subject \n",
    "            spam_records, spam_words, spam_count = 0,0,0   ### Spam email count, words in spam email, user-specified word count\n",
    "            ham_records, ham_words, ham_count = 0, 0, 0    ### Same as above but for not spam emails\n",
    "\n",
    "\n",
    "            records += 1                                   ### add one the the total sum of emails\n",
    "            if int(record[1]) == 1:                        ### If the email is labeled as spam\n",
    "                spam_records += 1                          ### add one to the email spam count\n",
    "                for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                    bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email                \n",
    "                    for word in bagofwords:                ### For each word\n",
    "                        words += 1                         ### add one to the total sum of words\n",
    "                        spam_words += 1                    ### add one to the total sum of spam words  \n",
    "                        for keyword in findwords:          ### for each word specified by user\n",
    "                            if keyword in word:            ### If there's a match then\n",
    "                                spam_count += 1            ### add one to the user specified word count as spam\n",
    "                                \n",
    "            else:                                          ### If email is not labeled as spam\n",
    "                ham_records +=1                            ### add one to the email ham count\n",
    "                for i in range (2,len(record)):            ### Starting from Subject to the Content               \n",
    "                    bagofwords = re.split(\" \",record[i])   ### Collect all words present on each email                \n",
    "                    for word in bagofwords:                ### For each word\n",
    "                        words += 1                         ### add one to the total sum of words\n",
    "                        ham_words += 1                     ### add one to the total sum of ham words  \n",
    "                        for keyword in findwords:          ### for each word specified by user\n",
    "                            if keyword in word:            ### If there's a match then\n",
    "                                ham_count += 1             ### add one to the user specified word count as ham\n",
    "            \n",
    "            record_id = record[0]\n",
    "            truth = record[1]\n",
    "            print spam_count, \" \", spam_words, \" \",  spam_records, \" \",  \\\n",
    "                  ham_count, \" \", ham_words, \" \",  ham_records, \" \", \\\n",
    "                  words, \" \", records, \" \",  record_id, \" \", truth      "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 10,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 27,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Alejandro J. Rojas\n",
    "## Description: reducer code for HW1.3-1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum_spam_records, sum_spam_words, sum_spam_count = 0,0,0\n",
    "sum_ham_records, sum_ham_words, sum_ham_count = 0,0,0\n",
    "sum_records,sum_words = 0,0\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "for file in filenames:\n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                factors = re.split(\" \", line)\n",
    "                sum_spam_count += int(factors[0])           ## sum up every time the word was found in a spam\n",
    "                sum_spam_words += int(factors[3])           ## sum up all words from spams\n",
    "                sum_spam_records+= int(factors[6])          ## sum up all emails labeled as spam\n",
    "                sum_ham_count  += int(factors[9])           ## sum up every time the word was found in a ham\n",
    "                sum_ham_words += int(factors[12])           ## sum up all words from hams\n",
    "                sum_ham_records += int(factors[15])         ## sum up all emails labeled as ham\n",
    "                sum_words += int(factors[18])               ## sum all words from all emails\n",
    "                sum_records += int(factors[21])             ## sum all emails\n",
    "                \n",
    "\n",
    "prior_spam = float(sum_spam_records)/float(sum_records)     ## prior prob of a spam email\n",
    "prior_ham = float(sum_ham_records)/float(sum_records)       ## prior prob of a ham email\n",
    "prob_word_spam = float(sum_spam_count)/float(sum_spam_words)## prob of word given that email is spam\n",
    "prob_word_ham = float(sum_ham_count)/float(sum_ham_words)   ## prob of word given that email is ham\n",
    "\n",
    "##check_prior = prior_spam + prior_ham                        ## check priors -> sum to 1\n",
    "##check_words = float(sum_words)/float(sum_spam_words+sum_ham_words) ## check probabilities of a word -> sum to 1\n",
    "##check_spam = prob_word_spam*float(sum_spam_words)/float(sum_spam_count) ## check spam counts -> sum to 1\n",
    "##check_ham = prob_word_ham*float(sum_ham_words)/float(sum_ham_count) ## check ham count -> sum to 1\n",
    "sum_count = sum_spam_count+sum_ham_count\n",
    "\n",
    "print \"Summary of Data\"\n",
    "print '%4s'%sum_records ,'emails examined, containing %6s'%sum_words, 'words, we found %3s'%sum_count ,'matches.'\n",
    "\n",
    "print '%30s' %'ID', '%10s' %'TRUTH', '%10s' %'CLASS', '%20s' %'CUMULATIVE ACCURACY'\n",
    "miss, sample_size = 0,0 \n",
    "for file in filenames:                                      \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            if line.strip():\n",
    "                data = re.split(\" \", line)\n",
    "                record_id = data[24]\n",
    "                y_true = int(data[27][0])\n",
    "                count = int(data[0]) + int(data[9])\n",
    "                p_spam = prior_spam*prob_word_spam**count\n",
    "                p_ham = prior_ham*prob_word_ham**count\n",
    "                if p_spam > p_ham:\n",
    "                    y_pred = 1\n",
    "                else:\n",
    "                    y_pred = 0\n",
    "                \n",
    "                if y_pred != y_true:\n",
    "                    miss+= 1.0\n",
    "                sample_size += 1.0\n",
    "                accuracy = ((sample_size-miss)/sample_size)*100\n",
    "                \n",
    "                print  '%30s' %record_id, '%10s' %y_true, '%10s' %y_pred, '%18.2f %%' % accuracy\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 28,
>>>>>>> origin/master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 29,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "numOfInstances=$(cat $data.output)\n",
    "echo \"NB Classifier based on word(s): $wordlist\" ## Print out words \n",
    "echo \"$numOfInstances\" ## Print out output data\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run file"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 30,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier based on word(s): assistance\r\n",
      "Summary of Data\r\n",
      "  98 emails examined, containing  35352 words, we found   9 matches.\r\n",
      "                            ID      TRUTH      CLASS  CUMULATIVE ACCURACY\r\n",
      "        0001.1999-12-10.farmer          0          0             100.00 %\r\n",
      "      0001.1999-12-10.kaminski          0          0             100.00 %\r\n",
      "          0001.2000-01-17.beck          0          0             100.00 %\r\n",
      "       0001.2001-02-07.kitchen          0          0             100.00 %\r\n",
      "      0001.2001-04-02.williams          0          0             100.00 %\r\n",
      "        0002.1999-12-13.farmer          0          0             100.00 %\r\n",
      "       0002.2001-02-07.kitchen          0          0             100.00 %\r\n",
      "     0002.2001-05-25.SA_and_HP          1          0              87.50 %\r\n",
      "            0002.2003-12-18.GP          1          0              77.78 %\r\n",
      "            0002.2004-08-01.BG          1          1              80.00 %\r\n",
      "      0003.1999-12-10.kaminski          0          0              81.82 %\r\n",
      "        0003.1999-12-14.farmer          0          0              83.33 %\r\n",
      "          0003.2000-01-17.beck          0          0              84.62 %\r\n",
      "       0003.2001-02-08.kitchen          0          0              85.71 %\r\n",
      "            0003.2003-12-18.GP          1          0              80.00 %\r\n",
      "            0003.2004-08-01.BG          1          0              75.00 %\r\n",
      "      0004.1999-12-10.kaminski          0          1              70.59 %\r\n",
      "        0004.1999-12-14.farmer          0          0              72.22 %\r\n",
      "      0004.2001-04-02.williams          0          0              73.68 %\r\n",
      "     0004.2001-06-12.SA_and_HP          1          0              70.00 %\r\n",
      "            0004.2004-08-01.BG          1          0              66.67 %\r\n",
      "      0005.1999-12-12.kaminski          0          1              63.64 %\r\n",
      "        0005.1999-12-14.farmer          0          0              65.22 %\r\n",
      "         0005.2000-06-06.lokay          0          0              66.67 %\r\n",
      "       0005.2001-02-08.kitchen          0          0              68.00 %\r\n",
      "     0005.2001-06-23.SA_and_HP          1          0              65.38 %\r\n",
      "            0005.2003-12-18.GP          1          0              62.96 %\r\n",
      "      0006.1999-12-13.kaminski          0          0              64.29 %\r\n",
      "       0006.2001-02-08.kitchen          0          0              65.52 %\r\n",
      "      0006.2001-04-03.williams          0          0              66.67 %\r\n",
      "     0006.2001-06-25.SA_and_HP          1          0              64.52 %\r\n",
      "            0006.2003-12-18.GP          1          0              62.50 %\r\n",
      "            0006.2004-08-01.BG          1          0              60.61 %\r\n",
      "      0007.1999-12-13.kaminski          0          0              61.76 %\r\n",
      "        0007.1999-12-14.farmer          0          0              62.86 %\r\n",
      "          0007.2000-01-17.beck          0          0              63.89 %\r\n",
      "       0007.2001-02-09.kitchen          0          0              64.86 %\r\n",
      "            0007.2003-12-18.GP          1          0              63.16 %\r\n",
      "            0007.2004-08-01.BG          1          0              61.54 %\r\n",
      "       0008.2001-02-09.kitchen          0          0              62.50 %\r\n",
      "     0008.2001-06-12.SA_and_HP          1          0              60.98 %\r\n",
      "     0008.2001-06-25.SA_and_HP          1          0              59.52 %\r\n",
      "            0008.2003-12-18.GP          1          0              58.14 %\r\n",
      "            0008.2004-08-01.BG          1          0              56.82 %\r\n",
      "      0009.1999-12-13.kaminski          0          0              57.78 %\r\n",
      "        0009.1999-12-14.farmer          0          0              58.70 %\r\n",
      "         0009.2000-06-07.lokay          0          0              59.57 %\r\n",
      "       0009.2001-02-09.kitchen          0          0              60.42 %\r\n",
      "            0009.2003-12-18.GP          1          0              59.18 %\r\n",
      "        0010.1999-12-14.farmer          0          0              60.00 %\r\n",
      "      0010.1999-12-14.kaminski          0          0              60.78 %\r\n",
      "       0010.2001-02-09.kitchen          0          0              61.54 %\r\n",
      "     0010.2001-06-28.SA_and_HP          1          1              62.26 %\r\n",
      "            0010.2003-12-18.GP          1          0              61.11 %\r\n",
      "            0010.2004-08-01.BG          1          0              60.00 %\r\n",
      "        0011.1999-12-14.farmer          0          0              60.71 %\r\n",
      "     0011.2001-06-28.SA_and_HP          1          0              59.65 %\r\n",
      "     0011.2001-06-29.SA_and_HP          1          0              58.62 %\r\n",
      "            0011.2003-12-18.GP          1          0              57.63 %\r\n",
      "            0011.2004-08-01.BG          1          0              56.67 %\r\n",
      "        0012.1999-12-14.farmer          0          0              57.38 %\r\n",
      "      0012.1999-12-14.kaminski          0          0              58.06 %\r\n",
      "          0012.2000-01-17.beck          0          0              58.73 %\r\n",
      "         0012.2000-06-08.lokay          0          0              59.38 %\r\n",
      "       0012.2001-02-09.kitchen          0          0              60.00 %\r\n",
      "            0012.2003-12-19.GP          1          0              59.09 %\r\n",
      "        0013.1999-12-14.farmer          0          0              59.70 %\r\n",
      "      0013.1999-12-14.kaminski          0          0              60.29 %\r\n",
      "      0013.2001-04-03.williams          0          0              60.87 %\r\n",
      "     0013.2001-06-30.SA_and_HP          1          0              60.00 %\r\n",
      "            0013.2004-08-01.BG          1          1              60.56 %\r\n",
      "      0014.1999-12-14.kaminski          0          0              61.11 %\r\n",
      "        0014.1999-12-15.farmer          0          0              61.64 %\r\n",
      "       0014.2001-02-12.kitchen          0          0              62.16 %\r\n",
      "     0014.2001-07-04.SA_and_HP          1          0              61.33 %\r\n",
      "            0014.2003-12-19.GP          1          0              60.53 %\r\n",
      "            0014.2004-08-01.BG          1          0              59.74 %\r\n",
      "      0015.1999-12-14.kaminski          0          0              60.26 %\r\n",
      "        0015.1999-12-15.farmer          0          0              60.76 %\r\n",
      "         0015.2000-06-09.lokay          0          0              61.25 %\r\n",
      "       0015.2001-02-12.kitchen          0          0              61.73 %\r\n",
      "     0015.2001-07-05.SA_and_HP          1          0              60.98 %\r\n",
      "            0015.2003-12-19.GP          1          0              60.24 %\r\n",
      "        0016.1999-12-15.farmer          0          0              60.71 %\r\n",
      "       0016.2001-02-12.kitchen          0          0              61.18 %\r\n",
      "     0016.2001-07-05.SA_and_HP          1          0              60.47 %\r\n",
      "     0016.2001-07-06.SA_and_HP          1          0              59.77 %\r\n",
      "            0016.2003-12-19.GP          1          0              59.09 %\r\n",
      "            0016.2004-08-01.BG          1          0              58.43 %\r\n",
      "      0017.1999-12-14.kaminski          0          0              58.89 %\r\n",
      "          0017.2000-01-17.beck          0          0              59.34 %\r\n",
      "      0017.2001-04-03.williams          0          0              59.78 %\r\n",
      "            0017.2003-12-18.GP          1          0              59.14 %\r\n",
      "            0017.2004-08-01.BG          1          0              58.51 %\r\n",
      "            0017.2004-08-02.BG          1          0              57.89 %\r\n",
      "      0018.1999-12-14.kaminski          0          0              58.33 %\r\n",
      "     0018.2001-07-13.SA_and_HP          1          1              58.76 %\r\n",
      "            0018.2003-12-18.GP          1          1              59.18 %\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <----------------------------------End of HW1.3------------------------------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW1.4.</h2> Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier based on word(s): assistance valium enlargementWithATypo\r\n",
      "Summary of Data\r\n",
      "  98 emails examined, containing  35352 words, we found  12 matches.\r\n",
      "                            ID      TRUTH      CLASS  CUMULATIVE ACCURACY\r\n",
      "        0001.1999-12-10.farmer          0          0             100.00 %\r\n",
      "      0001.1999-12-10.kaminski          0          0             100.00 %\r\n",
      "          0001.2000-01-17.beck          0          0             100.00 %\r\n",
      "       0001.2001-02-07.kitchen          0          0             100.00 %\r\n",
      "      0001.2001-04-02.williams          0          0             100.00 %\r\n",
      "        0002.1999-12-13.farmer          0          0             100.00 %\r\n",
      "       0002.2001-02-07.kitchen          0          0             100.00 %\r\n",
      "     0002.2001-05-25.SA_and_HP          1          0              87.50 %\r\n",
      "            0002.2003-12-18.GP          1          0              77.78 %\r\n",
      "            0002.2004-08-01.BG          1          1              80.00 %\r\n",
      "      0003.1999-12-10.kaminski          0          0              81.82 %\r\n",
      "        0003.1999-12-14.farmer          0          0              83.33 %\r\n",
      "          0003.2000-01-17.beck          0          0              84.62 %\r\n",
      "       0003.2001-02-08.kitchen          0          0              85.71 %\r\n",
      "            0003.2003-12-18.GP          1          0              80.00 %\r\n",
      "            0003.2004-08-01.BG          1          0              75.00 %\r\n",
      "      0004.1999-12-10.kaminski          0          1              70.59 %\r\n",
      "        0004.1999-12-14.farmer          0          0              72.22 %\r\n",
      "      0004.2001-04-02.williams          0          0              73.68 %\r\n",
      "     0004.2001-06-12.SA_and_HP          1          0              70.00 %\r\n",
      "            0004.2004-08-01.BG          1          0              66.67 %\r\n",
      "      0005.1999-12-12.kaminski          0          1              63.64 %\r\n",
      "        0005.1999-12-14.farmer          0          0              65.22 %\r\n",
      "         0005.2000-06-06.lokay          0          0              66.67 %\r\n",
      "       0005.2001-02-08.kitchen          0          0              68.00 %\r\n",
      "     0005.2001-06-23.SA_and_HP          1          0              65.38 %\r\n",
      "            0005.2003-12-18.GP          1          0              62.96 %\r\n",
      "      0006.1999-12-13.kaminski          0          0              64.29 %\r\n",
      "       0006.2001-02-08.kitchen          0          0              65.52 %\r\n",
      "      0006.2001-04-03.williams          0          0              66.67 %\r\n",
      "     0006.2001-06-25.SA_and_HP          1          0              64.52 %\r\n",
      "            0006.2003-12-18.GP          1          0              62.50 %\r\n",
      "            0006.2004-08-01.BG          1          0              60.61 %\r\n",
      "      0007.1999-12-13.kaminski          0          0              61.76 %\r\n",
      "        0007.1999-12-14.farmer          0          0              62.86 %\r\n",
      "          0007.2000-01-17.beck          0          0              63.89 %\r\n",
      "       0007.2001-02-09.kitchen          0          0              64.86 %\r\n",
      "            0007.2003-12-18.GP          1          0              63.16 %\r\n",
      "            0007.2004-08-01.BG          1          0              61.54 %\r\n",
      "       0008.2001-02-09.kitchen          0          0              62.50 %\r\n",
      "     0008.2001-06-12.SA_and_HP          1          0              60.98 %\r\n",
      "     0008.2001-06-25.SA_and_HP          1          0              59.52 %\r\n",
      "            0008.2003-12-18.GP          1          0              58.14 %\r\n",
      "            0008.2004-08-01.BG          1          0              56.82 %\r\n",
      "      0009.1999-12-13.kaminski          0          0              57.78 %\r\n",
      "        0009.1999-12-14.farmer          0          0              58.70 %\r\n",
      "         0009.2000-06-07.lokay          0          0              59.57 %\r\n",
      "       0009.2001-02-09.kitchen          0          0              60.42 %\r\n",
      "            0009.2003-12-18.GP          1          1              61.22 %\r\n",
      "        0010.1999-12-14.farmer          0          0              62.00 %\r\n",
      "      0010.1999-12-14.kaminski          0          0              62.75 %\r\n",
      "       0010.2001-02-09.kitchen          0          0              63.46 %\r\n",
      "     0010.2001-06-28.SA_and_HP          1          1              64.15 %\r\n",
      "            0010.2003-12-18.GP          1          0              62.96 %\r\n",
      "            0010.2004-08-01.BG          1          0              61.82 %\r\n",
      "        0011.1999-12-14.farmer          0          0              62.50 %\r\n",
      "     0011.2001-06-28.SA_and_HP          1          0              61.40 %\r\n",
      "     0011.2001-06-29.SA_and_HP          1          0              60.34 %\r\n",
      "            0011.2003-12-18.GP          1          0              59.32 %\r\n",
      "            0011.2004-08-01.BG          1          0              58.33 %\r\n",
      "        0012.1999-12-14.farmer          0          0              59.02 %\r\n",
      "      0012.1999-12-14.kaminski          0          0              59.68 %\r\n",
      "          0012.2000-01-17.beck          0          0              60.32 %\r\n",
      "         0012.2000-06-08.lokay          0          0              60.94 %\r\n",
      "       0012.2001-02-09.kitchen          0          0              61.54 %\r\n",
      "            0012.2003-12-19.GP          1          0              60.61 %\r\n",
      "        0013.1999-12-14.farmer          0          0              61.19 %\r\n",
      "      0013.1999-12-14.kaminski          0          0              61.76 %\r\n",
      "      0013.2001-04-03.williams          0          0              62.32 %\r\n",
      "     0013.2001-06-30.SA_and_HP          1          0              61.43 %\r\n",
      "            0013.2004-08-01.BG          1          1              61.97 %\r\n",
      "      0014.1999-12-14.kaminski          0          0              62.50 %\r\n",
      "        0014.1999-12-15.farmer          0          0              63.01 %\r\n",
      "       0014.2001-02-12.kitchen          0          0              63.51 %\r\n",
      "     0014.2001-07-04.SA_and_HP          1          0              62.67 %\r\n",
      "            0014.2003-12-19.GP          1          0              61.84 %\r\n",
      "            0014.2004-08-01.BG          1          0              61.04 %\r\n",
      "      0015.1999-12-14.kaminski          0          0              61.54 %\r\n",
      "        0015.1999-12-15.farmer          0          0              62.03 %\r\n",
      "         0015.2000-06-09.lokay          0          0              62.50 %\r\n",
      "       0015.2001-02-12.kitchen          0          0              62.96 %\r\n",
      "     0015.2001-07-05.SA_and_HP          1          0              62.20 %\r\n",
      "            0015.2003-12-19.GP          1          0              61.45 %\r\n",
      "        0016.1999-12-15.farmer          0          0              61.90 %\r\n",
      "       0016.2001-02-12.kitchen          0          0              62.35 %\r\n",
      "     0016.2001-07-05.SA_and_HP          1          0              61.63 %\r\n",
      "     0016.2001-07-06.SA_and_HP          1          0              60.92 %\r\n",
      "            0016.2003-12-19.GP          1          1              61.36 %\r\n",
      "            0016.2004-08-01.BG          1          0              60.67 %\r\n",
      "      0017.1999-12-14.kaminski          0          0              61.11 %\r\n",
      "          0017.2000-01-17.beck          0          0              61.54 %\r\n",
      "      0017.2001-04-03.williams          0          0              61.96 %\r\n",
      "            0017.2003-12-18.GP          1          0              61.29 %\r\n",
      "            0017.2004-08-01.BG          1          1              61.70 %\r\n",
      "            0017.2004-08-02.BG          1          0              61.05 %\r\n",
      "      0018.1999-12-14.kaminski          0          0              61.46 %\r\n",
      "     0018.2001-07-13.SA_and_HP          1          1              61.86 %\r\n",
      "            0018.2003-12-18.GP          1          1              62.24 %\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <----------------------------------End of HW1.4------------------------------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <----------------------------------End of HW1------------------------------------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
