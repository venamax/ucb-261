{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4, DATSCI W261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: \n",
    "Emails:  \n",
    "Time of Initial Submission: 9:21 PM EST, Monday, January 18, 2016  \n",
    "Time of **Resubmission**: 8:38 AM EST, Friday, January 22, 2016 \n",
    "W261-1, Spring 2016  \n",
    "Week 4 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #4=====\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #4\n",
    "Version 2016-01-27 (FINAL)\n",
    "\n",
    "SPECIAL INSTURCTIONS\n",
    " HW3 is a group exercise. Your team assignments for completing HW3 (and future homeworks) are located at:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1ncFQl5Tovn-16slD8mYjP_nzMTPSfiGeLLzW8v_sMjg/edit?usp=sharing\n",
    "\n",
    "See column Team assignment for Homeworks in tab \"Teams for HW Assignments\"\n",
    "\n",
    "Note: please submit one homework submission per group.\n",
    "\n",
    "---------------\n",
    "\n",
    "Please follow the instructions for submissions carefully.\n",
    "Note that all referenced files life in the enclosing directory.\n",
    "\n",
    "=== Week 4 ASSIGNMENTS ===\n",
    "\n",
    "HW 4.0. \n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?\n",
    "\n",
    "HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop? \n",
    "When it used in these frameworks? \n",
    "What is the default serialization mode for input and outputs for MrJob? \n",
    "\n",
    "HW 4.2: Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "V\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "V,1001,1,C, 10001\n",
    "V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this.\n",
    "\n",
    "HW 4.3: Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).\n",
    "\n",
    "HW 4.4: Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.\n",
    "\n",
    "HW 4.5 Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "\n",
    "HW4.6  (OPTIONAL) Scaleable K-MEANS++ \n",
    "\n",
    "Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf \n",
    "\n",
    "In MrJob, implement K-MEANS|| and compare with a random initializtion for the dataset above. \n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms.\n",
    "\n",
    "\n",
    "\n",
    "=====================\n",
    "END OF HOMEWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW 4.0. </h2>\n",
    "What is MrJob? How is it different to Hadoop MapReduce? What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MrJob is a python object-oriented framework that let's you define classes that integrates Hadoop MapReduce in a very elegant and clean structure so you can write and run MapReduce jobs. It contains methods that leverage Hadoop streaming to accomplish MapReduce taks. An MrJob class lets you define steps that you want your routine to follow. This python program comes with a variery of methods that make it simple to implement MapReduce, among them:\n",
    "mappint_init() -> to initialize mapper values\n",
    "mapper_final() -> returns key,value tuple where value is 1\n",
    "combiner_final() -> returns key,value tuple where value is local sum\n",
    "reducer_final() -> returns key,value tuple where value is total sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW 4.1</h2>\n",
    "What is serialization in the context of MrJob or Hadoop? When it used in these frameworks? What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MrJob does not take any input in binary. It accepts either text or json which makes it slower that running Hadoop streaming directly. So after receiving its input it converts it into ninary to do all processing. This is inefficient in terms of processing time but teh framework gives the flexibility to quickly write and run easily readable code which makes it an ideal solution to write and run MapReduce jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HW 4.2</h2>\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001 #Visitor id 10001 \n",
    "V,1000,1 #Visit by Visitor 10001 to page id 1000 \n",
    "V,1001,1 #Visit by Visitor 10001 to page id 1001 \n",
    "V,1002,1 #Visit by Visitor 10001 to page id 1002 \n",
    "C,\"10002\",10002 #Visitor id 10001 V Note: #denotes \n",
    "\n",
    "comments to the format:\n",
    "V,1000,1,C, 10001 V,1001,1,C, 10001 V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class MS_preprocess():\n",
    "    output_file = open(\"msweb-output.txt\", \"w\") # \n",
    "    input_file = 'anonymous-msweb.data'\n",
    "    with open (input_file, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            record = csv_readline(line)\n",
    "            if record[0] == 'C':\n",
    "                customer_record = record[:2]\n",
    "            elif record[0] == 'V':\n",
    "                new_record = record + customer_record\n",
    "                output_file.write(str(new_record))   \n",
    "                output_file.write('\\n')\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MS_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
