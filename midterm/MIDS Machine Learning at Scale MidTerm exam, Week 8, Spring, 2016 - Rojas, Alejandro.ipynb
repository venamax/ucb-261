{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Insructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIDS Machine Learning at Scale MidTerm exam, Week 8, Spring, 2016\n",
    "\n",
    "Exam location is at:\n",
    "\n",
    "     https://www.dropbox.com/s/jdkkttnwd88uxkl/MIDS-MLS-MidTerm-2016-Spring-Live.txt?dl=0\n",
    "===Instructions for midterm===\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1: Please acknowledge receipt of exam by sending a quick reply to the instructors\n",
    "2: Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form)\n",
    "3: Please keep all your work and responses in ONE (1) notebook only (and submit via the form)\n",
    "4: Please make sure that the NBViewer link for your Submission notebook works\n",
    "5: Please do NOT discuss this exam with anyone (including your class mates) until after 8AM (West coast time) Friday, March 4, 2016 \n",
    "\n",
    "Please use your live session time from week 8 to complete this mid term \n",
    "(plus an additional 30 minutes if you need it). \n",
    "This is an open book exam meaning you can consult webpages and textbooks \n",
    "(but not each other or other people). Please complete this exam by yourself.\n",
    "\n",
    "Please submit your solutions and notebook via the following form:\n",
    "\n",
    "      http://goo.gl/forms/ggNYfRXz0t\n",
    "\n",
    "===Exam durations (All times are in California Time)===\n",
    "\n",
    "Live Session Group #4 4:00 PM - 6:00 PM (Tuesday)\n",
    "Live Session Group #2 4:00 PM - 6:00 PM (Wednesday)\n",
    "Live Session Group #3 6:30 PM - 8:30 PM (Wednesday)\n",
    "\n",
    "\n",
    "=====Exam questions begins here=====\n",
    "\n",
    "\n",
    "===Map-Reduce===\n",
    "\n",
    "MT0. Which of the following statememts about map-reduce are true? Check all that apply.\n",
    "\n",
    "(a) If you only have 1 computer with 1 computing core, then map-reduce is unlikely to help\n",
    "(b) If we run map-reduce using N computers, then we will always get at least an N-Fold speedup compared to using 1 computer\n",
    "(c) Because of network latency and other overhead associated with map-reduce, if we run map-reduce using N computers, then we will get less than N-Fold speedup compared to using 1 computer\n",
    "(d) When using map-reduce with gradient descent, we usually use a single machine that accumulates the gradients from each of the map-reduce machines, in order to compute the paramter update for the iterion\n",
    "\n",
    "\n",
    "===Order inversion===\n",
    "\n",
    "MT1. Suppose you wish to write a MapReduce job that creates \n",
    "normalized word co-occurrence data form a large input text. \n",
    "To ensure that all (potentially many) reducers\n",
    "receive appropriate normalization factors (denominators)\n",
    "in the correct order in their input streams\n",
    "(so as to minimize memory overhead), \n",
    "the mapper should emit according to which pattern:\n",
    "\n",
    "(a) emit (*,word) count\n",
    "(b) There is no need to use  order inversion here\n",
    "(c) emit (word,*) count\n",
    "(d) None of the above\n",
    "\n",
    "===Apriori principle===\n",
    "\n",
    "MT2. When searching for frequent itemsets with the Apriori algorithm\n",
    "(using a threshold, N), the Apriori principle allows us to avoid \n",
    "tracking the occurrences of the itemset {A,B,C} provided\n",
    "\n",
    "(a) all subsets of {A,B,C} occur less than N times.\n",
    "(b) any pair of {A,B,C} occurs less than N times.\n",
    "(c) any subset of {A,B,C} occurs less than N times.\n",
    "(d) All of the above\n",
    "\n",
    "===Bayesian document classification===\n",
    "\n",
    "MT3. When building a Bayesian document classifier, Laplace smoothing serves what purpose?\n",
    "\n",
    "(a) It allows you to use your training data as your validation data.\n",
    "(b) It prevents zero-products in the posterior distribution.\n",
    "(c) It accounts for words that were missed by regular expressions. \n",
    "(d) None of the above\n",
    "\n",
    "===Bias-variance tradeoff===\n",
    "\n",
    "MT4. By increasing the complexity of a model regressed on some samples of data,\n",
    "it is likely that the ensemble will exhibit which of the following?\n",
    "\n",
    "(a) Increased variance and bias\n",
    "(b) Increased variance and decreased bias\n",
    "(c) Decreased variance and bias\n",
    "(d) Decreased variance and increased bias\n",
    "\n",
    "===Combiners===\n",
    "\n",
    "MT5. Combiners can be integral to the successful utilization of the Hadoop shuffle.\n",
    "This utility is as a result of\n",
    "\n",
    "(a) minimization of reducer workload\n",
    "(b) both (a) and (c)\n",
    "(c) minimization of network traffic\n",
    "(d) none of the above\n",
    "\n",
    "===Pairwise similarity using K-L divergence===\n",
    "\n",
    "In probability theory and information theory, the Kullback–Leibler divergence \n",
    "(also information divergence, information gain, relative entropy, KLIC, or KL divergence) \n",
    "is a non-symmetric measure of the difference between two probability distributions P and Q. \n",
    "Specifically, the Kullback–Leibler divergence of Q from P, denoted DKL(P‖Q), \n",
    "is a measure of the information lost when Q is used to approximate P:\n",
    "\n",
    "For discrete probability distributions P and Q, \n",
    "the Kullback–Leibler divergence of Q from P is defined to be\n",
    "\n",
    "KLDistance(P, Q) = Sum over i (P(i) log (P(i) / Q(i))      \n",
    "\n",
    "In the extreme cases, the KL Divergence is 1 when P and Q are maximally different\n",
    "and is 0 when the two distributions are exactly the same (follow the same distribution).\n",
    "\n",
    "For more information on K-L Divergence see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "\n",
    "For the next three question we will use an MRjob class for calculating pairwise similarity \n",
    "using K-L Divergence as the similarity measure:\n",
    "\n",
    "Job 1: create inverted index (assume just two objects)\n",
    "Job 2: calculate/accumulate the similarity of each pair of objects using K-L Divergence\n",
    "\n",
    "Download the following notebook and then fill in the code for the first reducer to calculate \n",
    "the K-L divergence of objects (letter documents) in line1 and line2, i.e., KLD(Line1||line2).\n",
    "\n",
    "Here we ignore characters which are not alphabetical. And all alphabetical characters are lower-cased in the first mapper.\n",
    "\n",
    "http://nbviewer.ipython.org/urls/dl.dropbox.com/s/9onx4c2dujtkgd7/Kullback%E2%80%93Leibler%20divergence-MIDS-Midterm.ipynb\n",
    "https://www.dropbox.com/s/zr9xfhwakrxz9hc/Kullback%E2%80%93Leibler%20divergence-MIDS-Midterm.ipynb?dl=0\n",
    "\n",
    "Questions:\n",
    "\n",
    "MT6. Which number below is the closest to the result you get for KLD(Line1||line2)?\n",
    "(a) 0.7\n",
    "(b) 0.5\n",
    "(c) 0.2\n",
    "(d) 0.1\n",
    "\n",
    "MT7. Which of the following letters are missing from these character vectors?\n",
    "(a) p and t\n",
    "(b) k and q\n",
    "(c) j and q\n",
    "(d) j and f\n",
    "\n",
    "MT8. The KL divergence on multinomials is defined only when they have nonzero entries. \n",
    "For zero entries, we have to smooth distributions. Suppose we smooth in this way: \n",
    "\n",
    "(ni+1)/(n+24)\n",
    "\n",
    "where ni is the count for letter i and n is the total count of all letters. \n",
    "After smoothing, which number below is the closest to the result you get for KLD(Line1||line2)??\n",
    "\n",
    "(a) 0.08\n",
    "(b) 0.71\n",
    "(c) 0.02\n",
    "(d) 0.11\n",
    "\n",
    "===Gradient descent===\n",
    "\n",
    "MT9. Which of the following are true statements with respect to gradient descent for machine learning, where alpha is the learning rate. Select all that apply\n",
    "\n",
    "(a) To make gradient descent converge, we must slowly decrease alpha over time and use a combiner in the context of Hadoop.\n",
    "(b) Gradient descent is guaranteed to find the global minimum for any function J() regardless of using a combiner or not in the context of Hadoop\n",
    "(c) Gradient descent can converge even if alpha is kept fixed. (But alpha cannot be too large, or else it may fail to converge.) Combiners will help speed up the process.\n",
    "(d) For the specific choice of cost function J() used in linear regression, there is no local optima (other than the global optimum).\n",
    "\n",
    "===Weighted K-means===\n",
    "\n",
    "Write a MapReduce job in MRJob to do the training at scale of a weighted K-means algorithm.\n",
    "\n",
    "You can write your own code or you can use most of the code from the following notebook:\n",
    "\n",
    "http://nbviewer.ipython.org/urls/dl.dropbox.com/s/kjtdyi10nwmk4ko/MrJobKmeans-MIDS-Midterm.ipynb\n",
    "https://www.dropbox.com/s/kjtdyi10nwmk4ko/MrJobKmeans-MIDS-Midterm.ipynb?dl=0\n",
    "\n",
    "Weight each example as follows using the inverse vector length (Euclidean norm): \n",
    "\n",
    "weight(X)= 1/||X||, \n",
    "\n",
    "where ||X|| = SQRT(X.X)= SQRT(X1^2 + X2^2)\n",
    "\n",
    "Here X is vector made up of X1 and X2.\n",
    "\n",
    "Using the following data answer the following questions:\n",
    "\n",
    "https://www.dropbox.com/s/ai1uc3q2ucverly/Kmeandata.csv?dl=0\n",
    "\n",
    "Questions:\n",
    "\n",
    "MT10. Which result below is the closest to the centroids you got after running your weighted K-means code for 10 iterations?\n",
    "\n",
    "(a) (-4.0,0.0), (4.0,0.0), (6.0,6.0)\n",
    "(b) (-4.5,0.0), (4.5,0.0), (0.0,4.5)\n",
    "(c) (-5.5,0.0), (0.0,0.0), (3.0,3.0)\n",
    "(d) (-4.5,0.0), (-4.0,0.0), (0.0,4.5)\n",
    "\n",
    "MT11. Using the result of the previous question, which number below is the closest \n",
    "to the average weighted distance between each example and its assigned (closest) centroid?\n",
    "The average weighted distance is defined as \n",
    "sum over i  (weighted_distance_i)     /  sum over i (weight_i)\n",
    "\n",
    "(a) 2.5\n",
    "(b) 1.5\n",
    "(c) 0.5\n",
    "(d) 4.0\n",
    "\n",
    "MT12. Which of the following statements are true? Select all that apply. \n",
    "a)\t  Since K-Means is an unsupervised learning algorithm, it cannot overfit the data, and thus it is always better to have as large a number of clusters as is computationally feasible. \n",
    "b)\t  The standard way of initializing K-means is setting μ1=⋯=μk to be equal to a vector of zeros. \n",
    "c)\t  For some datasets, the \"right\" or \"correct\" value of K (the number of clusters) can be ambiguous, and hard even for a human expert looking carefully at the data to decide. \n",
    "d)\t  A good way to initialize K-means is to select K (distinct) examples from the training set and set the cluster centroids equal to these selected examples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Map-Reduce===\n",
    "MT0. Which of the following statememts about map-reduce are true? Check all that apply.\n",
    "(a) If you only have 1 computer with 1 computing core, then map-reduce is unlikely to help (b) If we run map-reduce using N computers, then we will always get at least an N-Fold speedup compared to using 1 computer (c) Because of network latency and other overhead associated with map-reduce, if we run map-reduce using N computers, then we will get less than N-Fold speedup compared to using 1 computer (d) When using map-reduce with gradient descent, we usually use a single machine that accumulates the gradients from each of the map-reduce machines, in order to compute the paramter update for the iterion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: a, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Order inversion===\n",
    "MT1. Suppose you wish to write a MapReduce job that creates normalized word co-occurrence data form a large input text. To ensure that all (potentially many) reducers receive appropriate normalization factors (denominators) in the correct order in their input streams (so as to minimize memory overhead), the mapper should emit according to which pattern:\n",
    "(a) emit (,word) count (b) There is no need to use order inversion here (c) emit (word,) count (d) None of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Apriori principle===\n",
    "MT2. When searching for frequent itemsets with the Apriori algorithm (using a threshold, N), the Apriori principle allows us to avoid tracking the occurrences of the itemset {A,B,C} provided\n",
    "(a) all subsets of {A,B,C} occur less than N times. (b) any pair of {A,B,C} occurs less than N times. (c) any subset of {A,B,C} occurs less than N times. (d) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers : d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Bayesian document classification===\n",
    "MT3. When building a Bayesian document classifier, Laplace smoothing serves what purpose?\n",
    "(a) It allows you to use your training data as your validation data. (b) It prevents zero-products in the posterior distribution. (c) It accounts for words that were missed by regular expressions. (d) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: b , c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Bias-variance tradeoff===\n",
    "MT4. By increasing the complexity of a model regressed on some samples of data, it is likely that the ensemble will exhibit which of the following?\n",
    "(a) Increased variance and bias (b) Increased variance and decreased bias (c) Decreased variance and bias (d) Decreased variance and increased bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Combiners===\n",
    "MT5. Combiners can be integral to the successful utilization of the Hadoop shuffle. This utility is as a result of\n",
    "(a) minimization of reducer workload (b) both (a) and (c) (c) minimization of network traffic (d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Pairwise similarity using K-L divergence===\n",
    "In probability theory and information theory, the Kullback–Leibler divergence (also information divergence, information gain, relative entropy, KLIC, or KL divergence) is a non-symmetric measure of the difference between two probability distributions P and Q. Specifically, the Kullback–Leibler divergence of Q from P, denoted DKL(P‖Q), is a measure of the information lost when Q is used to approximate P:\n",
    "For discrete probability distributions P and Q, the Kullback–Leibler divergence of Q from P is defined to be\n",
    "KLDistance(P, Q) = Sum over i (P(i) log (P(i) / Q(i))\n",
    "In the extreme cases, the KL Divergence is 1 when P and Q are maximally different and is 0 when the two distributions are exactly the same (follow the same distribution).\n",
    "For more information on K-L Divergence see:\n",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "For the next three question we will use an MRjob class for calculating pairwise similarity using K-L Divergence as the similarity measure:\n",
    "Job 1: create inverted index (assume just two objects) Job 2: calculate/accumulate the similarity of each pair of objects using K-L Divergence\n",
    "Download the following notebook and then fill in the code for the first reducer to calculate the K-L divergence of objects (letter documents) in line1 and line2, i.e., KLD(Line1||line2).\n",
    "Here we ignore characters which are not alphabetical. And all alphabetical characters are lower-cased in the first mapper.\n",
    "http://nbviewer.ipython.org/urls/dl.dropbox.com/s/9onx4c2dujtkgd7/Kullback%E2%80%93Leibler%20divergence-MIDS-Midterm.ipynb https://www.dropbox.com/s/zr9xfhwakrxz9hc/Kullback%E2%80%93Leibler%20divergence-MIDS-Midterm.ipynb?dl=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using the MRJob Class below  calculate the  KL divergence of the following two objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MRjob class for calculating pairwise similarity using K-L Divergence as the similarity measure\n",
    "\n",
    "Job 1: create inverted index (assume just two objects) <P>\n",
    "Job 2: calculate the similarity of each pair of objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class kldivergence(MRJob):\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        doc_list ={}\n",
    "        for doc,prob in values:\n",
    "            doc_list[doc]=prob\n",
    "        p = doc_list[1] ## Probability for key in document 1\n",
    "        q = doc_list[2] ## Probability for key in document 2\n",
    "        kl = p * math.log(p/q) ## KL Divergence\n",
    "        yield key, kl\n",
    "        \n",
    "    \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield None, kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                self.mr(reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n",
      "WARNING:mrjob.job:mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-36dcacf1e251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmr_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkldivergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kltext.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# stream_output: get access of the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job already ran!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/mrjob/sim.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# run the reducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reducer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# move final output to output directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/mrjob/sim.pyc\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(self, step_num, step_type)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             self._run_step(step_num, step_type, input_path, output_path,\n\u001b[0;32m--> 260\u001b[0;31m                            working_dir, env)\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_outfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/mrjob/inline.pyc\u001b[0m in \u001b[0;36m_run_step\u001b[0;34m(self, step_num, step_type, input_path, output_path, working_dir, env, child_stdin)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mchild_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mrjob_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mchild_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_stdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchild_stdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mchild_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_combiner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_reducer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_reducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mrun_reducer\u001b[0;34m(self, step_num)\u001b[0m\n\u001b[1;32m    578\u001b[0m                                                key=lambda(k, v): k):\n\u001b[1;32m    579\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkv_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mout_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m                 \u001b[0mwrite_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Lissette/Dropbox/Machine Learning at Scale/Solutions/kldivergence.py\u001b[0m in \u001b[0;36mreducer1\u001b[0;34m(self, key, values)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreducer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdoc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "from kldivergence import kldivergence\n",
    "mr_job = kldivergence(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Gradient descent===\n",
    "MT9. Which of the following are true statements with respect to gradient descent for machine learning, where alpha is the learning rate. Select all that apply\n",
    "(a) To make gradient descent converge, we must slowly decrease alpha over time and use a combiner in the context of Hadoop. (b) Gradient descent is guaranteed to find the global minimum for any function J() regardless of using a combiner or not in the context of Hadoop (c) Gradient descent can converge even if alpha is kept fixed. (But alpha cannot be too large, or else it may fail to converge.) Combiners will help speed up the process. (d) For the specific choice of cost function J() used in linear regression, there is no local optima (other than the global optimum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: a, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===Weighted K-means===\n",
    "Write a MapReduce job in MRJob to do the training at scale of a weighted K-means algorithm.\n",
    "You can write your own code or you can use most of the code from the following notebook:\n",
    "http://nbviewer.ipython.org/urls/dl.dropbox.com/s/kjtdyi10nwmk4ko/MrJobKmeans-MIDS-Midterm.ipynb https://www.dropbox.com/s/kjtdyi10nwmk4ko/MrJobKmeans-MIDS-Midterm.ipynb?dl=0\n",
    "Weight each example as follows using the inverse vector length (Euclidean norm):\n",
    "weight(X)= 1/||X||,\n",
    "where ||X|| = SQRT(X.X)= SQRT(X1^2 + X2^2)\n",
    "Here X is vector made up of X1 and X2.\n",
    "Using the following data answer the following questions:\n",
    "https://www.dropbox.com/s/ai1uc3q2ucverly/Kmeandata.csv?dl=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT12. \n",
    "Which of the following statements are true? Select all that apply. a) Since K-Means is an unsupervised learning algorithm, it cannot overfit the data, and thus it is always better to have as large a number of clusters as is computationally feasible. b) The standard way of initializing K-means is setting μ1=⋯=μk to be equal to a vector of zeros. c) For some datasets, the \"right\" or \"correct\" value of K (the number of clusters) can be ambiguous, and hard even for a human expert looking carefully at the data to decide. d) A good way to initialize K-means is to select K (distinct) examples from the training set and set the cluster centroids equal to these selected examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers: c , d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
